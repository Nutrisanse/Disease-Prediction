{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Working directory set to: c:\\Users\\Dana\\Documents\\Kuliah\\Bangkit\\Capstone-C242-PS384_Project01\n",
      "\n",
      "Available files in directory:\n",
      "- anemia-dataset.csv\n",
      "- cholesterol-dataset.csv\n",
      "- chronic-kidney-disease-dataset.csv\n",
      "- combined_dataset.csv\n",
      "- diabetes-dataset.csv\n",
      "- health_data1_combined.csv\n",
      "- heart-disease-dataset.csv\n",
      "- hypertension-dataset.csv\n",
      "- metabolic-syndrome-dataset.csv\n",
      "- nafld1-dataset.csv\n",
      "- nafld2-dataset.csv\n",
      "- nwtco-dataset.csv\n",
      "- obesity-dataset.csv\n",
      "- stroke-dataset.csv\n",
      "\n",
      "Processing: anemia-dataset.csv\n",
      "Successfully loaded anemia-dataset.csv\n",
      "First few rows:\n",
      "   Gender  Hemoglobin   MCH  MCHC   MCV  Result\n",
      "0       1        14.9  22.7  29.1  83.7       0\n",
      "1       0        15.9  25.4  28.3  72.0       0\n",
      "2       0         9.0  21.5  29.6  71.2       1\n",
      "3       0        14.9  16.0  31.4  87.5       0\n",
      "4       1        14.7  22.0  28.2  99.5       0\n",
      "\n",
      "Processing: cholesterol-dataset.csv\n",
      "Successfully loaded cholesterol-dataset.csv\n",
      "First few rows:\n",
      "   age  sex  cp  trestbps  fbs  restecg  thalach  exang  oldpeak  slope ca  \\\n",
      "0   63    1   1       145    1        2      150      0      2.3      3  0   \n",
      "1   67    1   4       160    0        2      108      1      1.5      2  3   \n",
      "2   67    1   4       120    0        2      129      1      2.6      2  2   \n",
      "3   37    1   3       130    0        0      187      0      3.5      3  0   \n",
      "4   41    0   2       130    0        2      172      0      1.4      1  0   \n",
      "\n",
      "  thal  num  chol  \n",
      "0    6    0   233  \n",
      "1    3    2   286  \n",
      "2    7    1   229  \n",
      "3    3    0   250  \n",
      "4    3    0   204  \n",
      "\n",
      "Processing: chronic-kidney-disease-dataset.csv\n",
      "Successfully loaded chronic-kidney-disease-dataset.csv\n",
      "First few rows:\n",
      "   id   age    bp     sg   al   su     rbc        pc         pcc          ba  \\\n",
      "0   0  48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "1   1   7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "2   2  62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
      "3   3  48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
      "4   4  51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
      "\n",
      "   ...  pcv    wc   rc  htn   dm  cad appet   pe  ane classification  \n",
      "0  ...   44  7800  5.2  yes  yes   no  good   no   no            ckd  \n",
      "1  ...   38  6000  NaN   no   no   no  good   no   no            ckd  \n",
      "2  ...   31  7500  NaN   no  yes   no  poor   no  yes            ckd  \n",
      "3  ...   32  6700  3.9  yes   no   no  poor  yes  yes            ckd  \n",
      "4  ...   35  7300  4.6   no   no   no  good   no   no            ckd  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "Processing: combined_dataset.csv\n",
      "Successfully loaded combined_dataset.csv\n",
      "First few rows:\n",
      "   height  weight  gender  age  bp  bc     bg  bmi  sodium  fat  ...  anemia  \\\n",
      "0     NaN     NaN     0.0  NaN NaN NaN  104.3  NaN     NaN  NaN  ...     0.0   \n",
      "1     NaN     NaN     0.0  NaN NaN NaN  111.3  NaN     NaN  NaN  ...     0.0   \n",
      "2     NaN     NaN     0.0  NaN NaN NaN   63.0  NaN     NaN  NaN  ...     1.0   \n",
      "3     NaN     NaN     0.0  NaN NaN NaN  104.3  NaN     NaN  NaN  ...     0.0   \n",
      "4     NaN     NaN     0.0  NaN NaN NaN  102.9  NaN     NaN  NaN  ...     0.0   \n",
      "\n",
      "   cholesterol  ckd  diabetes  heart  hypertension  ms  nafld  obesity  stroke  \n",
      "0          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "1          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "2          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "3          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "4          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Processing: diabetes-dataset.csv\n",
      "Successfully loaded diabetes-dataset.csv\n",
      "First few rows:\n",
      "    Age  Sex  HighChol  CholCheck   BMI  Smoker  HeartDiseaseorAttack  \\\n",
      "0   4.0  1.0       0.0        1.0  26.0     0.0                   0.0   \n",
      "1  12.0  1.0       1.0        1.0  26.0     1.0                   0.0   \n",
      "2  13.0  1.0       0.0        1.0  26.0     0.0                   0.0   \n",
      "3  11.0  1.0       1.0        1.0  28.0     1.0                   0.0   \n",
      "4   8.0  0.0       0.0        1.0  29.0     1.0                   0.0   \n",
      "\n",
      "   PhysActivity  Fruits  Veggies  HvyAlcoholConsump  GenHlth  MentHlth  \\\n",
      "0           1.0     0.0      1.0                0.0      3.0       5.0   \n",
      "1           0.0     1.0      0.0                0.0      3.0       0.0   \n",
      "2           1.0     1.0      1.0                0.0      1.0       0.0   \n",
      "3           1.0     1.0      1.0                0.0      3.0       0.0   \n",
      "4           1.0     1.0      1.0                0.0      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Stroke  HighBP  Diabetes  \n",
      "0      30.0       0.0     0.0     1.0       0.0  \n",
      "1       0.0       0.0     1.0     1.0       0.0  \n",
      "2      10.0       0.0     0.0     0.0       0.0  \n",
      "3       3.0       0.0     0.0     1.0       0.0  \n",
      "4       0.0       0.0     0.0     0.0       0.0  \n",
      "\n",
      "Processing: health_data1_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dana\\AppData\\Local\\Temp\\ipykernel_167176\\2548305139.py:42: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded health_data1_combined.csv\n",
      "First few rows:\n",
      "  gender  hemoglobin  age  blood_pressure  cholesterol  glucose  bmi  height  \\\n",
      "0    1.0        14.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "1    0.0        15.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "2    0.0         9.0  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "3    0.0        14.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "4    1.0        14.7  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "\n",
      "   weight  HDL  Height  Weight  \n",
      "0     NaN  NaN     NaN     NaN  \n",
      "1     NaN  NaN     NaN     NaN  \n",
      "2     NaN  NaN     NaN     NaN  \n",
      "3     NaN  NaN     NaN     NaN  \n",
      "4     NaN  NaN     NaN     NaN  \n",
      "\n",
      "Processing: heart-disease-dataset.csv\n",
      "Successfully loaded heart-disease-dataset.csv\n",
      "First few rows:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
      "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
      "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
      "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
      "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   2     3       0  \n",
      "1   0     3       0  \n",
      "2   0     3       0  \n",
      "3   1     3       0  \n",
      "4   3     2       0  \n",
      "\n",
      "Processing: hypertension-dataset.csv\n",
      "Successfully loaded hypertension-dataset.csv\n",
      "First few rows:\n",
      "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0  57.0  1.0   3       145   233    1        0      150      0      2.3   \n",
      "1  64.0  0.0   2       130   250    0        1      187      0      3.5   \n",
      "2  52.0  1.0   1       130   204    0        0      172      0      1.4   \n",
      "3  56.0  0.0   1       120   236    0        1      178      0      0.8   \n",
      "4  66.0  0.0   0       120   354    0        1      163      1      0.6   \n",
      "\n",
      "   slope  ca  thal  target  \n",
      "0      0   0     1       1  \n",
      "1      0   0     2       1  \n",
      "2      2   0     2       1  \n",
      "3      2   0     2       1  \n",
      "4      2   0     2       1  \n",
      "\n",
      "Processing: metabolic-syndrome-dataset.csv\n",
      "Successfully loaded metabolic-syndrome-dataset.csv\n",
      "First few rows:\n",
      "    seqn  Age     Sex  Marital  Income   Race  WaistCirc   BMI  Albuminuria  \\\n",
      "0  62161   22    Male   Single  8200.0  White       81.0  23.3            0   \n",
      "1  62164   44  Female  Married  4500.0  White       80.1  23.2            0   \n",
      "2  62169   21    Male   Single   800.0  Asian       69.6  20.1            0   \n",
      "3  62172   43  Female   Single  2000.0  Black      120.4  33.3            0   \n",
      "4  62177   51    Male  Married     NaN  Asian       81.1  20.1            0   \n",
      "\n",
      "   UrAlbCr  UricAcid  BloodGlucose  HDL  Triglycerides  MetabolicSyndrome  \n",
      "0     3.88       4.9            92   41             84                  0  \n",
      "1     8.55       4.5            82   28             56                  0  \n",
      "2     5.07       5.4           107   43             78                  0  \n",
      "3     5.22       5.0           104   73            141                  0  \n",
      "4     8.13       5.0            95   43            126                  0  \n",
      "\n",
      "Processing: nafld1-dataset.csv\n",
      "Successfully loaded nafld1-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  id  age  male  weight  height        bmi  case.id  futime  \\\n",
      "0        3631   1   57     0    60.0   163.0  22.690939  10630.0    6261   \n",
      "1        8458   2   67     0    70.4   168.0  24.884028  14817.0     624   \n",
      "2        6298   3   53     1   105.8   186.0  30.453537      3.0    1783   \n",
      "3       15398   4   56     1   109.3   170.0  37.830100   6628.0    3143   \n",
      "4       13261   5   68     1     NaN     NaN        NaN   1871.0    1836   \n",
      "\n",
      "   status  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       1  \n",
      "\n",
      "Processing: nafld2-dataset.csv\n",
      "Successfully loaded nafld2-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  id  days  test  value\n",
      "0      135077   1  -459   hdl   75.0\n",
      "1      313143   1  -459  chol   75.0\n",
      "2      135078   1   183   hdl   64.0\n",
      "3      313144   1   183  chol   64.0\n",
      "4      135079   1  2030   hdl   74.0\n",
      "\n",
      "Processing: nwtco-dataset.csv\n",
      "Successfully loaded nwtco-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  seqno  instit  histol  stage  study  rel  edrel  age  \\\n",
      "0           1      1       2       2      1      3    0   6075   25   \n",
      "1           2      2       1       1      2      3    0   4121   50   \n",
      "2           3      3       2       2      1      3    0   6069    9   \n",
      "3           4      4       2       1      4      3    0   6200   28   \n",
      "4           5      5       2       2      2      3    0   1244   55   \n",
      "\n",
      "   in.subcohort  \n",
      "0         False  \n",
      "1         False  \n",
      "2         False  \n",
      "3          True  \n",
      "4         False  \n",
      "\n",
      "Processing: obesity-dataset.csv\n",
      "Successfully loaded obesity-dataset.csv\n",
      "First few rows:\n",
      "   ID  Age  Gender  Height  Weight   BMI          Label\n",
      "0   1   25    Male     175      80  25.3  Normal Weight\n",
      "1   2   30  Female     160      60  22.5  Normal Weight\n",
      "2   3   35    Male     180      90  27.3     Overweight\n",
      "3   4   40  Female     150      50  20.0    Underweight\n",
      "4   5   45    Male     190     100  31.2          Obese\n",
      "\n",
      "Processing: stroke-dataset.csv\n",
      "Successfully loaded stroke-dataset.csv\n",
      "First few rows:\n",
      "   sex   age  hypertension  heart_disease  ever_married  work_type  \\\n",
      "0  1.0  63.0             0              1             1          4   \n",
      "1  1.0  42.0             0              1             1          4   \n",
      "2  0.0  61.0             0              0             1          4   \n",
      "3  1.0  41.0             1              0             1          3   \n",
      "4  1.0  85.0             0              0             1          4   \n",
      "\n",
      "   Residence_type  avg_glucose_level   bmi  smoking_status  stroke  \n",
      "0               1             228.69  36.6               1       1  \n",
      "1               0             105.92  32.5               0       1  \n",
      "2               1             171.23  34.4               1       1  \n",
      "3               0             174.12  24.0               0       1  \n",
      "4               1             186.21  29.0               1       1  \n",
      "File inside the dataset folder: ['anemia-dataset.csv', 'cholesterol-dataset.csv', 'chronic-kidney-disease-dataset.csv', 'combined_dataset.csv', 'diabetes-dataset.csv', 'health_data1_combined.csv', 'heart-disease-dataset.csv', 'hypertension-dataset.csv', 'metabolic-syndrome-dataset.csv', 'nafld1-dataset.csv', 'nafld2-dataset.csv', 'nwtco-dataset.csv', 'obesity-dataset.csv', 'stroke-dataset.csv']\n",
      "All files are available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Step 0, chek dataset availability\n",
    "\n",
    "def set_project_directory():\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    if os.path.basename(current_dir) == 'scripts':\n",
    "        os.chdir('..')\n",
    "    \n",
    "    print(f\"Working directory set to: {os.getcwd()}\")\n",
    "\n",
    "def check_data_directory():\n",
    "    data_dir = os.path.join('dataset', 'health_data1')\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Directory not found: {data_dir}\")\n",
    "        print(\"Available directories:\", os.listdir('.'))\n",
    "        return False\n",
    "    \n",
    "    print(\"\\nAvailable files in directory:\")\n",
    "    for file in os.listdir(data_dir):\n",
    "        print(f\"- {file}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            return pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.XPT'):\n",
    "            return pd.read_sas(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_path}\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "set_project_directory()\n",
    "\n",
    "if check_data_directory():\n",
    "    data_directory = os.path.join('dataset', 'health_data1')\n",
    "    \n",
    "    for filename in os.listdir(data_directory):\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        data = load_data(file_path)\n",
    "        if data is not None:\n",
    "            print(f\"Successfully loaded {filename}\")\n",
    "            print(\"First few rows:\")\n",
    "            print(data.head())\n",
    "else:\n",
    "    print(\"Please check your directory structure and file locations\")\n",
    "\n",
    "folder_path = 'dataset/health_data1/'\n",
    "\n",
    "try:\n",
    "    print(\"File inside the dataset folder:\", os.listdir(folder_path))\n",
    "    required_files = [\n",
    "        'anemia-dataset.csv',\n",
    "        'cholesterol-dataset.csv',\n",
    "        'chronic-kidney-disease-dataset.csv',\n",
    "        'diabetes-dataset.csv',\n",
    "        'heart-disease-dataset.csv',\n",
    "        'hypertension-dataset.csv',\n",
    "        'metabolic-syndrome-dataset.csv',\n",
    "        'nafld1-dataset.csv',\n",
    "        'obesity-dataset.csv',\n",
    "        'stroke-dataset.csv'\n",
    "    ]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"Missing files: {', '.join(missing_files)}\")\n",
    "    else:\n",
    "        print(\"All files are available.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Folder missing: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: py310\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda activate py310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tidak ada GPU yang tersedia.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Cek apakah GPU tersedia\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU tersedia: {gpus}\")\n",
    "else:\n",
    "    print(\"Tidak ada GPU yang tersedia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tidak ada GPU yang tersedia, menggunakan CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dana\\AppData\\Local\\Temp\\ipykernel_167176\\3737213103.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features['gender'] = features['gender'].map({'Laki-laki': 1, 'Perempuan': 0})\n",
      "c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\utils\\extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\utils\\extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\utils\\extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3218/3218 [==============================] - 8s 2ms/step - loss: nan - accuracy: 0.2157 - val_loss: nan - val_accuracy: 0.2179\n",
      "Epoch 2/100\n",
      "3218/3218 [==============================] - 7s 2ms/step - loss: nan - accuracy: 0.2157 - val_loss: nan - val_accuracy: 0.2179\n",
      "Epoch 3/100\n",
      "2513/3218 [======================>.......] - ETA: 1s - loss: nan - accuracy: 0.2148"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Melatih model\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Simpan model\u001b[39;00m\n\u001b[0;32m     70\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/disease-prediction-model-fix\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\engine\\training.py:1813\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1811\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1812\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1813\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1815\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \n\u001b[0;32m    470\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    327\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    348\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    392\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 393\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m     logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m-> 1170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\utils\\generic_utils.py:296\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    293\u001b[0m         info \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m info\n\u001b[1;32m--> 296\u001b[0m     \u001b[43mio_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\utils\\io_utils.py:81\u001b[0m, in \u001b[0;36mprint_msg\u001b[1;34m(message, line_break)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m         sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(message)\n\u001b[1;32m---> 81\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(message)\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\ipykernel\\iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Cek apakah GPU tersedia dan atur penggunaan GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU tersedia: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"Tidak ada GPU yang tersedia, menggunakan CPU.\")\n",
    "\n",
    "def calculate_parameters(data):\n",
    "    data['bmi'] = data['weight'] / (data['height'] ** 2)\n",
    "    data['sodium'] = data['weight'] * 20\n",
    "    data['fat'] = np.where(data['gender'] == 'Laki-laki', \n",
    "                           data['weight'] * 0.15, \n",
    "                           data['weight'] * 0.25)\n",
    "    data['cholesterol'] = (data['bmi'] * 2) + (data['age'] * 0.15) + (data['bp'] * 0.05) + (data['bg'] * 0.02) + 150\n",
    "    data['protein'] = data['weight'] * 0.9\n",
    "    data['carbs'] = data['weight'] * 3\n",
    "    return data\n",
    "\n",
    "# Load dataset\n",
    "data_path = 'dataset/health_data1/combined_dataset.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Hitung parameter tambahan\n",
    "data = calculate_parameters(data)\n",
    "\n",
    "# Pilih fitur dan target\n",
    "features = data[['height', 'weight', 'gender', 'age', 'bp', \n",
    "                 'bc', 'bg', 'bmi', 'sodium', 'fat', \n",
    "                 'cholesterol', 'protein', 'carbs']]\n",
    "targets = data[['anemia', 'cholesterol', 'ckd', 'diabetes', 'heart', \n",
    "                 'hypertension', 'ms', 'nafld', 'obesity', 'stroke']]\n",
    "\n",
    "# Encode gender\n",
    "features['gender'] = features['gender'].map({'Laki-laki': 1, 'Perempuan': 0})\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardisasi fitur\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Membangun model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(y_train.shape[1], activation='sigmoid')  # Output layer for multi-label classification\n",
    "])\n",
    "\n",
    "# Compile model dengan MSE sebagai loss\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Melatih model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Simpan model\n",
    "model_dir = 'models/disease-prediction-model-fix'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model.save(os.path.join(model_dir, 'disease-prediction-model-tf.h5'))\n",
    "\n",
    "# Simpan arsitektur model ke JSON\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(model_dir, 'disease-prediction-model-tf.json'), 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "print(\"Model telah disimpan.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
