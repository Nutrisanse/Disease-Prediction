{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: c:\\Users\\Dana\\Documents\\Kuliah\\Bangkit\\Capstone-C242-PS384_Project01\n",
      "\n",
      "Available files in directory:\n",
      "- anemia-dataset.csv\n",
      "- cholesterol-dataset.csv\n",
      "- chronic-kidney-disease-dataset.csv\n",
      "- combined_dataset.csv\n",
      "- diabetes-dataset.csv\n",
      "- health_data1_combined.csv\n",
      "- heart-disease-dataset.csv\n",
      "- hypertension-dataset.csv\n",
      "- metabolic-syndrome-dataset.csv\n",
      "- nafld1-dataset.csv\n",
      "- nafld2-dataset.csv\n",
      "- nwtco-dataset.csv\n",
      "- obesity-dataset.csv\n",
      "- stroke-dataset.csv\n",
      "\n",
      "Processing: anemia-dataset.csv\n",
      "Successfully loaded anemia-dataset.csv\n",
      "First few rows:\n",
      "   Gender  Hemoglobin   MCH  MCHC   MCV  Result\n",
      "0       1        14.9  22.7  29.1  83.7       0\n",
      "1       0        15.9  25.4  28.3  72.0       0\n",
      "2       0         9.0  21.5  29.6  71.2       1\n",
      "3       0        14.9  16.0  31.4  87.5       0\n",
      "4       1        14.7  22.0  28.2  99.5       0\n",
      "\n",
      "Processing: cholesterol-dataset.csv\n",
      "Successfully loaded cholesterol-dataset.csv\n",
      "First few rows:\n",
      "   age  sex  cp  trestbps  fbs  restecg  thalach  exang  oldpeak  slope ca  \\\n",
      "0   63    1   1       145    1        2      150      0      2.3      3  0   \n",
      "1   67    1   4       160    0        2      108      1      1.5      2  3   \n",
      "2   67    1   4       120    0        2      129      1      2.6      2  2   \n",
      "3   37    1   3       130    0        0      187      0      3.5      3  0   \n",
      "4   41    0   2       130    0        2      172      0      1.4      1  0   \n",
      "\n",
      "  thal  num  chol  \n",
      "0    6    0   233  \n",
      "1    3    2   286  \n",
      "2    7    1   229  \n",
      "3    3    0   250  \n",
      "4    3    0   204  \n",
      "\n",
      "Processing: chronic-kidney-disease-dataset.csv\n",
      "Successfully loaded chronic-kidney-disease-dataset.csv\n",
      "First few rows:\n",
      "   id   age    bp     sg   al   su     rbc        pc         pcc          ba  \\\n",
      "0   0  48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "1   1   7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "2   2  62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
      "3   3  48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
      "4   4  51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
      "\n",
      "   ...  pcv    wc   rc  htn   dm  cad appet   pe  ane classification  \n",
      "0  ...   44  7800  5.2  yes  yes   no  good   no   no            ckd  \n",
      "1  ...   38  6000  NaN   no   no   no  good   no   no            ckd  \n",
      "2  ...   31  7500  NaN   no  yes   no  poor   no  yes            ckd  \n",
      "3  ...   32  6700  3.9  yes   no   no  poor  yes  yes            ckd  \n",
      "4  ...   35  7300  4.6   no   no   no  good   no   no            ckd  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "Processing: combined_dataset.csv\n",
      "Successfully loaded combined_dataset.csv\n",
      "First few rows:\n",
      "   height  weight  gender  age  bp  bc     bg  bmi  sodium  fat  ...  anemia  \\\n",
      "0     NaN     NaN     0.0  NaN NaN NaN  104.3  NaN     NaN  NaN  ...     0.0   \n",
      "1     NaN     NaN     0.0  NaN NaN NaN  111.3  NaN     NaN  NaN  ...     0.0   \n",
      "2     NaN     NaN     0.0  NaN NaN NaN   63.0  NaN     NaN  NaN  ...     1.0   \n",
      "3     NaN     NaN     0.0  NaN NaN NaN  104.3  NaN     NaN  NaN  ...     0.0   \n",
      "4     NaN     NaN     0.0  NaN NaN NaN  102.9  NaN     NaN  NaN  ...     0.0   \n",
      "\n",
      "   cholesterol  ckd  diabetes  heart  hypertension  ms  nafld  obesity  stroke  \n",
      "0          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "1          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "2          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "3          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "4          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Processing: diabetes-dataset.csv\n",
      "Successfully loaded diabetes-dataset.csv\n",
      "First few rows:\n",
      "    Age  Sex  HighChol  CholCheck   BMI  Smoker  HeartDiseaseorAttack  \\\n",
      "0   4.0  1.0       0.0        1.0  26.0     0.0                   0.0   \n",
      "1  12.0  1.0       1.0        1.0  26.0     1.0                   0.0   \n",
      "2  13.0  1.0       0.0        1.0  26.0     0.0                   0.0   \n",
      "3  11.0  1.0       1.0        1.0  28.0     1.0                   0.0   \n",
      "4   8.0  0.0       0.0        1.0  29.0     1.0                   0.0   \n",
      "\n",
      "   PhysActivity  Fruits  Veggies  HvyAlcoholConsump  GenHlth  MentHlth  \\\n",
      "0           1.0     0.0      1.0                0.0      3.0       5.0   \n",
      "1           0.0     1.0      0.0                0.0      3.0       0.0   \n",
      "2           1.0     1.0      1.0                0.0      1.0       0.0   \n",
      "3           1.0     1.0      1.0                0.0      3.0       0.0   \n",
      "4           1.0     1.0      1.0                0.0      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Stroke  HighBP  Diabetes  \n",
      "0      30.0       0.0     0.0     1.0       0.0  \n",
      "1       0.0       0.0     1.0     1.0       0.0  \n",
      "2      10.0       0.0     0.0     0.0       0.0  \n",
      "3       3.0       0.0     0.0     1.0       0.0  \n",
      "4       0.0       0.0     0.0     0.0       0.0  \n",
      "\n",
      "Processing: health_data1_combined.csv\n",
      "Successfully loaded health_data1_combined.csv\n",
      "First few rows:\n",
      "  gender  hemoglobin  age  blood_pressure  cholesterol  glucose  bmi  height  \\\n",
      "0    1.0        14.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "1    0.0        15.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "2    0.0         9.0  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "3    0.0        14.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "4    1.0        14.7  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "\n",
      "   weight  HDL  Height  Weight  \n",
      "0     NaN  NaN     NaN     NaN  \n",
      "1     NaN  NaN     NaN     NaN  \n",
      "2     NaN  NaN     NaN     NaN  \n",
      "3     NaN  NaN     NaN     NaN  \n",
      "4     NaN  NaN     NaN     NaN  \n",
      "\n",
      "Processing: heart-disease-dataset.csv\n",
      "Successfully loaded heart-disease-dataset.csv\n",
      "First few rows:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
      "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
      "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
      "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
      "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   2     3       0  \n",
      "1   0     3       0  \n",
      "2   0     3       0  \n",
      "3   1     3       0  \n",
      "4   3     2       0  \n",
      "\n",
      "Processing: hypertension-dataset.csv\n",
      "Successfully loaded hypertension-dataset.csv\n",
      "First few rows:\n",
      "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0  57.0  1.0   3       145   233    1        0      150      0      2.3   \n",
      "1  64.0  0.0   2       130   250    0        1      187      0      3.5   \n",
      "2  52.0  1.0   1       130   204    0        0      172      0      1.4   \n",
      "3  56.0  0.0   1       120   236    0        1      178      0      0.8   \n",
      "4  66.0  0.0   0       120   354    0        1      163      1      0.6   \n",
      "\n",
      "   slope  ca  thal  target  \n",
      "0      0   0     1       1  \n",
      "1      0   0     2       1  \n",
      "2      2   0     2       1  \n",
      "3      2   0     2       1  \n",
      "4      2   0     2       1  \n",
      "\n",
      "Processing: metabolic-syndrome-dataset.csv\n",
      "Successfully loaded metabolic-syndrome-dataset.csv\n",
      "First few rows:\n",
      "    seqn  Age     Sex  Marital  Income   Race  WaistCirc   BMI  Albuminuria  \\\n",
      "0  62161   22    Male   Single  8200.0  White       81.0  23.3            0   \n",
      "1  62164   44  Female  Married  4500.0  White       80.1  23.2            0   \n",
      "2  62169   21    Male   Single   800.0  Asian       69.6  20.1            0   \n",
      "3  62172   43  Female   Single  2000.0  Black      120.4  33.3            0   \n",
      "4  62177   51    Male  Married     NaN  Asian       81.1  20.1            0   \n",
      "\n",
      "   UrAlbCr  UricAcid  BloodGlucose  HDL  Triglycerides  MetabolicSyndrome  \n",
      "0     3.88       4.9            92   41             84                  0  \n",
      "1     8.55       4.5            82   28             56                  0  \n",
      "2     5.07       5.4           107   43             78                  0  \n",
      "3     5.22       5.0           104   73            141                  0  \n",
      "4     8.13       5.0            95   43            126                  0  \n",
      "\n",
      "Processing: nafld1-dataset.csv\n",
      "Successfully loaded nafld1-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  id  age  male  weight  height        bmi  case.id  futime  \\\n",
      "0        3631   1   57     0    60.0   163.0  22.690939  10630.0    6261   \n",
      "1        8458   2   67     0    70.4   168.0  24.884028  14817.0     624   \n",
      "2        6298   3   53     1   105.8   186.0  30.453537      3.0    1783   \n",
      "3       15398   4   56     1   109.3   170.0  37.830100   6628.0    3143   \n",
      "4       13261   5   68     1     NaN     NaN        NaN   1871.0    1836   \n",
      "\n",
      "   status  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       1  \n",
      "\n",
      "Processing: nafld2-dataset.csv\n",
      "Successfully loaded nafld2-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  id  days  test  value\n",
      "0      135077   1  -459   hdl   75.0\n",
      "1      313143   1  -459  chol   75.0\n",
      "2      135078   1   183   hdl   64.0\n",
      "3      313144   1   183  chol   64.0\n",
      "4      135079   1  2030   hdl   74.0\n",
      "\n",
      "Processing: nwtco-dataset.csv\n",
      "Successfully loaded nwtco-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  seqno  instit  histol  stage  study  rel  edrel  age  \\\n",
      "0           1      1       2       2      1      3    0   6075   25   \n",
      "1           2      2       1       1      2      3    0   4121   50   \n",
      "2           3      3       2       2      1      3    0   6069    9   \n",
      "3           4      4       2       1      4      3    0   6200   28   \n",
      "4           5      5       2       2      2      3    0   1244   55   \n",
      "\n",
      "   in.subcohort  \n",
      "0         False  \n",
      "1         False  \n",
      "2         False  \n",
      "3          True  \n",
      "4         False  \n",
      "\n",
      "Processing: obesity-dataset.csv\n",
      "Successfully loaded obesity-dataset.csv\n",
      "First few rows:\n",
      "   ID  Age  Gender  Height  Weight   BMI          Label\n",
      "0   1   25    Male     175      80  25.3  Normal Weight\n",
      "1   2   30  Female     160      60  22.5  Normal Weight\n",
      "2   3   35    Male     180      90  27.3     Overweight\n",
      "3   4   40  Female     150      50  20.0    Underweight\n",
      "4   5   45    Male     190     100  31.2          Obese\n",
      "\n",
      "Processing: stroke-dataset.csv\n",
      "Successfully loaded stroke-dataset.csv\n",
      "First few rows:\n",
      "   sex   age  hypertension  heart_disease  ever_married  work_type  \\\n",
      "0  1.0  63.0             0              1             1          4   \n",
      "1  1.0  42.0             0              1             1          4   \n",
      "2  0.0  61.0             0              0             1          4   \n",
      "3  1.0  41.0             1              0             1          3   \n",
      "4  1.0  85.0             0              0             1          4   \n",
      "\n",
      "   Residence_type  avg_glucose_level   bmi  smoking_status  stroke  \n",
      "0               1             228.69  36.6               1       1  \n",
      "1               0             105.92  32.5               0       1  \n",
      "2               1             171.23  34.4               1       1  \n",
      "3               0             174.12  24.0               0       1  \n",
      "4               1             186.21  29.0               1       1  \n",
      "Daftar file di dalam folder: ['anemia-dataset.csv', 'cholesterol-dataset.csv', 'chronic-kidney-disease-dataset.csv', 'combined_dataset.csv', 'diabetes-dataset.csv', 'health_data1_combined.csv', 'heart-disease-dataset.csv', 'hypertension-dataset.csv', 'metabolic-syndrome-dataset.csv', 'nafld1-dataset.csv', 'nafld2-dataset.csv', 'nwtco-dataset.csv', 'obesity-dataset.csv', 'stroke-dataset.csv']\n",
      "Semua file dataset ditemukan.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Step 0, chek dataset availability\n",
    "\n",
    "def set_project_directory():\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    if os.path.basename(current_dir) == 'scripts':\n",
    "        os.chdir('..')\n",
    "    \n",
    "    print(f\"Working directory set to: {os.getcwd()}\")\n",
    "\n",
    "def check_data_directory():\n",
    "    data_dir = os.path.join('dataset', 'health_data1')\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Directory not found: {data_dir}\")\n",
    "        print(\"Available directories:\", os.listdir('.'))\n",
    "        return False\n",
    "    \n",
    "    print(\"\\nAvailable files in directory:\")\n",
    "    for file in os.listdir(data_dir):\n",
    "        print(f\"- {file}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            return pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.XPT'):\n",
    "            return pd.read_sas(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_path}\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "set_project_directory()\n",
    "\n",
    "if check_data_directory():\n",
    "    data_directory = os.path.join('dataset', 'health_data1')\n",
    "    \n",
    "    for filename in os.listdir(data_directory):\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        data = load_data(file_path)\n",
    "        if data is not None:\n",
    "            print(f\"Successfully loaded {filename}\")\n",
    "            print(\"First few rows:\")\n",
    "            print(data.head())\n",
    "else:\n",
    "    print(\"Please check your directory structure and file locations\")\n",
    "\n",
    "folder_path = 'dataset/health_data1/'\n",
    "\n",
    "try:\n",
    "    print(\"Daftar file di dalam folder:\", os.listdir(folder_path))\n",
    "    required_files = [\n",
    "        'anemia-dataset.csv',\n",
    "        'cholesterol-dataset.csv',\n",
    "        'chronic-kidney-disease-dataset.csv',\n",
    "        'diabetes-dataset.csv',\n",
    "        'heart-disease-dataset.csv',\n",
    "        'hypertension-dataset.csv',\n",
    "        'metabolic-syndrome-dataset.csv',\n",
    "        'nafld1-dataset.csv',\n",
    "        'obesity-dataset.csv',\n",
    "        'stroke-dataset.csv'\n",
    "    ]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"File yang hilang: {', '.join(missing_files)}\")\n",
    "    else:\n",
    "        print(\"Semua file dataset ditemukan.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Folder tidak ditemukan: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset succesfully savd on dataset/health_data1/combined_dataset.csv\n",
      "\n",
      "combined dataset inform:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160892 entries, 0 to 160891\n",
      "Data columns (total 22 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   height        14489 non-null   float64\n",
      " 1   weight        12871 non-null   float64\n",
      " 2   gender        160492 non-null  float64\n",
      " 3   age           159462 non-null  float64\n",
      " 4   bp            27799 non-null   float64\n",
      " 5   bc            68321 non-null   float64\n",
      " 6   bg            5506 non-null    float64\n",
      " 7   bmi           126673 non-null  float64\n",
      " 8   sodium        313 non-null     float64\n",
      " 9   fat           0 non-null       float64\n",
      " 10  protein       0 non-null       float64\n",
      " 11  carbs         0 non-null       float64\n",
      " 12  anemia        160892 non-null  float64\n",
      " 13  cholesterol   160892 non-null  float64\n",
      " 14  ckd           160892 non-null  int64  \n",
      " 15  diabetes      160892 non-null  int64  \n",
      " 16  heart         160892 non-null  float64\n",
      " 17  hypertension  160892 non-null  float64\n",
      " 18  ms            160892 non-null  int64  \n",
      " 19  nafld         160892 non-null  float64\n",
      " 20  obesity       160892 non-null  int64  \n",
      " 21  stroke        160892 non-null  float64\n",
      "dtypes: float64(18), int64(4)\n",
      "memory usage: 27.0 MB\n",
      "None\n",
      "\n",
      "combined dataset stats:\n",
      "             height        weight         gender            age            bp  \\\n",
      "count  14489.000000  12871.000000  160492.000000  159462.000000  27799.000000   \n",
      "mean     169.413624     86.127947       0.145771      33.217437    130.825353   \n",
      "std       10.386545     22.435919       0.352877      26.020889     18.691371   \n",
      "min      120.000000     10.000000       0.000000      -9.000000     50.000000   \n",
      "25%      162.000000     69.900000       0.000000       9.000000    120.000000   \n",
      "50%      169.000000     83.700000       0.000000      28.000000    130.000000   \n",
      "75%      177.000000     99.100000       0.000000      56.000000    140.000000   \n",
      "max      215.000000    181.700000       1.000000     112.000000    200.000000   \n",
      "\n",
      "                 bc           bg            bmi      sodium  fat  ...  \\\n",
      "count  68321.000000  5506.000000  126673.000000  313.000000  0.0  ...   \n",
      "mean      98.870904   105.844624      30.026370  137.528754  NaN  ...   \n",
      "std      124.988318    33.940690       7.026505   10.408752  NaN  ...   \n",
      "min        0.000000    22.000000       3.900000    4.500000  NaN  ...   \n",
      "25%        0.000000    92.400000      25.000000  135.000000  NaN  ...   \n",
      "50%        0.000000   100.000000      29.000000  138.000000  NaN  ...   \n",
      "75%      227.000000   108.000000      33.300000  142.000000  NaN  ...   \n",
      "max      564.000000   490.000000      98.000000  163.000000  NaN  ...   \n",
      "\n",
      "              anemia    cholesterol            ckd       diabetes  \\\n",
      "count  160892.000000  160892.000000  160892.000000  160892.000000   \n",
      "mean        0.004226       0.231926       0.001541       0.220521   \n",
      "std         0.064874       0.422063       0.039231       0.414599   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "               heart   hypertension             ms          nafld   obesity  \\\n",
      "count  160892.000000  160892.000000  160892.000000  160892.000000  160892.0   \n",
      "mean        0.035956       0.391573       0.005109       0.008478       0.0   \n",
      "std         0.186181       0.488104       0.071295       0.091684       0.0   \n",
      "min         0.000000       0.000000       0.000000       0.000000       0.0   \n",
      "25%         0.000000       0.000000       0.000000       0.000000       0.0   \n",
      "50%         0.000000       0.000000       0.000000       0.000000       0.0   \n",
      "75%         0.000000       1.000000       0.000000       0.000000       0.0   \n",
      "max         1.000000       1.000000       1.000000       1.000000       0.0   \n",
      "\n",
      "              stroke  \n",
      "count  160892.000000  \n",
      "mean        0.127166  \n",
      "std         0.333160  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         1.000000  \n",
      "\n",
      "[8 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1, preprocess available data and merge it in the end\n",
    "columns = [\n",
    "    \"height\", \"weight\", \"gender\", \"age\", \"bp\", \"bc\", \"bg\", \"bmi\", \"sodium\", \n",
    "    \"fat\", \"protein\", \"carbs\", \"anemia\", \"cholesterol\", \"ckd\", \"diabetes\", \n",
    "    \"heart\", \"hypertension\", \"ms\", \"nafld\", \"obesity\", \"stroke\"\n",
    "]\n",
    "\n",
    "def create_data_dict(**kwargs):\n",
    "    base_dict = {\n",
    "        \"height\": np.nan, \"weight\": np.nan, \"gender\": np.nan, \"age\": np.nan,\n",
    "        \"bp\": np.nan, \"bc\": np.nan, \"bg\": np.nan, \"bmi\": np.nan,\n",
    "        \"sodium\": np.nan, \"fat\": np.nan, \"protein\": np.nan, \"carbs\": np.nan,\n",
    "        \"anemia\": 0, \"cholesterol\": 0, \"ckd\": 0, \"diabetes\": 0,\n",
    "        \"heart\": 0, \"hypertension\": 0, \"ms\": 0, \"nafld\": 0, \"obesity\": 0, \"stroke\": 0\n",
    "    }\n",
    "    base_dict.update({k: v for k, v in kwargs.items() if v is not None})\n",
    "    return base_dict\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# 1. Anemia dataset\n",
    "anemia_data = pd.read_csv(os.path.join(folder_path, \"anemia-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        gender=1 if row[\"Gender\"] == \"Male\" else 0,\n",
    "        bg=round(row[\"Hemoglobin\"] * 7, 1),\n",
    "        anemia=row[\"Result\"]\n",
    "    )\n",
    "    for _, row in anemia_data.iterrows()\n",
    "])\n",
    "\n",
    "# 2. Cholesterol dataset\n",
    "chol_data = pd.read_csv(os.path.join(folder_path, \"cholesterol-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=row[\"sex\"],\n",
    "        bp=row[\"trestbps\"],\n",
    "        bc=row[\"chol\"],\n",
    "        bg=120 if row[\"fbs\"] == 1 else 100,\n",
    "        cholesterol=1 if row[\"chol\"] > 240 else 0\n",
    "    )\n",
    "    for _, row in chol_data.iterrows()\n",
    "])\n",
    "\n",
    "# 3. Chronic Kidney Disease dataset\n",
    "ckd_data = pd.read_csv(os.path.join(folder_path, \"chronic-kidney-disease-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        bp=row[\"bp\"],\n",
    "        bg=row[\"bgr\"],\n",
    "        sodium=row[\"sod\"],\n",
    "        anemia=1 if row[\"ane\"] == \"yes\" else 0,\n",
    "        ckd=1 if row[\"classification\"] == \"ckd\" else 0,\n",
    "        diabetes=1 if row[\"dm\"] == \"yes\" else 0,\n",
    "        heart=1 if row[\"cad\"] == \"yes\" else 0,\n",
    "        hypertension=1 if row[\"htn\"] == \"yes\" else 0\n",
    "    )\n",
    "    for _, row in ckd_data.iterrows()\n",
    "])\n",
    "\n",
    "# 4. Diabetes dataset\n",
    "diabetes_data = pd.read_csv(os.path.join(folder_path, \"diabetes-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"Age\"],\n",
    "        gender=1 if row[\"Sex\"] == \"Male\" else 0,\n",
    "        bmi=row[\"BMI\"],\n",
    "        cholesterol=row[\"HighChol\"],\n",
    "        diabetes=1 if row[\"Diabetes\"] == 1 else 0,\n",
    "        hypertension=row[\"HighBP\"]\n",
    "    )\n",
    "    for _, row in diabetes_data.iterrows()\n",
    "])\n",
    "\n",
    "# 5. Heart Disease dataset\n",
    "heart_data = pd.read_csv(os.path.join(folder_path, \"heart-disease-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=1 if row[\"sex\"] == 1 else 0,\n",
    "        bp=row[\"trestbps\"],\n",
    "        bc=row[\"chol\"],\n",
    "        bg=120 if row[\"fbs\"] == 1 else 100,\n",
    "        heart=1 if row[\"target\"] == 1 else 0\n",
    "    )\n",
    "    for _, row in heart_data.iterrows()\n",
    "])\n",
    "\n",
    "# 6. Hypertension dataset\n",
    "hypertension_data = pd.read_csv(os.path.join(folder_path, \"hypertension-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=1 if row[\"sex\"] == 1 else 0,\n",
    "        bp=row[\"trestbps\"],\n",
    "        bc=row[\"chol\"],\n",
    "        hypertension=1 if row[\"target\"] == 1 else 0\n",
    "    )\n",
    "    for _, row in hypertension_data.iterrows()\n",
    "])\n",
    "\n",
    "# 7. Metabolic Syndrome dataset\n",
    "ms_data = pd.read_csv(os.path.join(folder_path, \"metabolic-syndrome-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"Age\"],\n",
    "        gender=1 if row[\"Sex\"] == \"Male\" else 0,\n",
    "        bg=row[\"BloodGlucose\"],\n",
    "        bmi=row[\"BMI\"],\n",
    "        ms=1 if row[\"MetabolicSyndrome\"] == 1 else 0\n",
    "    )\n",
    "    for _, row in ms_data.iterrows()\n",
    "])\n",
    "\n",
    "# 8. NAFLD dataset\n",
    "nafld_data = pd.read_csv(os.path.join(folder_path, \"nafld1-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=row[\"male\"],\n",
    "        weight=row[\"weight\"],\n",
    "        height=row[\"height\"],\n",
    "        bmi=round(row[\"bmi\"],1),\n",
    "        nafld=row[\"status\"]\n",
    "    )\n",
    "    for _, row in nafld_data.iterrows()\n",
    "])\n",
    "\n",
    "# 9. Obesity dataset\n",
    "obesity_data = pd.read_csv(os.path.join(folder_path, \"obesity-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"Age\"],\n",
    "        gender=1 if row[\"Gender\"] == \"Male\" else 0,\n",
    "        weight=row[\"Weight\"],\n",
    "        height=row[\"Height\"],\n",
    "        bmi=row[\"BMI\"],\n",
    "        obesity=1 if row[\"Label\"] == \"Obesity\" else 0\n",
    "    )\n",
    "    for _, row in obesity_data.iterrows()\n",
    "])\n",
    "\n",
    "# 10. Stroke dataset\n",
    "stroke_data = pd.read_csv(os.path.join(folder_path, \"stroke-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=1 if row[\"sex\"] == \"Male\" else 0,\n",
    "        bc=row[\"heart_disease\"],\n",
    "        bmi=row[\"bmi\"],\n",
    "        heart=row[\"heart_disease\"],\n",
    "        hypertension=row[\"hypertension\"],\n",
    "        stroke=row[\"stroke\"]\n",
    "    )\n",
    "    for _, row in stroke_data.iterrows()\n",
    "])\n",
    "\n",
    "combined_data = pd.DataFrame(all_data)\n",
    "\n",
    "output_path = os.path.join(folder_path, \"combined_dataset.csv\")\n",
    "combined_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Combined dataset succesfully savd on {output_path}\")\n",
    "print(\"\\ncombined dataset inform:\")\n",
    "print(combined_data.info())\n",
    "print(\"\\ncombined dataset stats:\")\n",
    "print(combined_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_features(X):\n",
    "    # Menambah fitur interaksi\n",
    "    X_new = X.copy()\n",
    "    \n",
    "    # BMI categories\n",
    "    X_new['bmi_category'] = pd.cut(X_new['bmi'], \n",
    "                                  bins=[0, 18.5, 25, 30, 100],\n",
    "                                  labels=[0, 1, 2, 3])\n",
    "    \n",
    "    # Age categories\n",
    "    X_new['age_category'] = pd.cut(X_new['age'], \n",
    "                                  bins=[0, 30, 45, 60, 100],\n",
    "                                  labels=[0, 1, 2, 3])\n",
    "    \n",
    "    # Blood pressure categories\n",
    "    X_new['bp_category'] = pd.cut(X_new['blood_pressure'], \n",
    "                                 bins=[0, 120, 140, 160, 300],\n",
    "                                 labels=[0, 1, 2, 3])\n",
    "    \n",
    "    # Interaksi antara fitur\n",
    "    X_new['bmi_age'] = X_new['bmi'] * X_new['age']\n",
    "    X_new['bp_age'] = X_new['blood_pressure'] * X_new['age']\n",
    "    X_new['bmi_bp'] = X_new['bmi'] * X_new['blood_pressure']\n",
    "    \n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2, load newly created combinedd dataset (combined_dataset.csv)\n",
    "def load_and_process_data(file_path='dataset/health_data1/combined_dataset.csv'):\n",
    "    print(\"Loading and processing data...\")\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    \n",
    "    # Define features and targets\n",
    "    available_features = ['height', 'weight', 'gender', 'age', 'bp', 'bc', 'bg', 'bmi', 'sodium']\n",
    "    target_variables = ['anemia', 'cholesterol', 'ckd', 'diabetes', 'heart',\n",
    "                       'hypertension', 'ms', 'nafld', 'obesity', 'stroke']\n",
    "    \n",
    "    print(\"Processing features...\")\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values first\n",
    "    print(\"Handling missing values...\")\n",
    "    for col in available_features:\n",
    "        if col not in df_processed.columns:\n",
    "            df_processed[col] = np.nan\n",
    "    \n",
    "    # Separate numeric and categorical features\n",
    "    numeric_features = df_processed[available_features].select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_features = df_processed[available_features].select_dtypes(include=['object']).columns\n",
    "    \n",
    "    print(f\"Found {len(numeric_features)} numeric features and {len(categorical_features)} categorical features\")\n",
    "    \n",
    "    # Initialize imputers\n",
    "    numeric_imputer = IterativeImputer(\n",
    "        random_state=42,\n",
    "        max_iter=100,\n",
    "        sample_posterior=True\n",
    "    )\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    # Impute numeric features\n",
    "    print(\"Imputing numeric features...\")\n",
    "    df_processed[numeric_features] = numeric_imputer.fit_transform(df_processed[numeric_features])\n",
    "    \n",
    "    # Impute categorical features if any\n",
    "    if len(categorical_features) > 0:\n",
    "        print(\"Imputing categorical features...\")\n",
    "        df_processed[categorical_features] = categorical_imputer.fit_transform(df_processed[categorical_features])\n",
    "    \n",
    "    print(\"Adding engineered features...\")\n",
    "    # Add engineered features\n",
    "    df_processed['bmi_category'] = pd.cut(\n",
    "        df_processed['bmi'],\n",
    "        bins=[float('-inf'), 18.5, 25, 30, float('inf')],\n",
    "        labels=[0, 1, 2, 3]\n",
    "    )\n",
    "    \n",
    "    df_processed['age_category'] = pd.cut(\n",
    "        df_processed['age'],\n",
    "        bins=[float('-inf'), 30, 45, 60, float('inf')],\n",
    "        labels=[0, 1, 2, 3]\n",
    "    )\n",
    "    \n",
    "    df_processed['bp_category'] = pd.cut(\n",
    "        df_processed['bp'],\n",
    "        bins=[float('-inf'), 120, 140, 160, float('inf')],\n",
    "        labels=[0, 1, 2, 3]\n",
    "    )\n",
    "    \n",
    "    # Create interaction features\n",
    "    df_processed['bmi_age'] = df_processed['bmi'] * df_processed['age']\n",
    "    df_processed['bp_age'] = df_processed['bp'] * df_processed['age']\n",
    "    df_processed['bmi_bp'] = df_processed['bmi'] * df_processed['bp']\n",
    "    \n",
    "    # Normalize features\n",
    "    print(\"Normalizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    df_processed[numeric_features] = scaler.fit_transform(df_processed[numeric_features])\n",
    "    \n",
    "    # Prepare final features\n",
    "    final_features = (\n",
    "        available_features + \n",
    "        ['bmi_category', 'age_category', 'bp_category', \n",
    "         'bmi_age', 'bp_age', 'bmi_bp']\n",
    "    )\n",
    "    \n",
    "    # Handle target variables\n",
    "    print(\"Processing target variables...\")\n",
    "    df_processed[target_variables] = df[target_variables].fillna(0)\n",
    "    \n",
    "    print(\"Data processing completed!\")\n",
    "    return df_processed[final_features], df_processed[target_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3, create the calculation for the derived data from main user input data\n",
    "def calculate_derived_features(height, weight, gender, age, blood_pressure, cholesterol, blood_glucose):\n",
    "    # bmi calculation\n",
    "    height_m = height / 100\n",
    "    bmi = weight / (height_m ** 2)\n",
    "    \n",
    "    # sodium calculation\n",
    "    sodium = weight * 20\n",
    "    \n",
    "    # fat based on gender calculatonn\n",
    "    fat = weight * (0.15 if gender == 1 else 0.25)\n",
    "    \n",
    "    # chols level calc\n",
    "    cholesterol_level = (bmi * 2) + (age * 0.15) + (blood_pressure * 0.05) + (blood_glucose * 0.02) + 150\n",
    "    \n",
    "    # protein calc\n",
    "    protein = weight * 0.9\n",
    "    \n",
    "    # carbo calc\n",
    "    carbs = weight * 3\n",
    "    \n",
    "    return {\n",
    "        'bmi': bmi,\n",
    "        'sodium': sodium,\n",
    "        'fat': fat,\n",
    "        'cholesterol_level': cholesterol_level,\n",
    "        'protein': protein,\n",
    "        'carbs': carbs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4, model training set + save\n",
    "def train_disease_models(X, y):\n",
    "    print(\"\\n=== Starting Model Training Process ===\")\n",
    "    models = {}\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    print(\"1. Initializing StandardScaler...\")\n",
    "    X = np.array(X)\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(\"   ✓ Data scaling completed\")\n",
    "    \n",
    "    total_diseases = len(y.columns)\n",
    "    print(f\"\\nTotal diseases to process: {total_diseases}\")\n",
    "    \n",
    "    # Parameter configurations remain the same\n",
    "    base_params = {\n",
    "        'n_estimators': 500,\n",
    "        'max_depth': 20,\n",
    "        'min_samples_split': 4,\n",
    "        'min_samples_leaf': 2,\n",
    "        'random_state': 42,\n",
    "        'class_weight': 'balanced_subsample',\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Disease specific parameters remain the same\n",
    "    disease_params = {\n",
    "        'heart': {\n",
    "            'n_estimators': 1000,\n",
    "            'max_depth': 25,\n",
    "            'min_samples_split': 3,\n",
    "            'class_weight': 'balanced_subsample'\n",
    "        },\n",
    "        'hypertension': {\n",
    "            'n_estimators': 800,\n",
    "            'max_depth': 22,\n",
    "            'min_samples_split': 3,\n",
    "            'class_weight': 'balanced_subsample'\n",
    "        },\n",
    "        'diabetes': {\n",
    "            'n_estimators': 800,\n",
    "            'max_depth': 22,\n",
    "            'min_samples_split': 3,\n",
    "            'class_weight': 'balanced_subsample'\n",
    "        },\n",
    "        'cholesterol': {\n",
    "            'n_estimators': 800,\n",
    "            'max_depth': 22,\n",
    "            'min_samples_split': 3,\n",
    "            'class_weight': 'balanced_subsample'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for idx, disease in enumerate(y.columns, 1):\n",
    "        print(f\"\\n[{idx}/{total_diseases}] Processing {disease.upper()} model\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(f\"   • Getting target values for {disease}\")\n",
    "        y_disease = y[disease].values\n",
    "        \n",
    "        if disease in ['heart', 'hypertension', 'diabetes', 'cholesterol']:\n",
    "            print(\"   • Applying SMOTE + Tomek Links for balanced sampling\")\n",
    "            smote = SMOTE(random_state=42, sampling_strategy='auto')\n",
    "            tomek = TomekLinks(sampling_strategy='auto')\n",
    "            \n",
    "            print(\"     - Running SMOTE oversampling...\")\n",
    "            X_res, y_res = smote.fit_resample(X_scaled, y_disease)\n",
    "            print(f\"     ✓ Data resampled from {len(y_disease)} to {len(y_res)} samples\")\n",
    "            \n",
    "            print(\"     - Running Tomek Links undersampling...\")\n",
    "            X_res, y_res = tomek.fit_resample(X_res, y_res)\n",
    "            print(f\"     ✓ Final sample size: {len(y_res)}\")\n",
    "        else:\n",
    "            X_res, y_res = X_scaled, y_disease\n",
    "        \n",
    "        print(\"   • Setting up model parameters\")\n",
    "        current_params = base_params.copy()\n",
    "        if disease in disease_params:\n",
    "            current_params.update(disease_params[disease])\n",
    "            print(\"     ✓ Using disease-specific parameters\")\n",
    "        \n",
    "        if disease in ['heart', 'hypertension', 'diabetes', 'cholesterol']:\n",
    "            print(\"   • Running GridSearchCV for parameter optimization\")\n",
    "            param_grid = {\n",
    "                'n_estimators': [current_params['n_estimators'], current_params['n_estimators']+200],\n",
    "                'max_depth': [current_params['max_depth']-2, current_params['max_depth'], current_params['max_depth']+2],\n",
    "                'min_samples_split': [2, 3, 4],\n",
    "                'min_samples_leaf': [1, 2, 3]\n",
    "            }\n",
    "            \n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            rf = RandomForestClassifier(random_state=42)\n",
    "            grid_search = GridSearchCV(rf, param_grid, cv=cv, scoring='f1', n_jobs=-1)\n",
    "            \n",
    "            print(\"     - Training model with cross-validation...\")\n",
    "            grid_search.fit(X_res, y_res)\n",
    "            model = grid_search.best_estimator_\n",
    "            print(f\"     ✓ Best parameters found: {grid_search.best_params_}\")\n",
    "        else:\n",
    "            print(\"   • Training RandomForest model...\")\n",
    "            model = RandomForestClassifier(**current_params)\n",
    "            model.fit(X_res, y_res)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"   ✓ {disease.upper()} model completed in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        models[disease] = model\n",
    "    \n",
    "    print(\"\\n=== Model Training Completed ===\")\n",
    "    print(f\"Total time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "    return models, scaler\n",
    "\n",
    "# def save_models(models, scaler, save_dir='models'):\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "    \n",
    "#     for disease, model in models.items():\n",
    "#         model_path = os.path.join(save_dir, f'{disease}_model.joblib')\n",
    "#         joblib.dump(model, model_path)\n",
    "    \n",
    "#     scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "#     joblib.dump(scaler, scaler_path)\n",
    "#     print(f\"Models and scaler saved in {save_dir}/\")\n",
    "\n",
    "def save_models(models, scaler, save_dir='models'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    models_path = os.path.join(save_dir, 'disease-prediction-model.joblib')\n",
    "    joblib.dump(models, models_path)\n",
    "\n",
    "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Models saved in {models_path} and scaler saved in {scaler_path}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing data...\n",
      "Dataset loaded with shape: (160892, 22)\n",
      "Processing features...\n",
      "Handling missing values...\n",
      "Found 9 numeric features and 0 categorical features\n",
      "Imputing numeric features...\n",
      "Adding engineered features...\n",
      "Normalizing features...\n",
      "Processing target variables...\n",
      "Data processing completed!\n",
      "\n",
      "=== Starting Model Training Process ===\n",
      "1. Initializing StandardScaler...\n",
      "   ✓ Data scaling completed\n",
      "\n",
      "Total diseases to process: 10\n",
      "\n",
      "[1/10] Processing ANEMIA model\n",
      "   • Getting target values for anemia\n",
      "   • Setting up model parameters\n",
      "   • Training RandomForest model...\n",
      "   ✓ ANEMIA model completed in 20.77 seconds\n",
      "\n",
      "[2/10] Processing CHOLESTEROL model\n",
      "   • Getting target values for cholesterol\n",
      "   • Applying SMOTE + Tomek Links for balanced sampling\n",
      "     - Running SMOTE oversampling...\n",
      "     ✓ Data resampled from 128713 to 197584 samples\n",
      "     - Running Tomek Links undersampling...\n",
      "     ✓ Final sample size: 197036\n",
      "   • Setting up model parameters\n",
      "     ✓ Using disease-specific parameters\n",
      "   • Running GridSearchCV for parameter optimization\n",
      "     - Training model with cross-validation...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m     evaluate_model_accuracy(models, X_test, y_test)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 80\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     73\u001b[0m     X, y, \n\u001b[0;32m     74\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \n\u001b[0;32m     75\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     76\u001b[0m     stratify\u001b[38;5;241m=\u001b[39my[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheart\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Stratifikasi berdasarkan penyakit dengan imbalance terburuk\u001b[39;00m\n\u001b[0;32m     77\u001b[0m )\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m models, scaler \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_disease_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Evaluasi model\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHasil Evaluasi pada Data Testing:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 96\u001b[0m, in \u001b[0;36mtrain_disease_models\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     93\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(rf, param_grid, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m     - Training model with cross-validation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 96\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m     ✓ Best parameters found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    963\u001b[0m     )\n\u001b[1;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model_accuracy(models, X, y):\n",
    "    print(\"\\n=== Starting Model Evaluation ===\")\n",
    "    print(\"accuracy model eval:\\n\")\n",
    "    print(\"{:<15} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "        \"Disease\", \"Accurancy\", \"Precision\", \"Recall\", \"F1-Score\"\n",
    "    ))\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    overall_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'precision': 0,\n",
    "        'recall': 0,\n",
    "        'f1': 0\n",
    "    }\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    \n",
    "    total_models = len(models)\n",
    "    print(f\"Total models to evaluate: {total_models}\")\n",
    "    \n",
    "    for idx, (disease, model) in enumerate(models.items(), 1):\n",
    "        try:\n",
    "            print(f\"\\n[{idx}/{total_models}] Evaluating {disease.upper()} model...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            y_pred = model.predict(X)\n",
    "            y_true = y[disease]\n",
    "            \n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "            \n",
    "            overall_metrics['accuracy'] += accuracy\n",
    "            overall_metrics['precision'] += precision\n",
    "            overall_metrics['recall'] += recall\n",
    "            overall_metrics['f1'] += f1\n",
    "            \n",
    "            print(\"{:<15} {:<10.2f} {:<10.2f} {:<10.2f} {:<10.2f}\".format(\n",
    "                disease,\n",
    "                accuracy * 100,\n",
    "                precision * 100,\n",
    "                recall * 100,\n",
    "                f1 * 100\n",
    "            ))\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"✓ Evaluation completed in {elapsed_time:.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error evaluating {disease} model: {str(e)}\")\n",
    "    \n",
    "    n_models = len(models)\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    print(\"Rata-rata Metrik:\")\n",
    "    print(\"Akurasi   : {:.2f}%\".format(overall_metrics['accuracy'] / n_models * 100))\n",
    "    print(\"Presisi   : {:.2f}%\".format(overall_metrics['precision'] / n_models * 100))\n",
    "    print(\"Recall    : {:.2f}%\".format(overall_metrics['recall'] / n_models * 100))\n",
    "    print(\"F1-Score  : {:.2f}%\".format(overall_metrics['f1'] / n_models * 100))\n",
    "    print(\"\\n=== Evaluation Complete ===\")\n",
    "\n",
    "# Import tambahan yang diperlukan\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Cara penggunaan:\n",
    "def main():\n",
    "    # Load dan preprocess data\n",
    "    X, y = load_and_process_data()\n",
    "    \n",
    "    # Split data dengan stratifikasi\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=y['heart']  # Stratifikasi berdasarkan penyakit dengan imbalance terburuk\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    models, scaler = train_disease_models(X_train, y_train)\n",
    "    \n",
    "    # Evaluasi model\n",
    "    print(\"\\nHasil Evaluasi pada Data Testing:\")\n",
    "    evaluate_model_accuracy(models, X_test, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 load model\n",
    "def load_models(save_dir='models'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        raise FileNotFoundError(f\"Directory {save_dir} not found. Please train the models first.\")\n",
    "    \n",
    "    models = {}\n",
    "    for model_file in os.listdir(save_dir):\n",
    "        if model_file.endswith('_model.joblib'):\n",
    "            disease = model_file.replace('_model.joblib', '')\n",
    "            model_path = os.path.join(save_dir, model_file)\n",
    "            models[disease] = joblib.load(model_path)\n",
    "    \n",
    "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "    if not os.path.exists(scaler_path):\n",
    "        raise FileNotFoundError(\"Scaler file not found. Please train the models first.\")\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    \n",
    "    print(f\"Models and scaler loaded from {save_dir}/\")\n",
    "    return models, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6, predictin the data\n",
    "def predict_health_risks(user_input, models, scaler):\n",
    "    derived = calculate_derived_features(\n",
    "        user_input['height'],\n",
    "        user_input['weight'],\n",
    "        user_input['gender'],\n",
    "        user_input['age'],\n",
    "        user_input['blood_pressure'],\n",
    "        user_input['cholesterol'],\n",
    "        user_input['blood_glucose']\n",
    "    )\n",
    "    \n",
    "    features = pd.DataFrame([[\n",
    "        user_input['height'],\n",
    "        user_input['weight'],\n",
    "        user_input['gender'],\n",
    "        user_input['age'],\n",
    "        user_input['blood_pressure'],\n",
    "        user_input['cholesterol'],\n",
    "        user_input['blood_glucose'],\n",
    "        derived['bmi'],\n",
    "        derived['sodium']\n",
    "    ]])\n",
    "    \n",
    "    features = features.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    predictions = {}\n",
    "    for disease, model in models.items():\n",
    "        try:\n",
    "            proba = model.predict_proba(features_scaled)[0]\n",
    "            if len(proba) > 1:\n",
    "                prob = proba[1]\n",
    "            else:\n",
    "                prob = model.predict(features_scaled)[0]\n",
    "        except:\n",
    "            prob = model.predict(features_scaled)[0]\n",
    "        \n",
    "        predictions[disease] = float(prob) * 100\n",
    "    \n",
    "    return predictions, derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved in models\\disease-prediction-model.joblib and scaler saved in models\\scaler.joblib/\n",
      "Using average value for blood_pressure: 130.83\n",
      "Using average value for cholesterol: 98.87\n",
      "Using average value for blood_glucose: 105.84\n",
      "\n",
      "Derived Features:\n",
      "bmi: 23.44\n",
      "sodium: 1200.00\n",
      "fat: 9.00\n",
      "cholesterol_level: 208.53\n",
      "protein: 54.00\n",
      "carbs: 180.00\n",
      "\n",
      "Disease Risk Predictions:\n",
      "anemia: 0.00%\n",
      "cholesterol: 9.93%\n",
      "ckd: 10.35%\n",
      "diabetes: 2.23%\n",
      "heart: 34.67%\n",
      "hypertension: 5.31%\n",
      "ms: 7.88%\n",
      "nafld: 1.40%\n",
      "obesity: 0.00%\n",
      "stroke: 0.42%\n"
     ]
    }
   ],
   "source": [
    "# Step 7, here we can input the user data\n",
    "def get_average_values(df):\n",
    "    return {\n",
    "        'height': df['height'].mean(),\n",
    "        'weight': df['weight'].mean(),\n",
    "        'gender': round(df['gender'].mean()),\n",
    "        'age': df['age'].mean(),\n",
    "        'blood_pressure': df['bp'].mean(),\n",
    "        'cholesterol': df['bc'].mean(),\n",
    "        'blood_glucose': df['bg'].mean()\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('dataset/health_data1/combined_dataset.csv')\n",
    "    avg_values = get_average_values(df)\n",
    "    \n",
    "    X, y = load_and_process_data()\n",
    "    models, scaler = train_disease_models(X, y)\n",
    "    \n",
    "    save_models(models, scaler)\n",
    "    \n",
    "    # default param value is \"None\"\n",
    "    user_input = {\n",
    "        'height': 160,\n",
    "        'weight': 60,\n",
    "        'gender': 1, # 1=male, 0=female\n",
    "        'age': 20,\n",
    "        'blood_pressure': None,\n",
    "        'cholesterol': None,\n",
    "        'blood_glucose': None\n",
    "    }\n",
    "    \n",
    "    for key in user_input:\n",
    "        if user_input[key] is None:\n",
    "            if key == 'blood_pressure':\n",
    "                user_input[key] = avg_values['blood_pressure']\n",
    "            elif key == 'blood_glucose':\n",
    "                user_input[key] = avg_values['blood_glucose']\n",
    "            elif key == 'cholesterol':\n",
    "                user_input[key] = avg_values['cholesterol']\n",
    "            else:\n",
    "                user_input[key] = avg_values[key]\n",
    "            print(f\"Using average value for {key}: {user_input[key]:.2f}\")\n",
    "    \n",
    "    predictions, derived_features = predict_health_risks(user_input, models, scaler)\n",
    "    \n",
    "    print(\"\\nDerived Features:\")\n",
    "    for feature, value in derived_features.items():\n",
    "        print(f\"{feature}: {value:.2f}\")\n",
    "    \n",
    "    print(\"\\nDisease Risk Predictions:\")\n",
    "    for disease, risk in predictions.items():\n",
    "        print(f\"{disease}: {risk:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted and saved to models\\disease-prediction-model.hdf5 and parameters saved to models\\disease-prediction-model.json\n"
     ]
    }
   ],
   "source": [
    "# Step 8, convert saved model from joblib to hdf5\n",
    "# def convert_all_joblib_to_hdf5(models_dir='models'):\n",
    "#     for model_file in os.listdir(models_dir):\n",
    "#         if model_file.endswith('_model.joblib'):\n",
    "#             joblib_model_path = os.path.join(models_dir, model_file)\n",
    "#             hdf5_model_path = os.path.join(models_dir, model_file.replace('.joblib', '.hdf5'))\n",
    "            \n",
    "#             model = joblib.load(joblib_model_path)\n",
    "            \n",
    "#             model_params = {\n",
    "#                 'n_estimators': getattr(model, 'n_estimators', None),\n",
    "#                 'max_depth': getattr(model, 'max_depth', None),\n",
    "#                 'min_samples_split': getattr(model, 'min_samples_split', None),\n",
    "#                 'min_samples_leaf': getattr(model, 'min_samples_leaf', None),\n",
    "#                 'feature_importances': getattr(model, 'feature_importances_', None)\n",
    "#             }\n",
    "            \n",
    "#             with h5py.File(hdf5_model_path, 'w') as hdf:\n",
    "#                 for key, value in model_params.items():\n",
    "#                     if value is not None and (not isinstance(value, (list, np.ndarray)) or len(value) > 0):\n",
    "#                         hdf.create_dataset(key, data=value)\n",
    "#                     else:\n",
    "#                         print(f\"Warning: Parameter '{key}' is empty or None for model '{model_file}'. Skipping this parameter.\")\n",
    "            \n",
    "#             print(f\"Model '{model_file}' converted and saved to {hdf5_model_path}\")\n",
    "\n",
    "# def convert_all_joblib_to_json(models_dir='models'):\n",
    "#     for model_file in os.listdir(models_dir):\n",
    "#         if model_file.endswith('_model.joblib'):\n",
    "#             joblib_model_path = os.path.join(models_dir, model_file)\n",
    "#             json_model_path = os.path.join(models_dir, model_file.replace('.joblib', '.json'))\n",
    "            \n",
    "#             model = joblib.load(joblib_model_path)\n",
    "            \n",
    "#             model_params = {\n",
    "#                 'n_estimators': getattr(model, 'n_estimators', None),\n",
    "#                 'max_depth': getattr(model, 'max_depth', None),\n",
    "#                 'min_samples_split': getattr(model, 'min_samples_split', None),\n",
    "#                 'min_samples_leaf': getattr(model, 'min_samples_leaf', None),\n",
    "#                 'feature_importances': getattr(model, 'feature_importances_', None)\n",
    "#             }\n",
    "            \n",
    "#             for key, value in model_params.items():\n",
    "#                 if isinstance(value, (np.ndarray)):\n",
    "#                     model_params[key] = value.tolist()\n",
    "            \n",
    "#             with open(json_model_path, 'w') as json_file:\n",
    "#                 json.dump(model_params, json_file, indent=4)\n",
    "            \n",
    "#             print(f\"Model '{model_file}' converted and saved to {json_model_path}\")\n",
    "\n",
    "# convert_all_joblib_to_hdf5()\n",
    "# convert_all_joblib_to_json()\n",
    "\n",
    "def convert_model_to_hdf5_n_json(models_dir='models'):\n",
    "    models_path = os.path.join(models_dir, 'disease-prediction-model.joblib')\n",
    "    hdf5_model_path = os.path.join(models_dir, 'disease-prediction-model.hdf5')\n",
    "    json_model_path = os.path.join(models_dir, 'disease-prediction-model.json')\n",
    "\n",
    "    models = joblib.load(models_path)\n",
    "\n",
    "    with h5py.File(hdf5_model_path, 'w') as hdf:\n",
    "        model_params_dict = {}\n",
    "        for disease, model in models.items():\n",
    "            model_params = {\n",
    "                'n_estimators': getattr(model, 'n_estimators', None),\n",
    "                'max_depth': getattr(model, 'max_depth', None),\n",
    "                'min_samples_split': getattr(model, 'min_samples_split', None),\n",
    "                'min_samples_leaf': getattr(model, 'min_samples_leaf', None),\n",
    "                'feature_importances': getattr(model, 'feature_importances_', None)\n",
    "            }\n",
    "            model_params_dict[disease] = {key: value.tolist() if isinstance(value, np.ndarray) else value for key, value in model_params.items()}\n",
    "\n",
    "            for key, value in model_params.items():\n",
    "                if value is not None and (not isinstance(value, (list, np.ndarray)) or len(value) > 0):\n",
    "                    hdf.create_dataset(f\"{disease}/{key}\", data=value)\n",
    "                else:\n",
    "                    print(f\"Warning: Parameter '{key}' is empty or None for model '{disease}'. Skipping this parameter.\")\n",
    "\n",
    "    with open(json_model_path, 'w') as json_file:\n",
    "        json.dump(model_params_dict, json_file, indent=4)\n",
    "\n",
    "    print(f\"Model converted and saved to {hdf5_model_path} and parameters saved to {json_model_path}\")\n",
    "\n",
    "convert_model_to_hdf5_n_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_project_directory():\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    if os.path.basename(current_dir) == 'scripts':\n",
    "        os.chdir('..')\n",
    "    \n",
    "    print(f\"Working directory set to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(file_path='dataset/health_data1/combined_dataset.csv'):\n",
    "    print(\"Loading and processing data...\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    \n",
    "    available_features = ['height', 'weight', 'gender', 'age', 'bp', 'bc', 'bg', 'bmi', 'sodium']\n",
    "    target_variables = ['anemia', 'cholesterol', 'ckd', 'diabetes', 'heart',\n",
    "                       'hypertension', 'ms', 'nafld', 'obesity', 'stroke']\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    print(\"Processing features...\")\n",
    "    numeric_features = [col for col in available_features if df[col].dtype in ['int64', 'float64']]\n",
    "    categorical_features = [col for col in available_features if col not in numeric_features]\n",
    "    \n",
    "    print(\"Handling missing values...\")\n",
    "    print(f\"Found {len(numeric_features)} numeric features and {len(categorical_features)} categorical features\")\n",
    "    \n",
    "    numeric_imputer = IterativeImputer(random_state=42, max_iter=100, sample_posterior=True)\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    df_processed[numeric_features] = numeric_imputer.fit_transform(df_processed[numeric_features])\n",
    "    if categorical_features:\n",
    "        df_processed[categorical_features] = categorical_imputer.fit_transform(df_processed[categorical_features])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df_processed[numeric_features] = scaler.fit_transform(df_processed[numeric_features])\n",
    "    \n",
    "    df_processed = add_engineered_features(df_processed)\n",
    "    \n",
    "    final_features = (\n",
    "        available_features + \n",
    "        ['bmi_category', 'age_category', 'bp_category', \n",
    "         'bmi_age', 'bp_age', 'bmi_bp']\n",
    "    )\n",
    "    \n",
    "    print(\"Processing target variables...\")\n",
    "    df_processed[target_variables] = df[target_variables].fillna(0)\n",
    "    \n",
    "    print(\"Data processing completed!\")\n",
    "    return df_processed[final_features], df_processed[target_variables]\n",
    "\n",
    "def add_engineered_features(X):\n",
    "    X_new = X.copy()\n",
    "    \n",
    "    X_new['bmi_category'] = pd.cut(X_new['bmi'], \n",
    "                                  bins=[float('-inf'), 18.5, 25, 30, float('inf')],\n",
    "                                  labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['age_category'] = pd.cut(X_new['age'], \n",
    "                                  bins=[float('-inf'), 30, 45, 60, float('inf')],\n",
    "                                  labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['bp_category'] = pd.cut(X_new['bp'], \n",
    "                                 bins=[float('-inf'), 120, 140, 160, float('inf')],\n",
    "                                 labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['bmi_age'] = X_new['bmi'] * X_new['age']\n",
    "    X_new['bp_age'] = X_new['bp'] * X_new['age']\n",
    "    X_new['bmi_bp'] = X_new['bmi'] * X_new['bp']\n",
    "    \n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, name=None):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ], name=name)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall(),\n",
    "                tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_disease_models(X, y):\n",
    "    print(\"\\n=== Starting Model Training Process ===\")\n",
    "    models = {}\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X = np.array(X)\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # GPU Configuration\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Optimizer settings\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    \n",
    "    BATCH_SIZE = 2560\n",
    "    BUFFER_SIZE = 10000\n",
    "    \n",
    "    # Split data terlebih dahulu\n",
    "    X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "    y_train, y_val = train_test_split(y.values, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\\\n",
    "        .cache()\\\n",
    "        .shuffle(BUFFER_SIZE)\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\\\n",
    "        .cache()\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create model\n",
    "    combined_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(y.columns), activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    combined_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='./logs',\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Training\n",
    "    print(\"\\nStarting combined model training...\")\n",
    "    history = combined_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=50,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return combined_model, scaler, history\n",
    "\n",
    "def evaluate_model_accuracy(models, X, y):\n",
    "    print(\"\\n=== Starting Model Evaluation ===\")\n",
    "    print(\"Model Evaluation Results:\\n\")\n",
    "    print(\"{:<15} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "        \"Disease\", \"Accuracy\", \"Precision\", \"Recall\", \"AUC\"\n",
    "    ))\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    \n",
    "    total_models = len(models)\n",
    "    overall_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'auc': 0}\n",
    "    \n",
    "    for idx, (disease, model) in enumerate(models.items(), 1):\n",
    "        try:\n",
    "            print(f\"\\n[{idx}/{total_models}] Evaluating {disease.upper()} model...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            metrics = model.evaluate(X, y[disease].values, verbose=0)\n",
    "            \n",
    "            print(\"{:<15} {:<10.2f} {:<10.2f} {:<10.2f} {:<10.2f}\".format(\n",
    "                disease,\n",
    "                metrics[1] * 100,#acc\n",
    "                metrics[2] * 100,#prec\n",
    "                metrics[3] * 100,#recall\n",
    "                metrics[4] * 100 #auc\n",
    "            ))\n",
    "            \n",
    "            overall_metrics['accuracy'] += metrics[1]\n",
    "            overall_metrics['precision'] += metrics[2]\n",
    "            overall_metrics['recall'] += metrics[3]\n",
    "            overall_metrics['auc'] += metrics[4]\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"✓ Evaluation completed in {elapsed_time:.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error evaluating {disease} model: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    for metric, value in overall_metrics.items():\n",
    "        print(f\"{metric.capitalize():10}: {value/total_models*100:.2f}%\")\n",
    "    print(\"\\n=== Evaluation Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(model, scaler, save_dir='models'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    print(\"\\nSaving model...\")\n",
    "    \n",
    "    # Save combined model in HDF5 format\n",
    "    model_path = os.path.join(save_dir, 'disease-prediction-tf-model.h5')\n",
    "    model.save(model_path)\n",
    "    print(\"✓ Saved combined model in HDF5 format\")\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(\"✓ Saved scaler\")\n",
    "    \n",
    "    print(\"\\nAll models and scaler saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(save_dir='models'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        raise FileNotFoundError(f\"Directory {save_dir} not found\")\n",
    "    \n",
    "    print(\"\\nLoading model...\")\n",
    "    model_path = os.path.join(save_dir, 'disease-prediction-tf-model.h5')\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(\"✓ Loaded combined model from HDF5\")\n",
    "    \n",
    "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print(\"✓ Loaded scaler\")\n",
    "    \n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/health_data1/combined_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisease\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrisk\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Load the dataset and calculate average values\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset/health_data1/combined_dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     avg_values \u001b[38;5;241m=\u001b[39m get_average_values(df)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/health_data1/combined_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "def set_project_directory():\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    if os.path.basename(current_dir) == 'scripts':\n",
    "        os.chdir('..')\n",
    "    \n",
    "    print(f\"Working directory set to: {os.getcwd()}\")\n",
    "\n",
    "def load_and_process_data(file_path='dataset/health_data1/combined_dataset.csv'):\n",
    "    print(\"Loading and processing data...\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    \n",
    "    available_features = ['height', 'weight', 'gender', 'age', 'bp', 'bc', 'bg', 'bmi', 'sodium']\n",
    "    target_variables = ['anemia', 'cholesterol', 'ckd', 'diabetes', 'heart',\n",
    "                       'hypertension', 'ms', 'nafld', 'obesity', 'stroke']\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    print(\"Processing features...\")\n",
    "    numeric_features = [col for col in available_features if df[col].dtype in ['int64', 'float64']]\n",
    "    categorical_features = [col for col in available_features if col not in numeric_features]\n",
    "    \n",
    "    print(\"Handling missing values...\")\n",
    "    print(f\"Found {len(numeric_features)} numeric features and {len(categorical_features)} categorical features\")\n",
    "    \n",
    "    numeric_imputer = IterativeImputer(random_state=42, max_iter=100, sample_posterior=True)\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    df_processed[numeric_features] = numeric_imputer.fit_transform(df_processed[numeric_features])\n",
    "    if categorical_features:\n",
    "        df_processed[categorical_features] = categorical_imputer.fit_transform(df_processed[categorical_features])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df_processed[numeric_features] = scaler.fit_transform(df_processed[numeric_features])\n",
    "    \n",
    "    df_processed = add_engineered_features(df_processed)\n",
    "    \n",
    "    final_features = (\n",
    "        available_features + \n",
    "        ['bmi_category', 'age_category', 'bp_category', \n",
    "         'bmi_age', 'bp_age', 'bmi_bp']\n",
    "    )\n",
    "    \n",
    "    print(\"Processing target variables...\")\n",
    "    df_processed[target_variables] = df[target_variables].fillna(0)\n",
    "    \n",
    "    print(\"Data processing completed!\")\n",
    "    return df_processed[final_features], df_processed[target_variables]\n",
    "\n",
    "def add_engineered_features(X):\n",
    "    X_new = X.copy()\n",
    "    \n",
    "    X_new['bmi_category'] = pd.cut(X_new['bmi'], \n",
    "                                  bins=[float('-inf'), 18.5, 25, 30, float('inf')],\n",
    "                                  labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['age_category'] = pd.cut(X_new['age'], \n",
    "                                  bins=[float('-inf'), 30, 45, 60, float('inf')],\n",
    "                                  labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['bp_category'] = pd.cut(X_new['bp'], \n",
    "                                 bins=[float('-inf'), 120, 140, 160, float('inf')],\n",
    "                                 labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['bmi_age'] = X_new['bmi'] * X_new['age']\n",
    "    X_new['bp_age'] = X_new['bp'] * X_new['age']\n",
    "    X_new['bmi_bp'] = X_new['bmi'] * X_new['bp']\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "def create_model(input_shape, name=None):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ], name=name)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall(),\n",
    "                tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_disease_models(X, y):\n",
    "    print(\"\\n=== Starting Model Training Process ===\")\n",
    "    models = {}\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X = np.array(X)\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # GPU Configuration\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Optimizer settings\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    \n",
    "    BATCH_SIZE = 2560\n",
    "    BUFFER_SIZE = 10000\n",
    "    \n",
    "    # Split data terlebih dahulu\n",
    "    X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "    y_train, y_val = train_test_split(y.values, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\\\n",
    "        .cache()\\\n",
    "        .shuffle(BUFFER_SIZE)\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\\\n",
    "        .cache()\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create model\n",
    "    combined_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(y.columns), activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    combined_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='./logs',\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Training\n",
    "    print(\"\\nStarting combined model training...\")\n",
    "    history = combined_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=50,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return combined_model, scaler, history\n",
    "\n",
    "def evaluate_model_accuracy(models, X, y):\n",
    "    print(\"\\n=== Starting Model Evaluation ===\")\n",
    "    print(\"Model Evaluation Results:\\n\")\n",
    "    print(\"{:<15} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "        \"Disease\", \"Accuracy\", \"Precision\", \"Recall\", \"AUC\"\n",
    "    ))\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    \n",
    "    total_models = len(models)\n",
    "    overall_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'auc': 0}\n",
    "    \n",
    "    for idx, (disease, model) in enumerate(models.items(), 1):\n",
    "        try:\n",
    "            print(f\"\\n[{idx}/{total_models}] Evaluating {disease.upper()} model...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            metrics = model.evaluate(X, y[disease].values, verbose=0)\n",
    "            \n",
    "            print(\"{:<15} {:<10.2f} {:<10.2f} {:<10.2f} {:<10.2f}\".format(\n",
    "                disease,\n",
    "                metrics[1] * 100,#acc\n",
    "                metrics[2] * 100,#prec\n",
    "                metrics[3] * 100,#recall\n",
    "                metrics[4] * 100 #auc\n",
    "            ))\n",
    "            \n",
    "            overall_metrics['accuracy'] += metrics[1]\n",
    "            overall_metrics['precision'] += metrics[2]\n",
    "            overall_metrics['recall'] += metrics[3]\n",
    "            overall_metrics['auc'] += metrics[4]\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"✓ Evaluation completed in {elapsed_time:.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error evaluating {disease} model: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    for metric, value in overall_metrics.items():\n",
    "        print(f\"{metric.capitalize():10}: {value/total_models*100:.2f}%\")\n",
    "    print(\"\\n=== Evaluation Complete ===\")\n",
    "\n",
    "def save_models(model, scaler, save_dir='models'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    print(\"\\nSaving model...\")\n",
    "    \n",
    "    # Save combined model in HDF5 format\n",
    "    model_path = os.path.join(save_dir, 'disease-prediction-tf-model.h5')\n",
    "    model.save(model_path)\n",
    "    print(\"✓ Saved combined model in HDF5 format\")\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(\"✓ Saved scaler\")\n",
    "    \n",
    "    print(\"\\nAll models and scaler saved successfully!\")\n",
    "\n",
    "def load_models(save_dir='models'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        raise FileNotFoundError(f\"Directory {save_dir} not found\")\n",
    "    \n",
    "    print(\"\\nLoading model...\")\n",
    "    model_path = os.path.join(save_dir, 'disease-prediction-tf-model.h5')\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(\"✓ Loaded combined model from HDF5\")\n",
    "    \n",
    "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print(\"✓ Loaded scaler\")\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "def predict_diseases(input_data, models, scaler):\n",
    "    print(\"\\n--- Predicting Diseases ---\")\n",
    "    \n",
    "    # Ensure input_data is a DataFrame\n",
    "    if not isinstance(input_data, pd.DataFrame):\n",
    "        input_data = pd.DataFrame([input_data])\n",
    "    \n",
    "    # Scale input data\n",
    "    X_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    predictions = {}\n",
    "    for disease, model in models.items():\n",
    "        pred_prob = model.predict(X_scaled, verbose=0)[0][0]\n",
    "        predictions[disease] = {\n",
    "            'probability': float(pred_prob),\n",
    "            'prediction': 1 if pred_prob >= 0.5 else 0\n",
    "        }\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def predict_disease_risks(user_input, model, scaler):\n",
    "    # Ensure input_data is a DataFrame\n",
    "    if not isinstance(user_input, pd.DataFrame):\n",
    "        user_input = pd.DataFrame([user_input])\n",
    "    \n",
    "    # Scale input data\n",
    "    X_scaled = scaler.transform(user_input)\n",
    "    \n",
    "    # Predict probabilities for each disease\n",
    "    predictions = model.predict(X_scaled, verbose=0)[0]\n",
    "    \n",
    "    # Convert predictions to percentage\n",
    "    predictions_percent = {f'disease_{i+1}': prob * 100 for i, prob in enumerate(predictions)}\n",
    "    \n",
    "    return predictions_percent\n",
    "\n",
    "def get_average_values(df):\n",
    "    return {\n",
    "        'height': df['height'].mean(),\n",
    "        'weight': df['weight'].mean(),\n",
    "        'gender': round(df['gender'].mean()),\n",
    "        'age': df['age'].mean(),\n",
    "        'blood_pressure': df['bp'].mean(),\n",
    "        'cholesterol': df['bc'].mean(),\n",
    "        'blood_glucose': df['bg'].mean()\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Load the dataset and calculate average values\n",
    "    df = pd.read_csv('dataset/health_data1/combined_dataset.csv')\n",
    "    avg_values = get_average_values(df)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X, y = load_and_process_data()\n",
    "    models, scaler = train_disease_models(X, y)\n",
    "    \n",
    "    save_models(models, scaler)\n",
    "    \n",
    "    # Default user input with None values\n",
    "    user_input = {\n",
    "        'height': 160,\n",
    "        'weight': 60,\n",
    "        'gender': 1,  # 1=male, 0=female\n",
    "        'age': 20,\n",
    "        'blood_pressure': None,\n",
    "        'cholesterol': None,\n",
    "        'blood_glucose': None\n",
    "    }\n",
    "    \n",
    "    # Fill missing user input values with averages\n",
    "    for key in user_input:\n",
    "        if user_input[key] is None:\n",
    "            user_input[key] = avg_values[key]\n",
    "            print(f\"Using average value for {key}: {user_input[key]:.2f}\")\n",
    "    \n",
    "    # Add derived features to user input\n",
    "    derived_features = calculate_derived_features(\n",
    "        user_input['height'],\n",
    "        user_input['weight'],\n",
    "        user_input['gender'],\n",
    "        user_input['age'],\n",
    "        user_input['blood_pressure'],\n",
    "        user_input['cholesterol'],\n",
    "        user_input['blood_glucose']\n",
    "    )\n",
    "    \n",
    "    # Combine user input and derived features\n",
    "    user_input.update(derived_features)\n",
    "    \n",
    "    # Ensure the input data is a DataFrame with the correct columns\n",
    "    input_df = pd.DataFrame([user_input])\n",
    "    \n",
    "    # Predict health risks\n",
    "    predictions = predict_disease_risks(input_df, models, scaler)\n",
    "    \n",
    "    # Display derived features and predictions\n",
    "    print(\"\\nDerived Features:\")\n",
    "    for feature, value in derived_features.items():\n",
    "        print(f\"{feature}: {value:.2f}\")\n",
    "    \n",
    "    print(\"\\nDisease Risk Predictions:\")\n",
    "    for disease, risk in predictions.items():\n",
    "        print(f\"{disease}: {risk:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
