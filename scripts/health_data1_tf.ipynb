{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Working directory set to: c:\\Users\\Dana\\Documents\\Kuliah\\Bangkit\\Capstone-C242-PS384_Project01\n",
      "\n",
      "Available files in directory:\n",
      "- anemia-dataset.csv\n",
      "- cholesterol-dataset.csv\n",
      "- chronic-kidney-disease-dataset.csv\n",
      "- combined_dataset.csv\n",
      "- diabetes-dataset.csv\n",
      "- health_data1_combined.csv\n",
      "- heart-disease-dataset.csv\n",
      "- hypertension-dataset.csv\n",
      "- metabolic-syndrome-dataset.csv\n",
      "- nafld1-dataset.csv\n",
      "- nafld2-dataset.csv\n",
      "- nwtco-dataset.csv\n",
      "- obesity-dataset.csv\n",
      "- stroke-dataset.csv\n",
      "\n",
      "Processing: anemia-dataset.csv\n",
      "Successfully loaded anemia-dataset.csv\n",
      "First few rows:\n",
      "   Gender  Hemoglobin   MCH  MCHC   MCV  Result\n",
      "0       1        14.9  22.7  29.1  83.7       0\n",
      "1       0        15.9  25.4  28.3  72.0       0\n",
      "2       0         9.0  21.5  29.6  71.2       1\n",
      "3       0        14.9  16.0  31.4  87.5       0\n",
      "4       1        14.7  22.0  28.2  99.5       0\n",
      "\n",
      "Processing: cholesterol-dataset.csv\n",
      "Successfully loaded cholesterol-dataset.csv\n",
      "First few rows:\n",
      "   age  sex  cp  trestbps  fbs  restecg  thalach  exang  oldpeak  slope ca  \\\n",
      "0   63    1   1       145    1        2      150      0      2.3      3  0   \n",
      "1   67    1   4       160    0        2      108      1      1.5      2  3   \n",
      "2   67    1   4       120    0        2      129      1      2.6      2  2   \n",
      "3   37    1   3       130    0        0      187      0      3.5      3  0   \n",
      "4   41    0   2       130    0        2      172      0      1.4      1  0   \n",
      "\n",
      "  thal  num  chol  \n",
      "0    6    0   233  \n",
      "1    3    2   286  \n",
      "2    7    1   229  \n",
      "3    3    0   250  \n",
      "4    3    0   204  \n",
      "\n",
      "Processing: chronic-kidney-disease-dataset.csv\n",
      "Successfully loaded chronic-kidney-disease-dataset.csv\n",
      "First few rows:\n",
      "   id   age    bp     sg   al   su     rbc        pc         pcc          ba  \\\n",
      "0   0  48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "1   1   7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "2   2  62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
      "3   3  48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
      "4   4  51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
      "\n",
      "   ...  pcv    wc   rc  htn   dm  cad appet   pe  ane classification  \n",
      "0  ...   44  7800  5.2  yes  yes   no  good   no   no            ckd  \n",
      "1  ...   38  6000  NaN   no   no   no  good   no   no            ckd  \n",
      "2  ...   31  7500  NaN   no  yes   no  poor   no  yes            ckd  \n",
      "3  ...   32  6700  3.9  yes   no   no  poor  yes  yes            ckd  \n",
      "4  ...   35  7300  4.6   no   no   no  good   no   no            ckd  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "Processing: combined_dataset.csv\n",
      "Successfully loaded combined_dataset.csv\n",
      "First few rows:\n",
      "   height  weight  gender  age  bp  bc     bg  bmi  sodium  fat  ...  anemia  \\\n",
      "0     NaN     NaN     0.0  NaN NaN NaN  104.3  NaN     NaN  NaN  ...     0.0   \n",
      "1     NaN     NaN     0.0  NaN NaN NaN  111.3  NaN     NaN  NaN  ...     0.0   \n",
      "2     NaN     NaN     0.0  NaN NaN NaN   63.0  NaN     NaN  NaN  ...     1.0   \n",
      "3     NaN     NaN     0.0  NaN NaN NaN  104.3  NaN     NaN  NaN  ...     0.0   \n",
      "4     NaN     NaN     0.0  NaN NaN NaN  102.9  NaN     NaN  NaN  ...     0.0   \n",
      "\n",
      "   cholesterol  ckd  diabetes  heart  hypertension  ms  nafld  obesity  stroke  \n",
      "0          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "1          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "2          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "3          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "4          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Processing: diabetes-dataset.csv\n",
      "Successfully loaded diabetes-dataset.csv\n",
      "First few rows:\n",
      "    Age  Sex  HighChol  CholCheck   BMI  Smoker  HeartDiseaseorAttack  \\\n",
      "0   4.0  1.0       0.0        1.0  26.0     0.0                   0.0   \n",
      "1  12.0  1.0       1.0        1.0  26.0     1.0                   0.0   \n",
      "2  13.0  1.0       0.0        1.0  26.0     0.0                   0.0   \n",
      "3  11.0  1.0       1.0        1.0  28.0     1.0                   0.0   \n",
      "4   8.0  0.0       0.0        1.0  29.0     1.0                   0.0   \n",
      "\n",
      "   PhysActivity  Fruits  Veggies  HvyAlcoholConsump  GenHlth  MentHlth  \\\n",
      "0           1.0     0.0      1.0                0.0      3.0       5.0   \n",
      "1           0.0     1.0      0.0                0.0      3.0       0.0   \n",
      "2           1.0     1.0      1.0                0.0      1.0       0.0   \n",
      "3           1.0     1.0      1.0                0.0      3.0       0.0   \n",
      "4           1.0     1.0      1.0                0.0      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Stroke  HighBP  Diabetes  \n",
      "0      30.0       0.0     0.0     1.0       0.0  \n",
      "1       0.0       0.0     1.0     1.0       0.0  \n",
      "2      10.0       0.0     0.0     0.0       0.0  \n",
      "3       3.0       0.0     0.0     1.0       0.0  \n",
      "4       0.0       0.0     0.0     0.0       0.0  \n",
      "\n",
      "Processing: health_data1_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dana\\AppData\\Local\\Temp\\ipykernel_19628\\2548305139.py:42: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded health_data1_combined.csv\n",
      "First few rows:\n",
      "  gender  hemoglobin  age  blood_pressure  cholesterol  glucose  bmi  height  \\\n",
      "0    1.0        14.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "1    0.0        15.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "2    0.0         9.0  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "3    0.0        14.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "4    1.0        14.7  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "\n",
      "   weight  HDL  Height  Weight  \n",
      "0     NaN  NaN     NaN     NaN  \n",
      "1     NaN  NaN     NaN     NaN  \n",
      "2     NaN  NaN     NaN     NaN  \n",
      "3     NaN  NaN     NaN     NaN  \n",
      "4     NaN  NaN     NaN     NaN  \n",
      "\n",
      "Processing: heart-disease-dataset.csv\n",
      "Successfully loaded heart-disease-dataset.csv\n",
      "First few rows:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
      "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
      "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
      "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
      "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   2     3       0  \n",
      "1   0     3       0  \n",
      "2   0     3       0  \n",
      "3   1     3       0  \n",
      "4   3     2       0  \n",
      "\n",
      "Processing: hypertension-dataset.csv\n",
      "Successfully loaded hypertension-dataset.csv\n",
      "First few rows:\n",
      "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0  57.0  1.0   3       145   233    1        0      150      0      2.3   \n",
      "1  64.0  0.0   2       130   250    0        1      187      0      3.5   \n",
      "2  52.0  1.0   1       130   204    0        0      172      0      1.4   \n",
      "3  56.0  0.0   1       120   236    0        1      178      0      0.8   \n",
      "4  66.0  0.0   0       120   354    0        1      163      1      0.6   \n",
      "\n",
      "   slope  ca  thal  target  \n",
      "0      0   0     1       1  \n",
      "1      0   0     2       1  \n",
      "2      2   0     2       1  \n",
      "3      2   0     2       1  \n",
      "4      2   0     2       1  \n",
      "\n",
      "Processing: metabolic-syndrome-dataset.csv\n",
      "Successfully loaded metabolic-syndrome-dataset.csv\n",
      "First few rows:\n",
      "    seqn  Age     Sex  Marital  Income   Race  WaistCirc   BMI  Albuminuria  \\\n",
      "0  62161   22    Male   Single  8200.0  White       81.0  23.3            0   \n",
      "1  62164   44  Female  Married  4500.0  White       80.1  23.2            0   \n",
      "2  62169   21    Male   Single   800.0  Asian       69.6  20.1            0   \n",
      "3  62172   43  Female   Single  2000.0  Black      120.4  33.3            0   \n",
      "4  62177   51    Male  Married     NaN  Asian       81.1  20.1            0   \n",
      "\n",
      "   UrAlbCr  UricAcid  BloodGlucose  HDL  Triglycerides  MetabolicSyndrome  \n",
      "0     3.88       4.9            92   41             84                  0  \n",
      "1     8.55       4.5            82   28             56                  0  \n",
      "2     5.07       5.4           107   43             78                  0  \n",
      "3     5.22       5.0           104   73            141                  0  \n",
      "4     8.13       5.0            95   43            126                  0  \n",
      "\n",
      "Processing: nafld1-dataset.csv\n",
      "Successfully loaded nafld1-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  id  age  male  weight  height        bmi  case.id  futime  \\\n",
      "0        3631   1   57     0    60.0   163.0  22.690939  10630.0    6261   \n",
      "1        8458   2   67     0    70.4   168.0  24.884028  14817.0     624   \n",
      "2        6298   3   53     1   105.8   186.0  30.453537      3.0    1783   \n",
      "3       15398   4   56     1   109.3   170.0  37.830100   6628.0    3143   \n",
      "4       13261   5   68     1     NaN     NaN        NaN   1871.0    1836   \n",
      "\n",
      "   status  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       1  \n",
      "\n",
      "Processing: nafld2-dataset.csv\n",
      "Successfully loaded nafld2-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  id  days  test  value\n",
      "0      135077   1  -459   hdl   75.0\n",
      "1      313143   1  -459  chol   75.0\n",
      "2      135078   1   183   hdl   64.0\n",
      "3      313144   1   183  chol   64.0\n",
      "4      135079   1  2030   hdl   74.0\n",
      "\n",
      "Processing: nwtco-dataset.csv\n",
      "Successfully loaded nwtco-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  seqno  instit  histol  stage  study  rel  edrel  age  \\\n",
      "0           1      1       2       2      1      3    0   6075   25   \n",
      "1           2      2       1       1      2      3    0   4121   50   \n",
      "2           3      3       2       2      1      3    0   6069    9   \n",
      "3           4      4       2       1      4      3    0   6200   28   \n",
      "4           5      5       2       2      2      3    0   1244   55   \n",
      "\n",
      "   in.subcohort  \n",
      "0         False  \n",
      "1         False  \n",
      "2         False  \n",
      "3          True  \n",
      "4         False  \n",
      "\n",
      "Processing: obesity-dataset.csv\n",
      "Successfully loaded obesity-dataset.csv\n",
      "First few rows:\n",
      "   ID  Age  Gender  Height  Weight   BMI          Label\n",
      "0   1   25    Male     175      80  25.3  Normal Weight\n",
      "1   2   30  Female     160      60  22.5  Normal Weight\n",
      "2   3   35    Male     180      90  27.3     Overweight\n",
      "3   4   40  Female     150      50  20.0    Underweight\n",
      "4   5   45    Male     190     100  31.2          Obese\n",
      "\n",
      "Processing: stroke-dataset.csv\n",
      "Successfully loaded stroke-dataset.csv\n",
      "First few rows:\n",
      "   sex   age  hypertension  heart_disease  ever_married  work_type  \\\n",
      "0  1.0  63.0             0              1             1          4   \n",
      "1  1.0  42.0             0              1             1          4   \n",
      "2  0.0  61.0             0              0             1          4   \n",
      "3  1.0  41.0             1              0             1          3   \n",
      "4  1.0  85.0             0              0             1          4   \n",
      "\n",
      "   Residence_type  avg_glucose_level   bmi  smoking_status  stroke  \n",
      "0               1             228.69  36.6               1       1  \n",
      "1               0             105.92  32.5               0       1  \n",
      "2               1             171.23  34.4               1       1  \n",
      "3               0             174.12  24.0               0       1  \n",
      "4               1             186.21  29.0               1       1  \n",
      "File inside the dataset folder: ['anemia-dataset.csv', 'cholesterol-dataset.csv', 'chronic-kidney-disease-dataset.csv', 'combined_dataset.csv', 'diabetes-dataset.csv', 'health_data1_combined.csv', 'heart-disease-dataset.csv', 'hypertension-dataset.csv', 'metabolic-syndrome-dataset.csv', 'nafld1-dataset.csv', 'nafld2-dataset.csv', 'nwtco-dataset.csv', 'obesity-dataset.csv', 'stroke-dataset.csv']\n",
      "All files are available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Step 0, chek dataset availability\n",
    "\n",
    "def set_project_directory():\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    if os.path.basename(current_dir) == 'scripts':\n",
    "        os.chdir('..')\n",
    "    \n",
    "    print(f\"Working directory set to: {os.getcwd()}\")\n",
    "\n",
    "def check_data_directory():\n",
    "    data_dir = os.path.join('dataset', 'health_data1')\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Directory not found: {data_dir}\")\n",
    "        print(\"Available directories:\", os.listdir('.'))\n",
    "        return False\n",
    "    \n",
    "    print(\"\\nAvailable files in directory:\")\n",
    "    for file in os.listdir(data_dir):\n",
    "        print(f\"- {file}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            return pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.XPT'):\n",
    "            return pd.read_sas(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_path}\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "set_project_directory()\n",
    "\n",
    "if check_data_directory():\n",
    "    data_directory = os.path.join('dataset', 'health_data1')\n",
    "    \n",
    "    for filename in os.listdir(data_directory):\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        data = load_data(file_path)\n",
    "        if data is not None:\n",
    "            print(f\"Successfully loaded {filename}\")\n",
    "            print(\"First few rows:\")\n",
    "            print(data.head())\n",
    "else:\n",
    "    print(\"Please check your directory structure and file locations\")\n",
    "\n",
    "folder_path = 'dataset/health_data1/'\n",
    "\n",
    "try:\n",
    "    print(\"File inside the dataset folder:\", os.listdir(folder_path))\n",
    "    required_files = [\n",
    "        'anemia-dataset.csv',\n",
    "        'cholesterol-dataset.csv',\n",
    "        'chronic-kidney-disease-dataset.csv',\n",
    "        'diabetes-dataset.csv',\n",
    "        'heart-disease-dataset.csv',\n",
    "        'hypertension-dataset.csv',\n",
    "        'metabolic-syndrome-dataset.csv',\n",
    "        'nafld1-dataset.csv',\n",
    "        'obesity-dataset.csv',\n",
    "        'stroke-dataset.csv'\n",
    "    ]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"Missing files: {', '.join(missing_files)}\")\n",
    "    else:\n",
    "        print(\"All files are available.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Folder missing: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 148\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# 10. Stroke dataset\u001b[39;00m\n\u001b[0;32m    147\u001b[0m stroke_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstroke-dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 148\u001b[0m all_data\u001b[38;5;241m.\u001b[39mextend([\n\u001b[0;32m    149\u001b[0m     create_data_dict(\n\u001b[0;32m    150\u001b[0m         age\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    151\u001b[0m         gender\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMale\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    152\u001b[0m         bc\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheart_disease\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    153\u001b[0m         bmi\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbmi\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    154\u001b[0m         heart\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheart_disease\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    155\u001b[0m         hypertension\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhypertension\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    156\u001b[0m         stroke\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstroke\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m stroke_data\u001b[38;5;241m.\u001b[39miterrows()\n\u001b[0;32m    159\u001b[0m ])\n\u001b[0;32m    161\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_data)\n\u001b[0;32m    163\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 148\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# 10. Stroke dataset\u001b[39;00m\n\u001b[0;32m    147\u001b[0m stroke_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstroke-dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 148\u001b[0m all_data\u001b[38;5;241m.\u001b[39mextend([\n\u001b[0;32m    149\u001b[0m     create_data_dict(\n\u001b[0;32m    150\u001b[0m         age\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    151\u001b[0m         gender\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMale\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    152\u001b[0m         bc\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheart_disease\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    153\u001b[0m         bmi\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbmi\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    154\u001b[0m         heart\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheart_disease\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    155\u001b[0m         hypertension\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhypertension\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    156\u001b[0m         stroke\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstroke\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m stroke_data\u001b[38;5;241m.\u001b[39miterrows()\n\u001b[0;32m    159\u001b[0m ])\n\u001b[0;32m    161\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_data)\n\u001b[0;32m    163\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\frame.py:1554\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m-> 1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[0;32m   1556\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\pandas\\core\\construction.py:600\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# GH#846\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    601\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mA\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# preprocess available data and merge it in the end\n",
    "columns = [\n",
    "    \"height\", \"weight\", \"gender\", \"age\", \"bp\", \"bc\", \"bg\", \"bmi\", \"sodium\", \n",
    "    \"fat\", \"protein\", \"carbs\", \"anemia\", \"cholesterol\", \"ckd\", \"diabetes\", \n",
    "    \"heart\", \"hypertension\", \"ms\", \"nafld\", \"obesity\", \"stroke\"\n",
    "]\n",
    "\n",
    "def create_data_dict(**kwargs):\n",
    "    base_dict = {\n",
    "        \"height\": np.nan, \"weight\": np.nan, \"gender\": np.nan, \"age\": np.nan,\n",
    "        \"bp\": np.nan, \"bc\": np.nan, \"bg\": np.nan, \"bmi\": np.nan,\n",
    "        \"sodium\": np.nan, \"fat\": np.nan, \"protein\": np.nan, \"carbs\": np.nan,\n",
    "        \"anemia\": 0, \"cholesterol\": 0, \"ckd\": 0, \"diabetes\": 0,\n",
    "        \"heart\": 0, \"hypertension\": 0, \"ms\": 0, \"nafld\": 0, \"obesity\": 0, \"stroke\": 0\n",
    "    }\n",
    "    base_dict.update({k: v for k, v in kwargs.items() if v is not None})\n",
    "    return base_dict\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# 1. Anemia dataset\n",
    "anemia_data = pd.read_csv(os.path.join(folder_path, \"anemia-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        gender=1 if row[\"Gender\"] == \"Male\" else 0,\n",
    "        bg=round(row[\"Hemoglobin\"] * 7, 1),\n",
    "        anemia=row[\"Result\"]\n",
    "    )\n",
    "    for _, row in anemia_data.iterrows()\n",
    "])\n",
    "\n",
    "# 2. Cholesterol dataset\n",
    "chol_data = pd.read_csv(os.path.join(folder_path, \"cholesterol-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=row[\"sex\"],\n",
    "        bp=row[\"trestbps\"],\n",
    "        bc=row[\"chol\"],\n",
    "        bg=120 if row[\"fbs\"] == 1 else 100,\n",
    "        cholesterol=1 if row[\"chol\"] > 240 else 0\n",
    "    )\n",
    "    for _, row in chol_data.iterrows()\n",
    "])\n",
    "\n",
    "# 3. Chronic Kidney Disease dataset\n",
    "ckd_data = pd.read_csv(os.path.join(folder_path, \"chronic-kidney-disease-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        bp=row[\"bp\"],\n",
    "        bg=row[\"bgr\"],\n",
    "        sodium=row[\"sod\"],\n",
    "        anemia=1 if row[\"ane\"] == \"yes\" else 0,\n",
    "        ckd=1 if row[\"classification\"] == \"ckd\" else 0,\n",
    "        diabetes=1 if row[\"dm\"] == \"yes\" else 0,\n",
    "        heart=1 if row[\"cad\"] == \"yes\" else 0,\n",
    "        hypertension=1 if row[\"htn\"] == \"yes\" else 0\n",
    "    )\n",
    "    for _, row in ckd_data.iterrows()\n",
    "])\n",
    "\n",
    "# 4. Diabetes dataset\n",
    "diabetes_data = pd.read_csv(os.path.join(folder_path, \"diabetes-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"Age\"],\n",
    "        gender=1 if row[\"Sex\"] == \"Male\" else 0,\n",
    "        bmi=row[\"BMI\"],\n",
    "        cholesterol=row[\"HighChol\"],\n",
    "        diabetes=1 if row[\"Diabetes\"] == 1 else 0,\n",
    "        hypertension=row[\"HighBP\"]\n",
    "    )\n",
    "    for _, row in diabetes_data.iterrows()\n",
    "])\n",
    "\n",
    "# 5. Heart Disease dataset\n",
    "heart_data = pd.read_csv(os.path.join(folder_path, \"heart-disease-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=1 if row[\"sex\"] == 1 else 0,\n",
    "        bp=row[\"trestbps\"],\n",
    "        bc=row[\"chol\"],\n",
    "        bg=120 if row[\"fbs\"] == 1 else 100,\n",
    "        heart=1 if row[\"target\"] == 1 else 0\n",
    "    )\n",
    "    for _, row in heart_data.iterrows()\n",
    "])\n",
    "\n",
    "# 6. Hypertension dataset\n",
    "hypertension_data = pd.read_csv(os.path.join(folder_path, \"hypertension-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=1 if row[\"sex\"] == 1 else 0,\n",
    "        bp=row[\"trestbps\"],\n",
    "        bc=row[\"chol\"],\n",
    "        bg=row[\"fbs\"],\n",
    "        hypertension=1 if row[\"target\"] == 1 else 0\n",
    "    )\n",
    "    for _, row in hypertension_data.iterrows()\n",
    "])\n",
    "\n",
    "# 7. Metabolic Syndrome dataset\n",
    "ms_data = pd.read_csv(os.path.join(folder_path, \"metabolic-syndrome-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"Age\"],\n",
    "        gender=1 if row[\"Sex\"] == \"Male\" else 0,\n",
    "        bg=row[\"BloodGlucose\"],\n",
    "        bmi=row[\"BMI\"],\n",
    "        ms=1 if row[\"MetabolicSyndrome\"] == 1 else 0\n",
    "    )\n",
    "    for _, row in ms_data.iterrows()\n",
    "])\n",
    "\n",
    "# 8. NAFLD dataset\n",
    "nafld_data = pd.read_csv(os.path.join(folder_path, \"nafld1-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=row[\"male\"],\n",
    "        weight=row[\"weight\"],\n",
    "        height=row[\"height\"],\n",
    "        bmi=round(row[\"bmi\"],1),\n",
    "        nafld=row[\"status\"]\n",
    "    )\n",
    "    for _, row in nafld_data.iterrows()\n",
    "])\n",
    "\n",
    "# 9. Obesity dataset\n",
    "obesity_data = pd.read_csv(os.path.join(folder_path, \"obesity-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"Age\"],\n",
    "        gender=1 if row[\"Gender\"] == \"Male\" else 0,\n",
    "        weight=row[\"Weight\"],\n",
    "        height=row[\"Height\"],\n",
    "        bmi=row[\"BMI\"],\n",
    "        obesity=1 if row[\"Label\"] == \"Obesity\" else 0\n",
    "    )\n",
    "    for _, row in obesity_data.iterrows()\n",
    "])\n",
    "\n",
    "# 10. Stroke dataset\n",
    "stroke_data = pd.read_csv(os.path.join(folder_path, \"stroke-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=1 if row[\"sex\"] == \"Male\" else 0,\n",
    "        bc=row[\"heart_disease\"],\n",
    "        bmi=row[\"bmi\"],\n",
    "        heart=row[\"heart_disease\"],\n",
    "        hypertension=row[\"hypertension\"],\n",
    "        stroke=row[\"stroke\"]\n",
    "    )\n",
    "    for _, row in stroke_data.iterrows()\n",
    "])\n",
    "\n",
    "combined_data = pd.DataFrame(all_data)\n",
    "\n",
    "output_path = os.path.join(folder_path, \"combined_dataset.csv\")\n",
    "combined_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Combined dataset succesfully savd on {output_path}\")\n",
    "print(\"\\ncombined dataset inform:\")\n",
    "print(combined_data.info())\n",
    "print(\"\\ncombined dataset stats:\")\n",
    "print(combined_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   height  weight  gender  age  bp  bc     bg  bmi  sodium  fat  ...  anemia  \\\n",
      "0     NaN     NaN     0.0  NaN NaN NaN  104.3  NaN     NaN  NaN  ...     0.0   \n",
      "1     NaN     NaN     0.0  NaN NaN NaN  111.3  NaN     NaN  NaN  ...     0.0   \n",
      "2     NaN     NaN     0.0  NaN NaN NaN   63.0  NaN     NaN  NaN  ...     1.0   \n",
      "3     NaN     NaN     0.0  NaN NaN NaN  104.3  NaN     NaN  NaN  ...     0.0   \n",
      "4     NaN     NaN     0.0  NaN NaN NaN  102.9  NaN     NaN  NaN  ...     0.0   \n",
      "\n",
      "   cholesterol  ckd  diabetes  heart  hypertension  ms  nafld  obesity  stroke  \n",
      "0          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "1          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "2          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "3          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "4          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Shape of the dataset:\n",
      "(160892, 22)\n",
      "\n",
      "Columns in the dataset:\n",
      "Index(['height', 'weight', 'gender', 'age', 'bp', 'bc', 'bg', 'bmi', 'sodium',\n",
      "       'fat', 'protein', 'carbs', 'anemia', 'cholesterol', 'ckd', 'diabetes',\n",
      "       'heart', 'hypertension', 'ms', 'nafld', 'obesity', 'stroke'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the combined dataset\n",
    "file_path = 'dataset/health_data1/combined_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and the shape of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nShape of the dataset:\")\n",
    "print(df.shape)\n",
    "\n",
    "# Display the columns to see the features\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing data...\n",
      "Dataset loaded with shape: (160892, 22)\n",
      "Processing features...\n",
      "Handling missing values...\n",
      "Found 9 numeric features and 0 categorical features\n",
      "Processing target variables...\n",
      "Data processing completed!\n",
      "\n",
      "=== Starting Model Training Process ===\n",
      "\n",
      "Starting combined model training...\n",
      "Epoch 1/50\n",
      "51/51 [==============================] - 5s 46ms/step - loss: 0.3247 - accuracy: 0.2570 - precision: 0.3162 - recall: 0.3886 - auc: 0.8223 - val_loss: 0.3272 - val_accuracy: 0.2544 - val_precision: 0.5870 - val_recall: 0.2191 - val_auc: 0.9280 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1837 - accuracy: 0.2492 - precision: 0.5617 - recall: 0.4218 - auc: 0.9331 - val_loss: 0.2230 - val_accuracy: 0.2490 - val_precision: 0.6207 - val_recall: 0.1795 - val_auc: 0.9372 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1723 - accuracy: 0.2516 - precision: 0.5888 - recall: 0.4576 - auc: 0.9413 - val_loss: 0.1935 - val_accuracy: 0.2369 - val_precision: 0.5707 - val_recall: 0.2542 - val_auc: 0.9375 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1651 - accuracy: 0.2576 - precision: 0.6121 - recall: 0.5197 - auc: 0.9468 - val_loss: 0.1893 - val_accuracy: 0.2100 - val_precision: 0.4853 - val_recall: 0.2556 - val_auc: 0.9312 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "51/51 [==============================] - 2s 31ms/step - loss: 0.1614 - accuracy: 0.2566 - precision: 0.6230 - recall: 0.5472 - auc: 0.9495 - val_loss: 0.1803 - val_accuracy: 0.2167 - val_precision: 0.5039 - val_recall: 0.3894 - val_auc: 0.9354 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1595 - accuracy: 0.2553 - precision: 0.6239 - recall: 0.5552 - auc: 0.9507 - val_loss: 0.1734 - val_accuracy: 0.2100 - val_precision: 0.5343 - val_recall: 0.4858 - val_auc: 0.9405 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1581 - accuracy: 0.2502 - precision: 0.6288 - recall: 0.5587 - auc: 0.9518 - val_loss: 0.1667 - val_accuracy: 0.2172 - val_precision: 0.5535 - val_recall: 0.5822 - val_auc: 0.9456 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "51/51 [==============================] - 2s 35ms/step - loss: 0.1567 - accuracy: 0.2507 - precision: 0.6315 - recall: 0.5613 - auc: 0.9529 - val_loss: 0.1630 - val_accuracy: 0.2080 - val_precision: 0.5656 - val_recall: 0.6447 - val_auc: 0.9491 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1555 - accuracy: 0.2497 - precision: 0.6312 - recall: 0.5681 - auc: 0.9538 - val_loss: 0.1602 - val_accuracy: 0.2085 - val_precision: 0.5712 - val_recall: 0.6959 - val_auc: 0.9513 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1546 - accuracy: 0.2491 - precision: 0.6319 - recall: 0.5734 - auc: 0.9544 - val_loss: 0.1562 - val_accuracy: 0.2101 - val_precision: 0.5895 - val_recall: 0.6764 - val_auc: 0.9536 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1534 - accuracy: 0.2490 - precision: 0.6329 - recall: 0.5793 - auc: 0.9551 - val_loss: 0.1535 - val_accuracy: 0.2226 - val_precision: 0.5971 - val_recall: 0.6952 - val_auc: 0.9554 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1528 - accuracy: 0.2538 - precision: 0.6322 - recall: 0.5837 - auc: 0.9554 - val_loss: 0.1520 - val_accuracy: 0.2181 - val_precision: 0.5990 - val_recall: 0.7109 - val_auc: 0.9563 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "51/51 [==============================] - 2s 31ms/step - loss: 0.1520 - accuracy: 0.2542 - precision: 0.6322 - recall: 0.5875 - auc: 0.9559 - val_loss: 0.1503 - val_accuracy: 0.2207 - val_precision: 0.6055 - val_recall: 0.7091 - val_auc: 0.9576 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "51/51 [==============================] - 2s 34ms/step - loss: 0.1511 - accuracy: 0.2569 - precision: 0.6324 - recall: 0.5916 - auc: 0.9565 - val_loss: 0.1492 - val_accuracy: 0.2316 - val_precision: 0.6071 - val_recall: 0.7123 - val_auc: 0.9580 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "51/51 [==============================] - 2s 31ms/step - loss: 0.1504 - accuracy: 0.2599 - precision: 0.6323 - recall: 0.5967 - auc: 0.9569 - val_loss: 0.1477 - val_accuracy: 0.2273 - val_precision: 0.6268 - val_recall: 0.6284 - val_auc: 0.9584 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "51/51 [==============================] - 2s 34ms/step - loss: 0.1499 - accuracy: 0.2604 - precision: 0.6317 - recall: 0.6009 - auc: 0.9571 - val_loss: 0.1468 - val_accuracy: 0.2411 - val_precision: 0.6199 - val_recall: 0.6803 - val_auc: 0.9591 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1491 - accuracy: 0.2637 - precision: 0.6329 - recall: 0.6048 - auc: 0.9576 - val_loss: 0.1465 - val_accuracy: 0.2388 - val_precision: 0.6192 - val_recall: 0.6857 - val_auc: 0.9592 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "51/51 [==============================] - 2s 31ms/step - loss: 0.1485 - accuracy: 0.2654 - precision: 0.6324 - recall: 0.6085 - auc: 0.9580 - val_loss: 0.1461 - val_accuracy: 0.2416 - val_precision: 0.6201 - val_recall: 0.6829 - val_auc: 0.9594 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1482 - accuracy: 0.2660 - precision: 0.6326 - recall: 0.6125 - auc: 0.9582 - val_loss: 0.1458 - val_accuracy: 0.2450 - val_precision: 0.6220 - val_recall: 0.6818 - val_auc: 0.9596 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "51/51 [==============================] - 2s 34ms/step - loss: 0.1480 - accuracy: 0.2677 - precision: 0.6320 - recall: 0.6156 - auc: 0.9582 - val_loss: 0.1447 - val_accuracy: 0.2450 - val_precision: 0.6197 - val_recall: 0.7049 - val_auc: 0.9602 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1473 - accuracy: 0.2675 - precision: 0.6336 - recall: 0.6141 - auc: 0.9586 - val_loss: 0.1454 - val_accuracy: 0.2417 - val_precision: 0.6199 - val_recall: 0.6873 - val_auc: 0.9599 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1470 - accuracy: 0.2706 - precision: 0.6313 - recall: 0.6192 - auc: 0.9587 - val_loss: 0.1439 - val_accuracy: 0.2508 - val_precision: 0.6313 - val_recall: 0.6676 - val_auc: 0.9605 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1465 - accuracy: 0.2692 - precision: 0.6332 - recall: 0.6227 - auc: 0.9590 - val_loss: 0.1435 - val_accuracy: 0.2476 - val_precision: 0.6290 - val_recall: 0.6786 - val_auc: 0.9608 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1463 - accuracy: 0.2711 - precision: 0.6337 - recall: 0.6195 - auc: 0.9591 - val_loss: 0.1434 - val_accuracy: 0.2562 - val_precision: 0.6324 - val_recall: 0.6592 - val_auc: 0.9608 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "51/51 [==============================] - 2s 31ms/step - loss: 0.1460 - accuracy: 0.2727 - precision: 0.6327 - recall: 0.6231 - auc: 0.9592 - val_loss: 0.1436 - val_accuracy: 0.2433 - val_precision: 0.6284 - val_recall: 0.6700 - val_auc: 0.9606 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1456 - accuracy: 0.2708 - precision: 0.6331 - recall: 0.6271 - auc: 0.9596 - val_loss: 0.1426 - val_accuracy: 0.2549 - val_precision: 0.6326 - val_recall: 0.6704 - val_auc: 0.9612 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1453 - accuracy: 0.2745 - precision: 0.6338 - recall: 0.6235 - auc: 0.9597 - val_loss: 0.1426 - val_accuracy: 0.2554 - val_precision: 0.6338 - val_recall: 0.6537 - val_auc: 0.9611 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1452 - accuracy: 0.2738 - precision: 0.6348 - recall: 0.6234 - auc: 0.9597 - val_loss: 0.1426 - val_accuracy: 0.2519 - val_precision: 0.6269 - val_recall: 0.6885 - val_auc: 0.9613 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1448 - accuracy: 0.2749 - precision: 0.6340 - recall: 0.6270 - auc: 0.9599 - val_loss: 0.1421 - val_accuracy: 0.2486 - val_precision: 0.6296 - val_recall: 0.6793 - val_auc: 0.9615 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1446 - accuracy: 0.2733 - precision: 0.6330 - recall: 0.6293 - auc: 0.9600 - val_loss: 0.1415 - val_accuracy: 0.2645 - val_precision: 0.6360 - val_recall: 0.6554 - val_auc: 0.9617 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1442 - accuracy: 0.2759 - precision: 0.6358 - recall: 0.6288 - auc: 0.9603 - val_loss: 0.1414 - val_accuracy: 0.2545 - val_precision: 0.6304 - val_recall: 0.6745 - val_auc: 0.9618 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1439 - accuracy: 0.2760 - precision: 0.6352 - recall: 0.6303 - auc: 0.9604 - val_loss: 0.1410 - val_accuracy: 0.2615 - val_precision: 0.6311 - val_recall: 0.6814 - val_auc: 0.9620 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1440 - accuracy: 0.2769 - precision: 0.6350 - recall: 0.6320 - auc: 0.9604 - val_loss: 0.1410 - val_accuracy: 0.2575 - val_precision: 0.6332 - val_recall: 0.6648 - val_auc: 0.9620 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1435 - accuracy: 0.2753 - precision: 0.6345 - recall: 0.6349 - auc: 0.9606 - val_loss: 0.1410 - val_accuracy: 0.2653 - val_precision: 0.6369 - val_recall: 0.6517 - val_auc: 0.9619 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "49/51 [===========================>..] - ETA: 0s - loss: 0.1432 - accuracy: 0.2781 - precision: 0.6358 - recall: 0.6261 - auc: 0.9607\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1432 - accuracy: 0.2776 - precision: 0.6362 - recall: 0.6263 - auc: 0.9607 - val_loss: 0.1422 - val_accuracy: 0.2550 - val_precision: 0.6299 - val_recall: 0.6720 - val_auc: 0.9616 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "51/51 [==============================] - 2s 34ms/step - loss: 0.1424 - accuracy: 0.2760 - precision: 0.6369 - recall: 0.6375 - auc: 0.9612 - val_loss: 0.1397 - val_accuracy: 0.2774 - val_precision: 0.6346 - val_recall: 0.6824 - val_auc: 0.9630 - lr: 2.0000e-04\n",
      "Epoch 37/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1420 - accuracy: 0.2777 - precision: 0.6379 - recall: 0.6364 - auc: 0.9614 - val_loss: 0.1393 - val_accuracy: 0.2741 - val_precision: 0.6360 - val_recall: 0.6772 - val_auc: 0.9631 - lr: 2.0000e-04\n",
      "Epoch 38/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1419 - accuracy: 0.2743 - precision: 0.6385 - recall: 0.6391 - auc: 0.9614 - val_loss: 0.1390 - val_accuracy: 0.2775 - val_precision: 0.6375 - val_recall: 0.6751 - val_auc: 0.9632 - lr: 2.0000e-04\n",
      "Epoch 39/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1414 - accuracy: 0.2776 - precision: 0.6393 - recall: 0.6384 - auc: 0.9618 - val_loss: 0.1387 - val_accuracy: 0.2733 - val_precision: 0.6367 - val_recall: 0.6777 - val_auc: 0.9633 - lr: 2.0000e-04\n",
      "Epoch 40/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1414 - accuracy: 0.2751 - precision: 0.6383 - recall: 0.6386 - auc: 0.9617 - val_loss: 0.1391 - val_accuracy: 0.2776 - val_precision: 0.6316 - val_recall: 0.6924 - val_auc: 0.9633 - lr: 2.0000e-04\n",
      "Epoch 41/50\n",
      "51/51 [==============================] - 2s 34ms/step - loss: 0.1411 - accuracy: 0.2776 - precision: 0.6385 - recall: 0.6416 - auc: 0.9618 - val_loss: 0.1382 - val_accuracy: 0.2770 - val_precision: 0.6411 - val_recall: 0.6622 - val_auc: 0.9633 - lr: 2.0000e-04\n",
      "Epoch 42/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1407 - accuracy: 0.2780 - precision: 0.6389 - recall: 0.6407 - auc: 0.9621 - val_loss: 0.1380 - val_accuracy: 0.2740 - val_precision: 0.6431 - val_recall: 0.6523 - val_auc: 0.9632 - lr: 2.0000e-04\n",
      "Epoch 43/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1405 - accuracy: 0.2794 - precision: 0.6382 - recall: 0.6393 - auc: 0.9621 - val_loss: 0.1372 - val_accuracy: 0.2738 - val_precision: 0.6417 - val_recall: 0.6655 - val_auc: 0.9637 - lr: 2.0000e-04\n",
      "Epoch 44/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1401 - accuracy: 0.2797 - precision: 0.6407 - recall: 0.6472 - auc: 0.9624 - val_loss: 0.1368 - val_accuracy: 0.2788 - val_precision: 0.6398 - val_recall: 0.6714 - val_auc: 0.9640 - lr: 2.0000e-04\n",
      "Epoch 45/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1401 - accuracy: 0.2815 - precision: 0.6399 - recall: 0.6471 - auc: 0.9624 - val_loss: 0.1364 - val_accuracy: 0.2763 - val_precision: 0.6414 - val_recall: 0.6762 - val_auc: 0.9644 - lr: 2.0000e-04\n",
      "Epoch 46/50\n",
      "51/51 [==============================] - 2s 31ms/step - loss: 0.1396 - accuracy: 0.2811 - precision: 0.6423 - recall: 0.6481 - auc: 0.9627 - val_loss: 0.1378 - val_accuracy: 0.2699 - val_precision: 0.6462 - val_recall: 0.6459 - val_auc: 0.9632 - lr: 2.0000e-04\n",
      "Epoch 47/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1394 - accuracy: 0.2808 - precision: 0.6424 - recall: 0.6506 - auc: 0.9627 - val_loss: 0.1354 - val_accuracy: 0.2786 - val_precision: 0.6450 - val_recall: 0.6825 - val_auc: 0.9649 - lr: 2.0000e-04\n",
      "Epoch 48/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1392 - accuracy: 0.2844 - precision: 0.6427 - recall: 0.6491 - auc: 0.9629 - val_loss: 0.1351 - val_accuracy: 0.2773 - val_precision: 0.6459 - val_recall: 0.6739 - val_auc: 0.9649 - lr: 2.0000e-04\n",
      "Epoch 49/50\n",
      "51/51 [==============================] - 2s 32ms/step - loss: 0.1387 - accuracy: 0.2844 - precision: 0.6421 - recall: 0.6555 - auc: 0.9632 - val_loss: 0.1361 - val_accuracy: 0.2775 - val_precision: 0.6461 - val_recall: 0.6542 - val_auc: 0.9640 - lr: 2.0000e-04\n",
      "Epoch 50/50\n",
      "51/51 [==============================] - 2s 33ms/step - loss: 0.1387 - accuracy: 0.2858 - precision: 0.6436 - recall: 0.6519 - auc: 0.9631 - val_loss: 0.1341 - val_accuracy: 0.2803 - val_precision: 0.6510 - val_recall: 0.6788 - val_auc: 0.9654 - lr: 2.0000e-04\n",
      "\n",
      "=== Training Results ===\n",
      "Epoch 1: Loss: 0.3247, Accuracy: 0.2570, Val Loss: 0.3272, Val Accuracy: 0.2544\n",
      "Epoch 2: Loss: 0.1837, Accuracy: 0.2492, Val Loss: 0.2230, Val Accuracy: 0.2490\n",
      "Epoch 3: Loss: 0.1723, Accuracy: 0.2516, Val Loss: 0.1935, Val Accuracy: 0.2369\n",
      "Epoch 4: Loss: 0.1651, Accuracy: 0.2576, Val Loss: 0.1893, Val Accuracy: 0.2100\n",
      "Epoch 5: Loss: 0.1614, Accuracy: 0.2566, Val Loss: 0.1803, Val Accuracy: 0.2167\n",
      "Epoch 6: Loss: 0.1595, Accuracy: 0.2553, Val Loss: 0.1734, Val Accuracy: 0.2100\n",
      "Epoch 7: Loss: 0.1581, Accuracy: 0.2502, Val Loss: 0.1667, Val Accuracy: 0.2172\n",
      "Epoch 8: Loss: 0.1567, Accuracy: 0.2507, Val Loss: 0.1630, Val Accuracy: 0.2080\n",
      "Epoch 9: Loss: 0.1555, Accuracy: 0.2497, Val Loss: 0.1602, Val Accuracy: 0.2085\n",
      "Epoch 10: Loss: 0.1546, Accuracy: 0.2491, Val Loss: 0.1562, Val Accuracy: 0.2101\n",
      "Epoch 11: Loss: 0.1534, Accuracy: 0.2490, Val Loss: 0.1535, Val Accuracy: 0.2226\n",
      "Epoch 12: Loss: 0.1528, Accuracy: 0.2538, Val Loss: 0.1520, Val Accuracy: 0.2181\n",
      "Epoch 13: Loss: 0.1520, Accuracy: 0.2542, Val Loss: 0.1503, Val Accuracy: 0.2207\n",
      "Epoch 14: Loss: 0.1511, Accuracy: 0.2569, Val Loss: 0.1492, Val Accuracy: 0.2316\n",
      "Epoch 15: Loss: 0.1504, Accuracy: 0.2599, Val Loss: 0.1477, Val Accuracy: 0.2273\n",
      "Epoch 16: Loss: 0.1499, Accuracy: 0.2604, Val Loss: 0.1468, Val Accuracy: 0.2411\n",
      "Epoch 17: Loss: 0.1491, Accuracy: 0.2637, Val Loss: 0.1465, Val Accuracy: 0.2388\n",
      "Epoch 18: Loss: 0.1485, Accuracy: 0.2654, Val Loss: 0.1461, Val Accuracy: 0.2416\n",
      "Epoch 19: Loss: 0.1482, Accuracy: 0.2660, Val Loss: 0.1458, Val Accuracy: 0.2450\n",
      "Epoch 20: Loss: 0.1480, Accuracy: 0.2677, Val Loss: 0.1447, Val Accuracy: 0.2450\n",
      "Epoch 21: Loss: 0.1473, Accuracy: 0.2675, Val Loss: 0.1454, Val Accuracy: 0.2417\n",
      "Epoch 22: Loss: 0.1470, Accuracy: 0.2706, Val Loss: 0.1439, Val Accuracy: 0.2508\n",
      "Epoch 23: Loss: 0.1465, Accuracy: 0.2692, Val Loss: 0.1435, Val Accuracy: 0.2476\n",
      "Epoch 24: Loss: 0.1463, Accuracy: 0.2711, Val Loss: 0.1434, Val Accuracy: 0.2562\n",
      "Epoch 25: Loss: 0.1460, Accuracy: 0.2727, Val Loss: 0.1436, Val Accuracy: 0.2433\n",
      "Epoch 26: Loss: 0.1456, Accuracy: 0.2708, Val Loss: 0.1426, Val Accuracy: 0.2549\n",
      "Epoch 27: Loss: 0.1453, Accuracy: 0.2745, Val Loss: 0.1426, Val Accuracy: 0.2554\n",
      "Epoch 28: Loss: 0.1452, Accuracy: 0.2738, Val Loss: 0.1426, Val Accuracy: 0.2519\n",
      "Epoch 29: Loss: 0.1448, Accuracy: 0.2749, Val Loss: 0.1421, Val Accuracy: 0.2486\n",
      "Epoch 30: Loss: 0.1446, Accuracy: 0.2733, Val Loss: 0.1415, Val Accuracy: 0.2645\n",
      "Epoch 31: Loss: 0.1442, Accuracy: 0.2759, Val Loss: 0.1414, Val Accuracy: 0.2545\n",
      "Epoch 32: Loss: 0.1439, Accuracy: 0.2760, Val Loss: 0.1410, Val Accuracy: 0.2615\n",
      "Epoch 33: Loss: 0.1440, Accuracy: 0.2769, Val Loss: 0.1410, Val Accuracy: 0.2575\n",
      "Epoch 34: Loss: 0.1435, Accuracy: 0.2753, Val Loss: 0.1410, Val Accuracy: 0.2653\n",
      "Epoch 35: Loss: 0.1432, Accuracy: 0.2776, Val Loss: 0.1422, Val Accuracy: 0.2550\n",
      "Epoch 36: Loss: 0.1424, Accuracy: 0.2760, Val Loss: 0.1397, Val Accuracy: 0.2774\n",
      "Epoch 37: Loss: 0.1420, Accuracy: 0.2777, Val Loss: 0.1393, Val Accuracy: 0.2741\n",
      "Epoch 38: Loss: 0.1419, Accuracy: 0.2743, Val Loss: 0.1390, Val Accuracy: 0.2775\n",
      "Epoch 39: Loss: 0.1414, Accuracy: 0.2776, Val Loss: 0.1387, Val Accuracy: 0.2733\n",
      "Epoch 40: Loss: 0.1414, Accuracy: 0.2751, Val Loss: 0.1391, Val Accuracy: 0.2776\n",
      "Epoch 41: Loss: 0.1411, Accuracy: 0.2776, Val Loss: 0.1382, Val Accuracy: 0.2770\n",
      "Epoch 42: Loss: 0.1407, Accuracy: 0.2780, Val Loss: 0.1380, Val Accuracy: 0.2740\n",
      "Epoch 43: Loss: 0.1405, Accuracy: 0.2794, Val Loss: 0.1372, Val Accuracy: 0.2738\n",
      "Epoch 44: Loss: 0.1401, Accuracy: 0.2797, Val Loss: 0.1368, Val Accuracy: 0.2788\n",
      "Epoch 45: Loss: 0.1401, Accuracy: 0.2815, Val Loss: 0.1364, Val Accuracy: 0.2763\n",
      "Epoch 46: Loss: 0.1396, Accuracy: 0.2811, Val Loss: 0.1378, Val Accuracy: 0.2699\n",
      "Epoch 47: Loss: 0.1394, Accuracy: 0.2808, Val Loss: 0.1354, Val Accuracy: 0.2786\n",
      "Epoch 48: Loss: 0.1392, Accuracy: 0.2844, Val Loss: 0.1351, Val Accuracy: 0.2773\n",
      "Epoch 49: Loss: 0.1387, Accuracy: 0.2844, Val Loss: 0.1361, Val Accuracy: 0.2775\n",
      "Epoch 50: Loss: 0.1387, Accuracy: 0.2858, Val Loss: 0.1341, Val Accuracy: 0.2803\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "\n",
      "=== Hasil Analisis Kesehatan (Model H5) ===\n",
      "\n",
      "BMI: 22.5\n",
      "Status BMI: Normal\n",
      "\n",
      "Kebutuhan Nutrisi Harian:\n",
      "Sodium: 1300.0 mg\n",
      "Lemak: 9.8 g\n",
      "Protein: 58.5 g\n",
      "Karbohidrat: 195.0 g\n",
      "\n",
      "Risiko Penyakit:\n",
      "Anemia: 100.0% (Risiko Sangat Tinggi)\n",
      "Kolesterol: 0.0% (Risiko Rendah)\n",
      "CKD: 0.0% (Risiko Rendah)\n",
      "Diabetes: 0.0% (Risiko Rendah)\n",
      "Jantung: 0.0% (Risiko Rendah)\n",
      "Hipertensi: 0.0% (Risiko Rendah)\n",
      "MS: 0.0% (Risiko Rendah)\n",
      "NAFLD: 0.0% (Risiko Rendah)\n",
      "Obesitas: 0.0% (Risiko Rendah)\n",
      "Stroke: 0.0% (Risiko Rendah)\n",
      "\n",
      "==================================================\n",
      "\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "\n",
      "=== Hasil Analisis Kesehatan (Model TFJS) ===\n",
      "\n",
      "BMI: 27.3\n",
      "Status BMI: Gemuk\n",
      "\n",
      "Kebutuhan Nutrisi Harian:\n",
      "Sodium: 1400.0 mg\n",
      "Lemak: 10.5 g\n",
      "Protein: 63.0 g\n",
      "Karbohidrat: 210.0 g\n",
      "\n",
      "Risiko Penyakit:\n",
      "Anemia: 48.6% (Risiko Tinggi)\n",
      "Kolesterol: 48.9% (Risiko Tinggi)\n",
      "CKD: 48.7% (Risiko Tinggi)\n",
      "Diabetes: 49.4% (Risiko Tinggi)\n",
      "Jantung: 48.8% (Risiko Tinggi)\n",
      "Hipertensi: 49.8% (Risiko Tinggi)\n",
      "MS: 48.6% (Risiko Tinggi)\n",
      "NAFLD: 48.4% (Risiko Tinggi)\n",
      "Obesitas: 48.5% (Risiko Tinggi)\n",
      "Stroke: 48.6% (Risiko Tinggi)\n"
     ]
    }
   ],
   "source": [
    "def load_and_process_data(file_path='dataset/health_data1/combined_dataset.csv'):\n",
    "    print(\"Loading and processing data...\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    \n",
    "    available_features = ['height', 'weight', 'gender', 'age', 'bp', 'bc', 'bg', 'bmi', 'sodium']\n",
    "    target_variables = ['anemia', 'cholesterol', 'ckd', 'diabetes', 'heart',\n",
    "                        'hypertension', 'ms', 'nafld', 'obesity', 'stroke']\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    print(\"Processing features...\")\n",
    "    numeric_features = [col for col in available_features if pd.api.types.is_numeric_dtype(df[col])]\n",
    "    categorical_features = [col for col in available_features if col not in numeric_features]\n",
    "    \n",
    "    print(\"Handling missing values...\")\n",
    "    print(f\"Found {len(numeric_features)} numeric features and {len(categorical_features)} categorical features\")\n",
    "    \n",
    "    numeric_imputer = IterativeImputer(random_state=42, max_iter=100, sample_posterior=True)\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    df_processed[numeric_features] = numeric_imputer.fit_transform(df_processed[numeric_features])\n",
    "    if categorical_features:\n",
    "        df_processed[categorical_features] = categorical_imputer.fit_transform(df_processed[categorical_features])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df_processed[numeric_features] = scaler.fit_transform(df_processed[numeric_features])\n",
    "    \n",
    "    df_processed = add_engineered_features(df_processed)\n",
    "    \n",
    "    final_features = available_features + [\n",
    "        'bmi_category', 'age_category', 'bp_category',\n",
    "        'bmi_age', 'bp_age', 'bmi_bp'\n",
    "    ]\n",
    "    \n",
    "    print(\"Processing target variables...\")\n",
    "    df_processed[target_variables] = df[target_variables].fillna(0)\n",
    "    \n",
    "    print(\"Data processing completed!\")\n",
    "    return df_processed[final_features], df_processed[target_variables]\n",
    "\n",
    "def add_engineered_features(X):\n",
    "    X_new = X.copy()\n",
    "    X_new['bmi_category'] = pd.cut(X_new['bmi'], \n",
    "                                  bins=[float('-inf'), 18.5, 25, 30, float('inf')],\n",
    "                                  labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['age_category'] = pd.cut(X_new['age'], \n",
    "                                  bins=[float('-inf'), 30, 45, 60, float('inf')],\n",
    "                                  labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['bp_category'] = pd.cut(X_new['bp'], \n",
    "                                 bins=[float('-inf'), 120, 140, 160, float('inf')],\n",
    "                                 labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['bmi_age'] = X_new['bmi'] * X_new['age']\n",
    "    X_new['bp_age'] = X_new['bp'] * X_new['age']\n",
    "    X_new['bmi_bp'] = X_new['bmi'] * X_new['bp']\n",
    "    return X_new\n",
    "\n",
    "def create_model(input_shape, name=None):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(input_shape,)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ], name=name)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall(),\n",
    "                tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_disease_models(X, y):\n",
    "    print(\"\\n=== Starting Model Training Process ===\")\n",
    "    models = {}\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X = np.array(X)\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # GPU Configuration\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # Optimizer settings\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    \n",
    "    BATCH_SIZE = 2560\n",
    "    BUFFER_SIZE = 10000\n",
    "    \n",
    "    # Split data terlebih dahulu\n",
    "    X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "    y_train, y_val = train_test_split(y.values, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\\\n",
    "        .cache()\\\n",
    "        .shuffle(BUFFER_SIZE)\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\\\n",
    "        .cache()\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create model\n",
    "    combined_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(y.columns), activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    combined_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir='./logs',\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Training\n",
    "    print(\"\\nStarting combined model training...\")\n",
    "    history = combined_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=50,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Display epoch results\n",
    "    print(\"\\n=== Training Results ===\")\n",
    "    for epoch in range(len(history.history['loss'])):\n",
    "        print(f\"Epoch {epoch + 1}: \"\n",
    "              f\"Loss: {history.history['loss'][epoch]:.4f}, \"\n",
    "              f\"Accuracy: {history.history['accuracy'][epoch]:.4f}, \"\n",
    "              f\"Val Loss: {history.history['val_loss'][epoch]:.4f}, \"\n",
    "              f\"Val Accuracy: {history.history['val_accuracy'][epoch]:.4f}\")\n",
    "    \n",
    "    return combined_model, scaler, history\n",
    "\n",
    "def evaluate_model_performance(models, X, y):\n",
    "    print(\"\\n=== Starting Model Evaluation ===\")\n",
    "    print(\"Model Evaluation Results:\\n\")\n",
    "    print(\"{:<15} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "        \"Disease\", \"Accuracy\", \"Precision\", \"Recall\", \"AUC\"\n",
    "    ))\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    \n",
    "    total_models = len(models)\n",
    "    overall_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'auc': 0}\n",
    "    \n",
    "    for idx, (disease, model) in enumerate(models.items(), 1):\n",
    "        try:\n",
    "            print(f\"\\n[{idx}/{total_models}] Evaluating {disease.upper()} model...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            metrics = model.evaluate(X, y[disease].values, verbose=0)\n",
    "            \n",
    "            print(\"{:<15} {:<10.2f} {:<10.2f} {:<10.2f} {:<10.2f}\".format(\n",
    "                disease,\n",
    "                metrics[1] * 100,#acc\n",
    "                metrics[2] * 100,#prec\n",
    "                metrics[3] * 100,#recall\n",
    "                metrics[4] * 100 #auc\n",
    "            ))\n",
    "            \n",
    "            overall_metrics['accuracy'] += metrics[1]\n",
    "            overall_metrics['precision'] += metrics[2]\n",
    "            overall_metrics['recall'] += metrics[3]\n",
    "            overall_metrics['auc'] += metrics[4]\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\" Evaluation completed in {elapsed_time:.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error evaluating {disease} model: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    for metric, value in overall_metrics.items():\n",
    "        print(f\"{metric.capitalize():10}: {value/total_models*100:.2f}%\")\n",
    "    print(\"\\n=== Evaluation Complete ===\")\n",
    "\n",
    "def save_models(model, scaler, save_dir='models'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    print(\"\\nSaving model...\")\n",
    "    \n",
    "    model_path = os.path.join(save_dir, 'disease-prediction-tf-model.h5')\n",
    "    model.save(model_path)\n",
    "    print(\" Saved combined model in HDF5 format\")\n",
    "    \n",
    "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(\" Saved scaler\")\n",
    "    \n",
    "    print(\"\\nAll models and scaler saved successfully!\")\n",
    "\n",
    "def load_models(save_dir='models'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        raise FileNotFoundError(f\"Directory {save_dir} not found\")\n",
    "    \n",
    "    print(\"\\nLoading model...\")\n",
    "    model_path = os.path.join(save_dir, 'disease-prediction-tf-model.h5')\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(\" Loaded combined model from HDF5\")\n",
    "    \n",
    "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print(\" Loaded scaler\")\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "def predict_diseases(input_data, models, scaler):\n",
    "    print(\"\\n--- Predicting Diseases ---\")\n",
    "    \n",
    "    if not isinstance(input_data, pd.DataFrame):\n",
    "        input_data = pd.DataFrame([input_data])\n",
    "    \n",
    "    X_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    predictions = {}\n",
    "    for disease, model in models.items():\n",
    "        pred_prob = model.predict(X_scaled, verbose=0)[0][0]\n",
    "        predictions[disease] = {\n",
    "            'probability': float(pred_prob),\n",
    "            'prediction': 1 if pred_prob >= 0.5 else 0\n",
    "        }\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def predict_disease_risks(user_input, combined_model, prediction_scaler):\n",
    "    expected_features = [\n",
    "        'height', 'weight', 'gender', 'age', 'bp', 'bc', 'bg', 'bmi', 'sodium',\n",
    "        'bmi_category', 'age_category', 'bp_category',\n",
    "        'bmi_age', 'bp_age', 'bmi_bp'\n",
    "    ]\n",
    "    \n",
    "    if isinstance(user_input, pd.DataFrame):\n",
    "        for feature in expected_features:\n",
    "            if feature not in user_input.columns:\n",
    "                user_input[feature] = 0\n",
    "        \n",
    "        user_input = user_input[expected_features]\n",
    "    \n",
    "    X_scaled = prediction_scaler.transform(user_input)\n",
    "    \n",
    "    predictions = combined_model.predict(X_scaled, verbose=0)[0]\n",
    "    \n",
    "    disease_names = ['anemia', 'cholesterol', 'ckd', 'diabetes', 'heart',\n",
    "                    'hypertension', 'ms', 'nafld', 'obesity', 'stroke']\n",
    "    \n",
    "    predictions_percent = {disease: prob * 100 \n",
    "                         for disease, prob in zip(disease_names, predictions)}\n",
    "    \n",
    "    return predictions_percent\n",
    "\n",
    "def calculate_derived_features(height, weight, gender, age, blood_pressure, cholesterol, blood_glucose):\n",
    "    # bmi calculation\n",
    "    height_m = height / 100\n",
    "    bmi = weight / (height_m ** 2)\n",
    "    \n",
    "    # sodium calculation\n",
    "    sodium = weight * 20\n",
    "    \n",
    "    # fat based on gender calculatonn\n",
    "    fat = weight * (0.15 if gender == 1 else 0.25)\n",
    "    \n",
    "    # chols level calc\n",
    "    cholesterol_level = (bmi * 2) + (age * 0.15) + (blood_pressure * 0.05) + (blood_glucose * 0.02) + 150\n",
    "    \n",
    "    # protein calc\n",
    "    protein = weight * 0.9\n",
    "    \n",
    "    # carbo calc\n",
    "    carbs = weight * 3\n",
    "    \n",
    "    return {\n",
    "        'bmi': bmi,\n",
    "        'sodium': sodium,\n",
    "        'fat': fat,\n",
    "        'cholesterol_level': cholesterol_level,\n",
    "        'protein': protein,\n",
    "        'carbs': carbs\n",
    "    }\n",
    "\n",
    "def get_average_values(df):\n",
    "    return {\n",
    "        'height': df['height'].mean(),\n",
    "        'weight': df['weight'].mean(),\n",
    "        'gender': round(df['gender'].mean()),\n",
    "        'age': df['age'].mean(),\n",
    "        'blood_pressure': df['bp'].mean(),\n",
    "        'cholesterol': df['bc'].mean(),\n",
    "        'blood_glucose': df['bg'].mean()\n",
    "    }\n",
    "\n",
    "def calculate_derived_features(height, weight, gender, age, blood_pressure, cholesterol, blood_glucose):\n",
    "    height_m = height / 100\n",
    "    bmi = weight / (height_m ** 2)\n",
    "    \n",
    "    # Categories\n",
    "    bmi_category = 0 if bmi < 18.5 else 1 if bmi < 25 else 2 if bmi < 30 else 3\n",
    "    age_category = 0 if age < 30 else 1 if age < 45 else 2 if age < 60 else 3\n",
    "    bp_category = 0 if blood_pressure < 120 else 1 if blood_pressure < 140 else 2 if blood_pressure < 160 else 3\n",
    "    \n",
    "    # Interactions\n",
    "    bmi_age = bmi * age\n",
    "    bp_age = blood_pressure * age\n",
    "    bmi_bp = bmi * blood_pressure\n",
    "    \n",
    "    # Nutrition calculations\n",
    "    sodium = weight * 20\n",
    "    fat = weight * (0.15 if gender == 1 else 0.25)\n",
    "    protein = weight * 0.9\n",
    "    carbs = weight * 3\n",
    "    \n",
    "    return {\n",
    "        'bmi': bmi,\n",
    "        'bmi_category': bmi_category,\n",
    "        'age_category': age_category,\n",
    "        'bp_category': bp_category,\n",
    "        'bmi_age': bmi_age,\n",
    "        'bp_age': bp_age,\n",
    "        'bmi_bp': bmi_bp,\n",
    "        'sodium': sodium,\n",
    "        'fat': fat,\n",
    "        'protein': protein,\n",
    "        'carbs': carbs\n",
    "    }\n",
    "\n",
    "def predict_h5_model():\n",
    "    try:\n",
    "        model = tf.keras.models.load_model('models/disease-prediction-tf-model.h5')\n",
    "        \n",
    "        # User input\n",
    "        user_input = {\n",
    "            'height': 170,\n",
    "            'weight': 65,\n",
    "            'gender': 1,\n",
    "            'age': 25,\n",
    "            'blood_pressure': 120,\n",
    "            'cholesterol': 180,\n",
    "            'blood_glucose': 90\n",
    "        }\n",
    "        \n",
    "        # Calculate derived features\n",
    "        derived = calculate_derived_features(\n",
    "            user_input['height'],\n",
    "            user_input['weight'],\n",
    "            user_input['gender'],\n",
    "            user_input['age'],\n",
    "            user_input['blood_pressure'],\n",
    "            user_input['cholesterol'],\n",
    "            user_input['blood_glucose']\n",
    "        )\n",
    "        \n",
    "        # Prepare features (15 features sesuai model)\n",
    "        features = np.array([[\n",
    "            user_input['height'],\n",
    "            user_input['weight'],\n",
    "            user_input['gender'],\n",
    "            user_input['age'],\n",
    "            user_input['blood_pressure'],\n",
    "            user_input['cholesterol'],\n",
    "            user_input['blood_glucose'],\n",
    "            derived['bmi'],\n",
    "            derived['bmi_category'],\n",
    "            derived['age_category'],\n",
    "            derived['bp_category'],\n",
    "            derived['bmi_age'],\n",
    "            derived['bp_age'],\n",
    "            derived['bmi_bp'],\n",
    "            derived['sodium']\n",
    "        ]])\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(features)\n",
    "        diseases = ['Anemia', 'Kolesterol', 'CKD', 'Diabetes', 'Jantung', \n",
    "                   'Hipertensi', 'MS', 'NAFLD', 'Obesitas', 'Stroke']\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n=== Hasil Analisis Kesehatan (Model H5) ===\")\n",
    "        print(f\"\\nBMI: {derived['bmi']:.1f}\")\n",
    "        print(f\"Status BMI: {'Kurus' if derived['bmi'] < 18.5 else 'Normal' if derived['bmi'] < 25 else 'Gemuk' if derived['bmi'] < 30 else 'Obesitas'}\")\n",
    "        \n",
    "        print(\"\\nKebutuhan Nutrisi Harian:\")\n",
    "        print(f\"Sodium: {derived['sodium']:.1f} mg\")\n",
    "        print(f\"Lemak: {derived['fat']:.1f} g\")\n",
    "        print(f\"Protein: {derived['protein']:.1f} g\")\n",
    "        print(f\"Karbohidrat: {derived['carbs']:.1f} g\")\n",
    "        \n",
    "        print(\"\\nRisiko Penyakit:\")\n",
    "        for disease, prob in zip(diseases, predictions[0]):\n",
    "            risk_level = \"Rendah\" if prob < 0.2 else \"Sedang\" if prob < 0.4 else \"Tinggi\" if prob < 0.6 else \"Sangat Tinggi\"\n",
    "            print(f\"{disease}: {prob*100:.1f}% (Risiko {risk_level})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "def predict_tfjs_model():\n",
    "    try:\n",
    "        import json\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # Load model JSON\n",
    "        model_path = os.path.join('models', 'tfjs_disease_model', 'model.json')\n",
    "        with open(model_path, 'r') as f:\n",
    "            model_json = json.load(f)\n",
    "            \n",
    "        # Extract model architecture yang benar dari model_config\n",
    "        model_config = model_json[\"modelTopology\"][\"model_config\"]\n",
    "        \n",
    "        # Buat model dari config\n",
    "        model = tf.keras.models.model_from_json(json.dumps(model_config))\n",
    "        \n",
    "        # Load weights dengan cara yang benar\n",
    "        weights_manifest = model_json['weightsManifest']\n",
    "        weights_path = os.path.join('models', 'tfjs_disease_model', 'group1-shard1of1.bin')\n",
    "        \n",
    "        with open(weights_path, 'rb') as f:\n",
    "            weights_data = f.read()\n",
    "            \n",
    "        # Convert weights data ke float32 array    \n",
    "        weights_array = np.frombuffer(weights_data, dtype=np.float32)\n",
    "        \n",
    "        # Set weights dengan memperhatikan shape dari manifest\n",
    "        current_pos = 0\n",
    "        for layer in model.layers:\n",
    "            if layer.weights:\n",
    "                layer_weights = []\n",
    "                for weight in layer.weights:\n",
    "                    shape = tuple(weight.shape.as_list())\n",
    "                    n_elements = np.prod(shape)\n",
    "                    layer_weight = weights_array[current_pos:current_pos + n_elements]\n",
    "                    layer_weight = np.reshape(layer_weight, shape)\n",
    "                    layer_weights.append(layer_weight)\n",
    "                    current_pos += n_elements\n",
    "                layer.set_weights(layer_weights)\n",
    "        \n",
    "        # User input dan prediksi (sama seperti sebelumnya)\n",
    "        user_input = {\n",
    "            'height': 160,\n",
    "            'weight': 70,\n",
    "            'gender': 1,\n",
    "            'age': 20,\n",
    "            'blood_pressure': 120,\n",
    "            'cholesterol': 180,\n",
    "            'blood_glucose': 90\n",
    "        }\n",
    "        \n",
    "        derived = calculate_derived_features(\n",
    "            user_input['height'],\n",
    "            user_input['weight'],\n",
    "            user_input['gender'],\n",
    "            user_input['age'],\n",
    "            user_input['blood_pressure'],\n",
    "            user_input['cholesterol'],\n",
    "            user_input['blood_glucose']\n",
    "        )\n",
    "        \n",
    "        features = np.array([[\n",
    "            user_input['height'],\n",
    "            user_input['weight'],\n",
    "            user_input['gender'],\n",
    "            user_input['age'],\n",
    "            user_input['blood_pressure'],\n",
    "            user_input['cholesterol'],\n",
    "            user_input['blood_glucose'],\n",
    "            derived['bmi'],\n",
    "            derived['bmi_category'],\n",
    "            derived['age_category'],\n",
    "            derived['bp_category'],\n",
    "            derived['bmi_age'],\n",
    "            derived['bp_age'],\n",
    "            derived['bmi_bp'],\n",
    "            derived['sodium']\n",
    "        ]])\n",
    "        \n",
    "        predictions = model.predict(features)\n",
    "        diseases = ['Anemia', 'Kolesterol', 'CKD', 'Diabetes', 'Jantung', \n",
    "                   'Hipertensi', 'MS', 'NAFLD', 'Obesitas', 'Stroke']\n",
    "        \n",
    "        print(\"\\n=== Hasil Analisis Kesehatan (Model TFJS) ===\")\n",
    "        print(f\"\\nBMI: {derived['bmi']:.1f}\")\n",
    "        print(f\"Status BMI: {'Kurus' if derived['bmi'] < 18.5 else 'Normal' if derived['bmi'] < 25 else 'Gemuk' if derived['bmi'] < 30 else 'Obesitas'}\")\n",
    "        \n",
    "        print(\"\\nKebutuhan Nutrisi Harian:\")\n",
    "        print(f\"Sodium: {derived['sodium']:.1f} mg\")\n",
    "        print(f\"Lemak: {derived['fat']:.1f} g\")\n",
    "        print(f\"Protein: {derived['protein']:.1f} g\")\n",
    "        print(f\"Karbohidrat: {derived['carbs']:.1f} g\")\n",
    "        \n",
    "        print(\"\\nRisiko Penyakit:\")\n",
    "        for disease, prob in zip(diseases, predictions[0]):\n",
    "            risk_level = \"Rendah\" if prob < 0.2 else \"Sedang\" if prob < 0.4 else \"Tinggi\" if prob < 0.6 else \"Sangat Tinggi\"\n",
    "            print(f\"{disease}: {prob*100:.1f}% (Risiko {risk_level})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Test kedua model\n",
    "\n",
    "# Call the function in your main code\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data here\n",
    "    X, y = load_and_process_data()  # Ensure this function is defined and returns the correct data\n",
    "    combined_model, scaler, history = train_disease_models(X, y)\n",
    "predict_h5_model()\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "predict_tfjs_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Health Risk Assessment ===\n",
      "Using average value for blood_pressure: 130.83\n",
      "Using average value for cholesterol: 98.87\n",
      "Using average value for blood_glucose: 18.57\n",
      "\n",
      "Loading model...\n",
      " Loaded combined model from HDF5\n",
      " Loaded scaler\n",
      "Error during prediction: float() argument must be a string or a real number, not 'dict'\n",
      "Failed to generate predictions. Please check if the model files exist.\n"
     ]
    }
   ],
   "source": [
    "def load_models(save_dir='models'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        raise FileNotFoundError(f\"Directory {save_dir} not found\")\n",
    "    \n",
    "    print(\"\\nLoading model...\")\n",
    "    model_path = os.path.join(save_dir, 'disease-prediction-tf-model.h5')\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(\" Loaded combined model from HDF5\")\n",
    "    \n",
    "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print(\" Loaded scaler\")\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "def calculate_derived_features(height, weight, gender, age, blood_pressure, cholesterol, blood_glucose):\n",
    "    # BMI calculation\n",
    "    bmi = weight / ((height/100) ** 2)\n",
    "    \n",
    "    # Categories calculation\n",
    "    bmi_category = 0 if bmi < 18.5 else 1 if bmi < 25 else 2 if bmi < 30 else 3\n",
    "    age_category = 0 if age < 30 else 1 if age < 45 else 2 if age < 60 else 3\n",
    "    bp_category = 0 if blood_pressure < 120 else 1 if blood_pressure < 140 else 2 if blood_pressure < 160 else 3\n",
    "    \n",
    "    # Interaction features\n",
    "    bmi_age = bmi * age\n",
    "    bp_age = blood_pressure * age\n",
    "    bmi_bp = bmi * blood_pressure\n",
    "    \n",
    "    # Nutrition calculations\n",
    "    sodium = weight * 20  # mg\n",
    "    fat = weight * (0.15 if gender == 1 else 0.25)  # g\n",
    "    cholesterol_level = (bmi * 2) + (age * 0.15) + (blood_pressure * 0.05) + (blood_glucose * 0.02) + 150  # mg/dL\n",
    "    protein = weight * 0.9  # g\n",
    "    carbs = weight * 3  # g\n",
    "    \n",
    "    return {\n",
    "        'bmi': bmi,\n",
    "        'bmi_category': bmi_category,\n",
    "        'age_category': age_category,\n",
    "        'bp_category': bp_category,\n",
    "        'bmi_age': bmi_age,\n",
    "        'bp_age': bp_age,\n",
    "        'bmi_bp': bmi_bp,\n",
    "        'sodium': sodium,\n",
    "        'fat': fat,\n",
    "        'cholesterol_level': cholesterol_level,\n",
    "        'protein': protein,\n",
    "        'carbs': carbs\n",
    "    }\n",
    "\n",
    "def predict_health_status(user_input):\n",
    "    df = pd.read_csv('dataset/health_data1/combined_dataset.csv')\n",
    "    \n",
    "    column_mapping = {\n",
    "        'height': 'height',\n",
    "        'weight': 'weight',\n",
    "        'gender': 'gender',\n",
    "        'age': 'age',\n",
    "        'blood_pressure': 'bp',\n",
    "        'cholesterol': 'bc',\n",
    "        'blood_glucose': 'bg'\n",
    "    }\n",
    "    \n",
    "    avg_values = {}\n",
    "    for key, col in column_mapping.items():\n",
    "        if col in df.columns:\n",
    "            avg_values[key] = df[col].mean()\n",
    "        else:\n",
    "            print(f\"Warning: Column {col} not found in dataset\")\n",
    "            avg_values[key] = 0\n",
    "    \n",
    "    if 'gender' in avg_values:\n",
    "        avg_values['gender'] = round(avg_values['gender'])\n",
    "    \n",
    "    for key in ['blood_pressure', 'cholesterol', 'blood_glucose']:\n",
    "        if user_input[key] is None:\n",
    "            user_input[key] = avg_values[key]\n",
    "            print(f\"Using average value for {key}: {user_input[key]:.2f}\")\n",
    "    \n",
    "    model, scaler = load_models()\n",
    "    if model is None or scaler is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        derived = calculate_derived_features(\n",
    "            user_input['height'],\n",
    "            user_input['weight'],\n",
    "            user_input['gender'],\n",
    "            user_input['age'],\n",
    "            user_input['blood_pressure'],\n",
    "            user_input['cholesterol'],\n",
    "            user_input['blood_glucose']\n",
    "        )\n",
    "        \n",
    "        features = {\n",
    "            'height': user_input['height'],\n",
    "            'weight': user_input['weight'],\n",
    "            'gender': user_input['gender'],\n",
    "            'age': user_input['age'],\n",
    "            'bp': user_input['blood_pressure'],\n",
    "            'bc': user_input['cholesterol'],\n",
    "            'bg': user_input['blood_glucose'],\n",
    "            'bmi': derived['bmi'],\n",
    "            'bmi_category': derived['bmi_category'],\n",
    "            'age_category': derived['age_category'],\n",
    "            'bp_category': derived['bp_category'],\n",
    "            'bmi_age': derived['bmi_age'],\n",
    "            'bp_age': derived['bp_age'],\n",
    "            'bmi_bp': derived['bmi_bp'],\n",
    "            'sodium': derived['sodium']\n",
    "        }\n",
    "        \n",
    "        input_scaled = scaler.transform(features)  # Gunakan scaler yang sama dengan model H5\n",
    "        predictions = model.predict(input_scaled)\n",
    "        \n",
    "        diseases = ['anemia', 'cholesterol', 'ckd', 'diabetes', 'heart', \n",
    "                   'hypertension', 'ms', 'nafld', 'obesity', 'stroke']\n",
    "        results = {disease: float(pred*100) for disease, pred in zip(diseases, predictions[0])}\n",
    "        \n",
    "        return results, derived\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = {\n",
    "        'height': 165,\n",
    "        'weight': 55,\n",
    "        'gender': 1,  # 1 for male, 0 for female\n",
    "        'age': 20,\n",
    "        'blood_pressure': None,\n",
    "        'cholesterol': None,\n",
    "        'blood_glucose': None\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Health Risk Assessment ===\")\n",
    "    results = predict_health_status(user_input)\n",
    "    \n",
    "    if results:\n",
    "        predictions, derived = results\n",
    "        print(\"\\nNutrition in User's Body:\")\n",
    "        print(f\"Sodium: {derived['sodium']:.1f} mg\")\n",
    "        print(f\"Fat: {derived['fat']:.1f} g\")\n",
    "        print(f\"Cholesterol: {derived['cholesterol_level']:.1f} mg/dL\")\n",
    "        print(f\"Protein: {derived['protein']:.1f} g\")\n",
    "        print(f\"Carbohydrates: {derived['carbs']:.1f} g\")\n",
    "        \n",
    "        print(f\"\\nBMI: {derived['bmi']:.1f}\")\n",
    "        \n",
    "        print(\"\\nDisease Risk Predictions:\")\n",
    "        for disease, risk in predictions.items():\n",
    "            print(f\"{disease.upper()}: {risk:.1f}%\")\n",
    "    else:\n",
    "        print(\"Failed to generate predictions. Please check if the model files exist.\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== User Body Metrics Input ===\n",
      "Height: 170 cm\n",
      "Weight: 65 kg\n",
      "Gender: Male\n",
      "Age: 25 years old\n",
      "Blood Pressure: 120 mmHg\n",
      "Cholesterol: 180 mg/dL\n",
      "Blood Glucose: 90 mg/dL\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000012752BEC940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "\n",
      "=== Analysis Result ===\n",
      "\n",
      "BMI: 22.5\n",
      "Status BMI: Normal\n",
      "\n",
      "Body Nutrition:\n",
      "Sodium: 1300.0 mg\n",
      "Fat: 9.8 g\n",
      "Protein: 58.5 g\n",
      "Carbohydrate: 195.0 g\n",
      "\n",
      "Disease Risk:\n",
      "Anemia: 1.0\n",
      "Kolesterol: 0.0\n",
      "CKD: 0.0\n",
      "Diabetes: 0.0\n",
      "Jantung: 0.0\n",
      "Hipertensi: 0.0\n",
      "MS: 0.0\n",
      "NAFLD: 0.0\n",
      "Obesitas: 0.0\n",
      "Stroke: 0.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_derived_features(height, weight, gender, age, blood_pressure, cholesterol, blood_glucose):\n",
    "    height_m = height / 100\n",
    "    bmi = weight / (height_m ** 2)\n",
    "    \n",
    "    # Categories\n",
    "    bmi_category = 0 if bmi < 18.5 else 1 if bmi < 25 else 2 if bmi < 30 else 3\n",
    "    age_category = 0 if age < 30 else 1 if age < 45 else 2 if age < 60 else 3\n",
    "    bp_category = 0 if blood_pressure < 120 else 1 if blood_pressure < 140 else 2 if blood_pressure < 160 else 3\n",
    "    \n",
    "    # Interactions\n",
    "    bmi_age = bmi * age\n",
    "    bp_age = blood_pressure * age\n",
    "    bmi_bp = bmi * blood_pressure\n",
    "    \n",
    "    # Nutrition calculations\n",
    "    sodium = weight * 20\n",
    "    fat = weight * (0.15 if gender == 1 else 0.25)\n",
    "    protein = weight * 0.9\n",
    "    carbs = weight * 3\n",
    "    \n",
    "    return {\n",
    "        'bmi': bmi,\n",
    "        'bmi_category': bmi_category,\n",
    "        'age_category': age_category,\n",
    "        'bp_category': bp_category,\n",
    "        'bmi_age': bmi_age,\n",
    "        'bp_age': bp_age,\n",
    "        'bmi_bp': bmi_bp,\n",
    "        'sodium': sodium,\n",
    "        'fat': fat,\n",
    "        'protein': protein,\n",
    "        'carbs': carbs\n",
    "    }\n",
    "\n",
    "def predict_h5_model():\n",
    "    try:\n",
    "        model = tf.keras.models.load_model('models/disease-prediction-tf-model.h5')\n",
    "        \n",
    "        # User input\n",
    "        user_input = {\n",
    "            'height': 170,\n",
    "            'weight': 65,\n",
    "            'gender': 1,\n",
    "            'age': 25,\n",
    "            'blood_pressure': 120,\n",
    "            'cholesterol': 180,\n",
    "            'blood_glucose': 90\n",
    "        }\n",
    "        \n",
    "        # Display user input\n",
    "        print(\"\\n=== User Body Metrics Input ===\")\n",
    "        print(f\"Height: {user_input['height']} cm\")\n",
    "        print(f\"Weight: {user_input['weight']} kg\")\n",
    "        print(f\"Gender: {'Male' if user_input['gender'] == 1 else 'Female'}\")\n",
    "        print(f\"Age: {user_input['age']} years old\")\n",
    "        print(f\"Blood Pressure: {user_input['blood_pressure']} mmHg\")\n",
    "        print(f\"Cholesterol: {user_input['cholesterol']} mg/dL\")\n",
    "        print(f\"Blood Glucose: {user_input['blood_glucose']} mg/dL\")\n",
    "        \n",
    "        # Calculate derived features\n",
    "        derived = calculate_derived_features(\n",
    "            user_input['height'],\n",
    "            user_input['weight'],\n",
    "            user_input['gender'],\n",
    "            user_input['age'],\n",
    "            user_input['blood_pressure'],\n",
    "            user_input['cholesterol'],\n",
    "            user_input['blood_glucose']\n",
    "        )\n",
    "        \n",
    "        # Prepare features (15 features sesuai model)\n",
    "        features = np.array([[ \n",
    "            user_input['height'],\n",
    "            user_input['weight'],\n",
    "            user_input['gender'],\n",
    "            user_input['age'],\n",
    "            user_input['blood_pressure'],\n",
    "            user_input['cholesterol'],\n",
    "            user_input['blood_glucose'],\n",
    "            derived['bmi'],\n",
    "            derived['bmi_category'],\n",
    "            derived['age_category'],\n",
    "            derived['bp_category'],\n",
    "            derived['bmi_age'],\n",
    "            derived['bp_age'],\n",
    "            derived['bmi_bp'],\n",
    "            derived['sodium']\n",
    "        ]])\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(features)\n",
    "        diseases = ['Anemia', 'Kolesterol', 'CKD', 'Diabetes', 'Jantung', \n",
    "                   'Hipertensi', 'MS', 'NAFLD', 'Obesitas', 'Stroke']\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n=== Analysis Result ===\")\n",
    "        print(f\"\\nBMI: {derived['bmi']:.1f}\")\n",
    "        print(f\"Status BMI: {'Thin' if derived['bmi'] < 18.5 else 'Normal' if derived['bmi'] < 25 else 'Fatk' if derived['bmi'] < 30 else 'Obesity'}\")\n",
    "        \n",
    "        print(\"\\nBody Nutrition:\")\n",
    "        print(f\"Sodium: {derived['sodium']:.1f} mg\")\n",
    "        print(f\"Fat: {derived['fat']:.1f} g\")\n",
    "        print(f\"Protein: {derived['protein']:.1f} g\")\n",
    "        print(f\"Carbohydrate: {derived['carbs']:.1f} g\")\n",
    "        \n",
    "        print(\"\\nDisease Risk:\")\n",
    "        for disease, prob in zip(diseases, predictions[0]):\n",
    "            print(f\"{disease}: {prob:.1f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "predict_h5_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 256)               4096      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47434 (185.29 KB)\n",
      "Trainable params: 46666 (182.29 KB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models\\disease-prediction-tf-model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirement-model1.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
