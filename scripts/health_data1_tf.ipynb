{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: c:\\Users\\Dana\\Documents\\Kuliah\\Bangkit\\Capstone-C242-PS384_Project01\n",
      "\n",
      "Available files in directory:\n",
      "- anemia-dataset.csv\n",
      "- cholesterol-dataset.csv\n",
      "- chronic-kidney-disease-dataset.csv\n",
      "- combined_dataset.csv\n",
      "- diabetes-dataset.csv\n",
      "- health_data1_combined.csv\n",
      "- heart-disease-dataset.csv\n",
      "- hypertension-dataset.csv\n",
      "- metabolic-syndrome-dataset.csv\n",
      "- nafld1-dataset.csv\n",
      "- nafld2-dataset.csv\n",
      "- nwtco-dataset.csv\n",
      "- obesity-dataset.csv\n",
      "- stroke-dataset.csv\n",
      "\n",
      "Processing: anemia-dataset.csv\n",
      "Successfully loaded anemia-dataset.csv\n",
      "First few rows:\n",
      "   Gender  Hemoglobin   MCH  MCHC   MCV  Result\n",
      "0       1        14.9  22.7  29.1  83.7       0\n",
      "1       0        15.9  25.4  28.3  72.0       0\n",
      "2       0         9.0  21.5  29.6  71.2       1\n",
      "3       0        14.9  16.0  31.4  87.5       0\n",
      "4       1        14.7  22.0  28.2  99.5       0\n",
      "\n",
      "Processing: cholesterol-dataset.csv\n",
      "Successfully loaded cholesterol-dataset.csv\n",
      "First few rows:\n",
      "   age  sex  cp  trestbps  fbs  restecg  thalach  exang  oldpeak  slope ca  \\\n",
      "0   63    1   1       145    1        2      150      0      2.3      3  0   \n",
      "1   67    1   4       160    0        2      108      1      1.5      2  3   \n",
      "2   67    1   4       120    0        2      129      1      2.6      2  2   \n",
      "3   37    1   3       130    0        0      187      0      3.5      3  0   \n",
      "4   41    0   2       130    0        2      172      0      1.4      1  0   \n",
      "\n",
      "  thal  num  chol  \n",
      "0    6    0   233  \n",
      "1    3    2   286  \n",
      "2    7    1   229  \n",
      "3    3    0   250  \n",
      "4    3    0   204  \n",
      "\n",
      "Processing: chronic-kidney-disease-dataset.csv\n",
      "Successfully loaded chronic-kidney-disease-dataset.csv\n",
      "First few rows:\n",
      "   id   age    bp     sg   al   su     rbc        pc         pcc          ba  \\\n",
      "0   0  48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "1   1   7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "2   2  62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
      "3   3  48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
      "4   4  51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
      "\n",
      "   ...  pcv    wc   rc  htn   dm  cad appet   pe  ane classification  \n",
      "0  ...   44  7800  5.2  yes  yes   no  good   no   no            ckd  \n",
      "1  ...   38  6000  NaN   no   no   no  good   no   no            ckd  \n",
      "2  ...   31  7500  NaN   no  yes   no  poor   no  yes            ckd  \n",
      "3  ...   32  6700  3.9  yes   no   no  poor  yes  yes            ckd  \n",
      "4  ...   35  7300  4.6   no   no   no  good   no   no            ckd  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "Processing: combined_dataset.csv\n",
      "Successfully loaded combined_dataset.csv\n",
      "First few rows:\n",
      "   height  weight  gender  age  bp  bc     bg  bmi  sodium  fat  ...  anemia  \\\n",
      "0     NaN     NaN     0.0  NaN NaN NaN  104.3  NaN     NaN  NaN  ...     0.0   \n",
      "1     NaN     NaN     0.0  NaN NaN NaN  111.3  NaN     NaN  NaN  ...     0.0   \n",
      "2     NaN     NaN     0.0  NaN NaN NaN   63.0  NaN     NaN  NaN  ...     1.0   \n",
      "3     NaN     NaN     0.0  NaN NaN NaN  104.3  NaN     NaN  NaN  ...     0.0   \n",
      "4     NaN     NaN     0.0  NaN NaN NaN  102.9  NaN     NaN  NaN  ...     0.0   \n",
      "\n",
      "   cholesterol  ckd  diabetes  heart  hypertension  ms  nafld  obesity  stroke  \n",
      "0          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "1          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "2          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "3          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "4          0.0    0         0    0.0           0.0   0    0.0        0     0.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Processing: diabetes-dataset.csv\n",
      "Successfully loaded diabetes-dataset.csv\n",
      "First few rows:\n",
      "    Age  Sex  HighChol  CholCheck   BMI  Smoker  HeartDiseaseorAttack  \\\n",
      "0   4.0  1.0       0.0        1.0  26.0     0.0                   0.0   \n",
      "1  12.0  1.0       1.0        1.0  26.0     1.0                   0.0   \n",
      "2  13.0  1.0       0.0        1.0  26.0     0.0                   0.0   \n",
      "3  11.0  1.0       1.0        1.0  28.0     1.0                   0.0   \n",
      "4   8.0  0.0       0.0        1.0  29.0     1.0                   0.0   \n",
      "\n",
      "   PhysActivity  Fruits  Veggies  HvyAlcoholConsump  GenHlth  MentHlth  \\\n",
      "0           1.0     0.0      1.0                0.0      3.0       5.0   \n",
      "1           0.0     1.0      0.0                0.0      3.0       0.0   \n",
      "2           1.0     1.0      1.0                0.0      1.0       0.0   \n",
      "3           1.0     1.0      1.0                0.0      3.0       0.0   \n",
      "4           1.0     1.0      1.0                0.0      2.0       0.0   \n",
      "\n",
      "   PhysHlth  DiffWalk  Stroke  HighBP  Diabetes  \n",
      "0      30.0       0.0     0.0     1.0       0.0  \n",
      "1       0.0       0.0     1.0     1.0       0.0  \n",
      "2      10.0       0.0     0.0     0.0       0.0  \n",
      "3       3.0       0.0     0.0     1.0       0.0  \n",
      "4       0.0       0.0     0.0     0.0       0.0  \n",
      "\n",
      "Processing: health_data1_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dana\\AppData\\Local\\Temp\\ipykernel_5048\\2548305139.py:42: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded health_data1_combined.csv\n",
      "First few rows:\n",
      "  gender  hemoglobin  age  blood_pressure  cholesterol  glucose  bmi  height  \\\n",
      "0    1.0        14.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "1    0.0        15.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "2    0.0         9.0  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "3    0.0        14.9  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "4    1.0        14.7  NaN             NaN          NaN      NaN  NaN     NaN   \n",
      "\n",
      "   weight  HDL  Height  Weight  \n",
      "0     NaN  NaN     NaN     NaN  \n",
      "1     NaN  NaN     NaN     NaN  \n",
      "2     NaN  NaN     NaN     NaN  \n",
      "3     NaN  NaN     NaN     NaN  \n",
      "4     NaN  NaN     NaN     NaN  \n",
      "\n",
      "Processing: heart-disease-dataset.csv\n",
      "Successfully loaded heart-disease-dataset.csv\n",
      "First few rows:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
      "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
      "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
      "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
      "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   2     3       0  \n",
      "1   0     3       0  \n",
      "2   0     3       0  \n",
      "3   1     3       0  \n",
      "4   3     2       0  \n",
      "\n",
      "Processing: hypertension-dataset.csv\n",
      "Successfully loaded hypertension-dataset.csv\n",
      "First few rows:\n",
      "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0  57.0  1.0   3       145   233    1        0      150      0      2.3   \n",
      "1  64.0  0.0   2       130   250    0        1      187      0      3.5   \n",
      "2  52.0  1.0   1       130   204    0        0      172      0      1.4   \n",
      "3  56.0  0.0   1       120   236    0        1      178      0      0.8   \n",
      "4  66.0  0.0   0       120   354    0        1      163      1      0.6   \n",
      "\n",
      "   slope  ca  thal  target  \n",
      "0      0   0     1       1  \n",
      "1      0   0     2       1  \n",
      "2      2   0     2       1  \n",
      "3      2   0     2       1  \n",
      "4      2   0     2       1  \n",
      "\n",
      "Processing: metabolic-syndrome-dataset.csv\n",
      "Successfully loaded metabolic-syndrome-dataset.csv\n",
      "First few rows:\n",
      "    seqn  Age     Sex  Marital  Income   Race  WaistCirc   BMI  Albuminuria  \\\n",
      "0  62161   22    Male   Single  8200.0  White       81.0  23.3            0   \n",
      "1  62164   44  Female  Married  4500.0  White       80.1  23.2            0   \n",
      "2  62169   21    Male   Single   800.0  Asian       69.6  20.1            0   \n",
      "3  62172   43  Female   Single  2000.0  Black      120.4  33.3            0   \n",
      "4  62177   51    Male  Married     NaN  Asian       81.1  20.1            0   \n",
      "\n",
      "   UrAlbCr  UricAcid  BloodGlucose  HDL  Triglycerides  MetabolicSyndrome  \n",
      "0     3.88       4.9            92   41             84                  0  \n",
      "1     8.55       4.5            82   28             56                  0  \n",
      "2     5.07       5.4           107   43             78                  0  \n",
      "3     5.22       5.0           104   73            141                  0  \n",
      "4     8.13       5.0            95   43            126                  0  \n",
      "\n",
      "Processing: nafld1-dataset.csv\n",
      "Successfully loaded nafld1-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  id  age  male  weight  height        bmi  case.id  futime  \\\n",
      "0        3631   1   57     0    60.0   163.0  22.690939  10630.0    6261   \n",
      "1        8458   2   67     0    70.4   168.0  24.884028  14817.0     624   \n",
      "2        6298   3   53     1   105.8   186.0  30.453537      3.0    1783   \n",
      "3       15398   4   56     1   109.3   170.0  37.830100   6628.0    3143   \n",
      "4       13261   5   68     1     NaN     NaN        NaN   1871.0    1836   \n",
      "\n",
      "   status  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       1  \n",
      "\n",
      "Processing: nafld2-dataset.csv\n",
      "Successfully loaded nafld2-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  id  days  test  value\n",
      "0      135077   1  -459   hdl   75.0\n",
      "1      313143   1  -459  chol   75.0\n",
      "2      135078   1   183   hdl   64.0\n",
      "3      313144   1   183  chol   64.0\n",
      "4      135079   1  2030   hdl   74.0\n",
      "\n",
      "Processing: nwtco-dataset.csv\n",
      "Successfully loaded nwtco-dataset.csv\n",
      "First few rows:\n",
      "   Unnamed: 0  seqno  instit  histol  stage  study  rel  edrel  age  \\\n",
      "0           1      1       2       2      1      3    0   6075   25   \n",
      "1           2      2       1       1      2      3    0   4121   50   \n",
      "2           3      3       2       2      1      3    0   6069    9   \n",
      "3           4      4       2       1      4      3    0   6200   28   \n",
      "4           5      5       2       2      2      3    0   1244   55   \n",
      "\n",
      "   in.subcohort  \n",
      "0         False  \n",
      "1         False  \n",
      "2         False  \n",
      "3          True  \n",
      "4         False  \n",
      "\n",
      "Processing: obesity-dataset.csv\n",
      "Successfully loaded obesity-dataset.csv\n",
      "First few rows:\n",
      "   ID  Age  Gender  Height  Weight   BMI          Label\n",
      "0   1   25    Male     175      80  25.3  Normal Weight\n",
      "1   2   30  Female     160      60  22.5  Normal Weight\n",
      "2   3   35    Male     180      90  27.3     Overweight\n",
      "3   4   40  Female     150      50  20.0    Underweight\n",
      "4   5   45    Male     190     100  31.2          Obese\n",
      "\n",
      "Processing: stroke-dataset.csv\n",
      "Successfully loaded stroke-dataset.csv\n",
      "First few rows:\n",
      "   sex   age  hypertension  heart_disease  ever_married  work_type  \\\n",
      "0  1.0  63.0             0              1             1          4   \n",
      "1  1.0  42.0             0              1             1          4   \n",
      "2  0.0  61.0             0              0             1          4   \n",
      "3  1.0  41.0             1              0             1          3   \n",
      "4  1.0  85.0             0              0             1          4   \n",
      "\n",
      "   Residence_type  avg_glucose_level   bmi  smoking_status  stroke  \n",
      "0               1             228.69  36.6               1       1  \n",
      "1               0             105.92  32.5               0       1  \n",
      "2               1             171.23  34.4               1       1  \n",
      "3               0             174.12  24.0               0       1  \n",
      "4               1             186.21  29.0               1       1  \n",
      "File inside the dataset folder: ['anemia-dataset.csv', 'cholesterol-dataset.csv', 'chronic-kidney-disease-dataset.csv', 'combined_dataset.csv', 'diabetes-dataset.csv', 'health_data1_combined.csv', 'heart-disease-dataset.csv', 'hypertension-dataset.csv', 'metabolic-syndrome-dataset.csv', 'nafld1-dataset.csv', 'nafld2-dataset.csv', 'nwtco-dataset.csv', 'obesity-dataset.csv', 'stroke-dataset.csv']\n",
      "All files are available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Step 0, chek dataset availability\n",
    "\n",
    "def set_project_directory():\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    if os.path.basename(current_dir) == 'scripts':\n",
    "        os.chdir('..')\n",
    "    \n",
    "    print(f\"Working directory set to: {os.getcwd()}\")\n",
    "\n",
    "def check_data_directory():\n",
    "    data_dir = os.path.join('dataset', 'health_data1')\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Directory not found: {data_dir}\")\n",
    "        print(\"Available directories:\", os.listdir('.'))\n",
    "        return False\n",
    "    \n",
    "    print(\"\\nAvailable files in directory:\")\n",
    "    for file in os.listdir(data_dir):\n",
    "        print(f\"- {file}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            return pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.XPT'):\n",
    "            return pd.read_sas(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_path}\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "set_project_directory()\n",
    "\n",
    "if check_data_directory():\n",
    "    data_directory = os.path.join('dataset', 'health_data1')\n",
    "    \n",
    "    for filename in os.listdir(data_directory):\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        data = load_data(file_path)\n",
    "        if data is not None:\n",
    "            print(f\"Successfully loaded {filename}\")\n",
    "            print(\"First few rows:\")\n",
    "            print(data.head())\n",
    "else:\n",
    "    print(\"Please check your directory structure and file locations\")\n",
    "\n",
    "folder_path = 'dataset/health_data1/'\n",
    "\n",
    "try:\n",
    "    print(\"File inside the dataset folder:\", os.listdir(folder_path))\n",
    "    required_files = [\n",
    "        'anemia-dataset.csv',\n",
    "        'cholesterol-dataset.csv',\n",
    "        'chronic-kidney-disease-dataset.csv',\n",
    "        'diabetes-dataset.csv',\n",
    "        'heart-disease-dataset.csv',\n",
    "        'hypertension-dataset.csv',\n",
    "        'metabolic-syndrome-dataset.csv',\n",
    "        'nafld1-dataset.csv',\n",
    "        'obesity-dataset.csv',\n",
    "        'stroke-dataset.csv'\n",
    "    ]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"Missing files: {', '.join(missing_files)}\")\n",
    "    else:\n",
    "        print(\"All files are available.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Folder missing: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset succesfully savd on dataset/health_data1/combined_dataset.csv\n",
      "\n",
      "combined dataset inform:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160892 entries, 0 to 160891\n",
      "Data columns (total 22 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   height        14489 non-null   float64\n",
      " 1   weight        12871 non-null   float64\n",
      " 2   gender        160492 non-null  float64\n",
      " 3   age           159462 non-null  float64\n",
      " 4   bp            27799 non-null   float64\n",
      " 5   bc            68321 non-null   float64\n",
      " 6   bg            31589 non-null   float64\n",
      " 7   bmi           126673 non-null  float64\n",
      " 8   sodium        313 non-null     float64\n",
      " 9   fat           0 non-null       float64\n",
      " 10  protein       0 non-null       float64\n",
      " 11  carbs         0 non-null       float64\n",
      " 12  anemia        160892 non-null  float64\n",
      " 13  cholesterol   160892 non-null  float64\n",
      " 14  ckd           160892 non-null  int64  \n",
      " 15  diabetes      160892 non-null  int64  \n",
      " 16  heart         160892 non-null  float64\n",
      " 17  hypertension  160892 non-null  float64\n",
      " 18  ms            160892 non-null  int64  \n",
      " 19  nafld         160892 non-null  float64\n",
      " 20  obesity       160892 non-null  int64  \n",
      " 21  stroke        160892 non-null  float64\n",
      "dtypes: float64(18), int64(4)\n",
      "memory usage: 27.0 MB\n",
      "None\n",
      "\n",
      "combined dataset stats:\n",
      "             height        weight         gender            age            bp  \\\n",
      "count  14489.000000  12871.000000  160492.000000  159462.000000  27799.000000   \n",
      "mean     169.413624     86.127947       0.145771      33.217437    130.825353   \n",
      "std       10.386545     22.435919       0.352877      26.020889     18.691371   \n",
      "min      120.000000     10.000000       0.000000      -9.000000     50.000000   \n",
      "25%      162.000000     69.900000       0.000000       9.000000    120.000000   \n",
      "50%      169.000000     83.700000       0.000000      28.000000    130.000000   \n",
      "75%      177.000000     99.100000       0.000000      56.000000    140.000000   \n",
      "max      215.000000    181.700000       1.000000     112.000000    200.000000   \n",
      "\n",
      "                 bc            bg            bmi      sodium  fat  ...  \\\n",
      "count  68321.000000  31589.000000  126673.000000  313.000000  0.0  ...   \n",
      "mean      98.870904     18.572494      30.026370  137.528754  NaN  ...   \n",
      "std      124.988318     42.528903       7.026505   10.408752  NaN  ...   \n",
      "min        0.000000      0.000000       3.900000    4.500000  NaN  ...   \n",
      "25%        0.000000      0.000000      25.000000  135.000000  NaN  ...   \n",
      "50%        0.000000      0.000000      29.000000  138.000000  NaN  ...   \n",
      "75%      227.000000      1.000000      33.300000  142.000000  NaN  ...   \n",
      "max      564.000000    490.000000      98.000000  163.000000  NaN  ...   \n",
      "\n",
      "              anemia    cholesterol            ckd       diabetes  \\\n",
      "count  160892.000000  160892.000000  160892.000000  160892.000000   \n",
      "mean        0.004226       0.231926       0.001541       0.220521   \n",
      "std         0.064874       0.422063       0.039231       0.414599   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "               heart   hypertension             ms          nafld   obesity  \\\n",
      "count  160892.000000  160892.000000  160892.000000  160892.000000  160892.0   \n",
      "mean        0.035956       0.391573       0.005109       0.008478       0.0   \n",
      "std         0.186181       0.488104       0.071295       0.091684       0.0   \n",
      "min         0.000000       0.000000       0.000000       0.000000       0.0   \n",
      "25%         0.000000       0.000000       0.000000       0.000000       0.0   \n",
      "50%         0.000000       0.000000       0.000000       0.000000       0.0   \n",
      "75%         0.000000       1.000000       0.000000       0.000000       0.0   \n",
      "max         1.000000       1.000000       1.000000       1.000000       0.0   \n",
      "\n",
      "              stroke  \n",
      "count  160892.000000  \n",
      "mean        0.127166  \n",
      "std         0.333160  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         1.000000  \n",
      "\n",
      "[8 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# preprocess available data and merge it in the end\n",
    "columns = [\n",
    "    \"height\", \"weight\", \"gender\", \"age\", \"bp\", \"bc\", \"bg\", \"bmi\", \"sodium\", \n",
    "    \"fat\", \"protein\", \"carbs\", \"anemia\", \"cholesterol\", \"ckd\", \"diabetes\", \n",
    "    \"heart\", \"hypertension\", \"ms\", \"nafld\", \"obesity\", \"stroke\"\n",
    "]\n",
    "\n",
    "def create_data_dict(**kwargs):\n",
    "    base_dict = {\n",
    "        \"height\": np.nan, \"weight\": np.nan, \"gender\": np.nan, \"age\": np.nan,\n",
    "        \"bp\": np.nan, \"bc\": np.nan, \"bg\": np.nan, \"bmi\": np.nan,\n",
    "        \"sodium\": np.nan, \"fat\": np.nan, \"protein\": np.nan, \"carbs\": np.nan,\n",
    "        \"anemia\": 0, \"cholesterol\": 0, \"ckd\": 0, \"diabetes\": 0,\n",
    "        \"heart\": 0, \"hypertension\": 0, \"ms\": 0, \"nafld\": 0, \"obesity\": 0, \"stroke\": 0\n",
    "    }\n",
    "    base_dict.update({k: v for k, v in kwargs.items() if v is not None})\n",
    "    return base_dict\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# 1. Anemia dataset\n",
    "anemia_data = pd.read_csv(os.path.join(folder_path, \"anemia-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        gender=1 if row[\"Gender\"] == \"Male\" else 0,\n",
    "        bg=round(row[\"Hemoglobin\"] * 7, 1),\n",
    "        anemia=row[\"Result\"]\n",
    "    )\n",
    "    for _, row in anemia_data.iterrows()\n",
    "])\n",
    "\n",
    "# 2. Cholesterol dataset\n",
    "chol_data = pd.read_csv(os.path.join(folder_path, \"cholesterol-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=row[\"sex\"],\n",
    "        bp=row[\"trestbps\"],\n",
    "        bc=row[\"chol\"],\n",
    "        bg=120 if row[\"fbs\"] == 1 else 100,\n",
    "        cholesterol=1 if row[\"chol\"] > 240 else 0\n",
    "    )\n",
    "    for _, row in chol_data.iterrows()\n",
    "])\n",
    "\n",
    "# 3. Chronic Kidney Disease dataset\n",
    "ckd_data = pd.read_csv(os.path.join(folder_path, \"chronic-kidney-disease-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        bp=row[\"bp\"],\n",
    "        bg=row[\"bgr\"],\n",
    "        sodium=row[\"sod\"],\n",
    "        anemia=1 if row[\"ane\"] == \"yes\" else 0,\n",
    "        ckd=1 if row[\"classification\"] == \"ckd\" else 0,\n",
    "        diabetes=1 if row[\"dm\"] == \"yes\" else 0,\n",
    "        heart=1 if row[\"cad\"] == \"yes\" else 0,\n",
    "        hypertension=1 if row[\"htn\"] == \"yes\" else 0\n",
    "    )\n",
    "    for _, row in ckd_data.iterrows()\n",
    "])\n",
    "\n",
    "# 4. Diabetes dataset\n",
    "diabetes_data = pd.read_csv(os.path.join(folder_path, \"diabetes-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"Age\"],\n",
    "        gender=1 if row[\"Sex\"] == \"Male\" else 0,\n",
    "        bmi=row[\"BMI\"],\n",
    "        cholesterol=row[\"HighChol\"],\n",
    "        diabetes=1 if row[\"Diabetes\"] == 1 else 0,\n",
    "        hypertension=row[\"HighBP\"]\n",
    "    )\n",
    "    for _, row in diabetes_data.iterrows()\n",
    "])\n",
    "\n",
    "# 5. Heart Disease dataset\n",
    "heart_data = pd.read_csv(os.path.join(folder_path, \"heart-disease-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=1 if row[\"sex\"] == 1 else 0,\n",
    "        bp=row[\"trestbps\"],\n",
    "        bc=row[\"chol\"],\n",
    "        bg=120 if row[\"fbs\"] == 1 else 100,\n",
    "        heart=1 if row[\"target\"] == 1 else 0\n",
    "    )\n",
    "    for _, row in heart_data.iterrows()\n",
    "])\n",
    "\n",
    "# 6. Hypertension dataset\n",
    "hypertension_data = pd.read_csv(os.path.join(folder_path, \"hypertension-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=1 if row[\"sex\"] == 1 else 0,\n",
    "        bp=row[\"trestbps\"],\n",
    "        bc=row[\"chol\"],\n",
    "        bg=row[\"fbs\"],\n",
    "        hypertension=1 if row[\"target\"] == 1 else 0\n",
    "    )\n",
    "    for _, row in hypertension_data.iterrows()\n",
    "])\n",
    "\n",
    "# 7. Metabolic Syndrome dataset\n",
    "ms_data = pd.read_csv(os.path.join(folder_path, \"metabolic-syndrome-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"Age\"],\n",
    "        gender=1 if row[\"Sex\"] == \"Male\" else 0,\n",
    "        bg=row[\"BloodGlucose\"],\n",
    "        bmi=row[\"BMI\"],\n",
    "        ms=1 if row[\"MetabolicSyndrome\"] == 1 else 0\n",
    "    )\n",
    "    for _, row in ms_data.iterrows()\n",
    "])\n",
    "\n",
    "# 8. NAFLD dataset\n",
    "nafld_data = pd.read_csv(os.path.join(folder_path, \"nafld1-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=row[\"male\"],\n",
    "        weight=row[\"weight\"],\n",
    "        height=row[\"height\"],\n",
    "        bmi=round(row[\"bmi\"],1),\n",
    "        nafld=row[\"status\"]\n",
    "    )\n",
    "    for _, row in nafld_data.iterrows()\n",
    "])\n",
    "\n",
    "# 9. Obesity dataset\n",
    "obesity_data = pd.read_csv(os.path.join(folder_path, \"obesity-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"Age\"],\n",
    "        gender=1 if row[\"Gender\"] == \"Male\" else 0,\n",
    "        weight=row[\"Weight\"],\n",
    "        height=row[\"Height\"],\n",
    "        bmi=row[\"BMI\"],\n",
    "        obesity=1 if row[\"Label\"] == \"Obesity\" else 0\n",
    "    )\n",
    "    for _, row in obesity_data.iterrows()\n",
    "])\n",
    "\n",
    "# 10. Stroke dataset\n",
    "stroke_data = pd.read_csv(os.path.join(folder_path, \"stroke-dataset.csv\"))\n",
    "all_data.extend([\n",
    "    create_data_dict(\n",
    "        age=row[\"age\"],\n",
    "        gender=1 if row[\"sex\"] == \"Male\" else 0,\n",
    "        bc=row[\"heart_disease\"],\n",
    "        bmi=row[\"bmi\"],\n",
    "        heart=row[\"heart_disease\"],\n",
    "        hypertension=row[\"hypertension\"],\n",
    "        stroke=row[\"stroke\"]\n",
    "    )\n",
    "    for _, row in stroke_data.iterrows()\n",
    "])\n",
    "\n",
    "combined_data = pd.DataFrame(all_data)\n",
    "\n",
    "output_path = os.path.join(folder_path, \"combined_dataset.csv\")\n",
    "combined_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Combined dataset succesfully savd on {output_path}\")\n",
    "print(\"\\ncombined dataset inform:\")\n",
    "print(combined_data.info())\n",
    "print(\"\\ncombined dataset stats:\")\n",
    "print(combined_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing data...\n",
      "Dataset loaded with shape: (160892, 22)\n",
      "Processing features...\n",
      "Handling missing values...\n",
      "Found 9 numeric features and 0 categorical features\n",
      "Processing target variables...\n",
      "Data processing completed!\n",
      "\n",
      "=== Starting Model Training Process ===\n",
      "WARNING:tensorflow:From c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "\n",
      "Starting combined model training...\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "41/41 [==============================] - 4s 32ms/step - loss: 0.4107 - accuracy: 0.2659 - precision: 0.2557 - recall: 0.4690 - auc: 0.7835 - val_loss: 0.4018 - val_accuracy: 0.2297 - val_precision: 0.5546 - val_recall: 0.4021 - val_auc: 0.9189 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1919 - accuracy: 0.2528 - precision: 0.5526 - recall: 0.4276 - auc: 0.9282 - val_loss: 0.2463 - val_accuracy: 0.1928 - val_precision: 0.6145 - val_recall: 0.2580 - val_auc: 0.9361 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1753 - accuracy: 0.2496 - precision: 0.5815 - recall: 0.4478 - auc: 0.9391 - val_loss: 0.2051 - val_accuracy: 0.1928 - val_precision: 0.5964 - val_recall: 0.3004 - val_auc: 0.9376 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1687 - accuracy: 0.2530 - precision: 0.6016 - recall: 0.4887 - auc: 0.9441 - val_loss: 0.1932 - val_accuracy: 0.1939 - val_precision: 0.5490 - val_recall: 0.3121 - val_auc: 0.9333 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1643 - accuracy: 0.2507 - precision: 0.6177 - recall: 0.5179 - auc: 0.9474 - val_loss: 0.1912 - val_accuracy: 0.1969 - val_precision: 0.5111 - val_recall: 0.3314 - val_auc: 0.9287 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1613 - accuracy: 0.2536 - precision: 0.6230 - recall: 0.5428 - auc: 0.9497 - val_loss: 0.1870 - val_accuracy: 0.1992 - val_precision: 0.5009 - val_recall: 0.3629 - val_auc: 0.9304 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1595 - accuracy: 0.2483 - precision: 0.6272 - recall: 0.5456 - auc: 0.9510 - val_loss: 0.1809 - val_accuracy: 0.2000 - val_precision: 0.5037 - val_recall: 0.4775 - val_auc: 0.9346 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1579 - accuracy: 0.2497 - precision: 0.6295 - recall: 0.5520 - auc: 0.9522 - val_loss: 0.1742 - val_accuracy: 0.2022 - val_precision: 0.5195 - val_recall: 0.5658 - val_auc: 0.9398 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1566 - accuracy: 0.2514 - precision: 0.6312 - recall: 0.5609 - auc: 0.9532 - val_loss: 0.1701 - val_accuracy: 0.2022 - val_precision: 0.5318 - val_recall: 0.6542 - val_auc: 0.9444 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1556 - accuracy: 0.2520 - precision: 0.6322 - recall: 0.5636 - auc: 0.9539 - val_loss: 0.1656 - val_accuracy: 0.2053 - val_precision: 0.5457 - val_recall: 0.7039 - val_auc: 0.9478 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1546 - accuracy: 0.2498 - precision: 0.6320 - recall: 0.5720 - auc: 0.9545 - val_loss: 0.1615 - val_accuracy: 0.2039 - val_precision: 0.5643 - val_recall: 0.6997 - val_auc: 0.9509 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1539 - accuracy: 0.2467 - precision: 0.6319 - recall: 0.5683 - auc: 0.9549 - val_loss: 0.1577 - val_accuracy: 0.2177 - val_precision: 0.5732 - val_recall: 0.7416 - val_auc: 0.9540 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1531 - accuracy: 0.2503 - precision: 0.6307 - recall: 0.5825 - auc: 0.9554 - val_loss: 0.1540 - val_accuracy: 0.2298 - val_precision: 0.5902 - val_recall: 0.7184 - val_auc: 0.9555 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1527 - accuracy: 0.2513 - precision: 0.6326 - recall: 0.5789 - auc: 0.9556 - val_loss: 0.1516 - val_accuracy: 0.2541 - val_precision: 0.6039 - val_recall: 0.7033 - val_auc: 0.9567 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1518 - accuracy: 0.2539 - precision: 0.6341 - recall: 0.5856 - auc: 0.9560 - val_loss: 0.1505 - val_accuracy: 0.2412 - val_precision: 0.6024 - val_recall: 0.7124 - val_auc: 0.9572 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1512 - accuracy: 0.2550 - precision: 0.6343 - recall: 0.5935 - auc: 0.9565 - val_loss: 0.1494 - val_accuracy: 0.2343 - val_precision: 0.6114 - val_recall: 0.6928 - val_auc: 0.9577 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1508 - accuracy: 0.2565 - precision: 0.6326 - recall: 0.5900 - auc: 0.9567 - val_loss: 0.1485 - val_accuracy: 0.2466 - val_precision: 0.6166 - val_recall: 0.6727 - val_auc: 0.9579 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1501 - accuracy: 0.2569 - precision: 0.6345 - recall: 0.5946 - auc: 0.9571 - val_loss: 0.1488 - val_accuracy: 0.2640 - val_precision: 0.6042 - val_recall: 0.7167 - val_auc: 0.9582 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1498 - accuracy: 0.2607 - precision: 0.6322 - recall: 0.6006 - auc: 0.9574 - val_loss: 0.1477 - val_accuracy: 0.2640 - val_precision: 0.6125 - val_recall: 0.6866 - val_auc: 0.9584 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1493 - accuracy: 0.2627 - precision: 0.6333 - recall: 0.6014 - auc: 0.9576 - val_loss: 0.1470 - val_accuracy: 0.2479 - val_precision: 0.6165 - val_recall: 0.6822 - val_auc: 0.9588 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1492 - accuracy: 0.2600 - precision: 0.6328 - recall: 0.6031 - auc: 0.9576 - val_loss: 0.1467 - val_accuracy: 0.2664 - val_precision: 0.6263 - val_recall: 0.6533 - val_auc: 0.9589 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1488 - accuracy: 0.2637 - precision: 0.6323 - recall: 0.6077 - auc: 0.9579 - val_loss: 0.1461 - val_accuracy: 0.2568 - val_precision: 0.6200 - val_recall: 0.6719 - val_auc: 0.9593 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1482 - accuracy: 0.2642 - precision: 0.6336 - recall: 0.6076 - auc: 0.9583 - val_loss: 0.1458 - val_accuracy: 0.2634 - val_precision: 0.6187 - val_recall: 0.6790 - val_auc: 0.9594 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1479 - accuracy: 0.2641 - precision: 0.6341 - recall: 0.6117 - auc: 0.9584 - val_loss: 0.1456 - val_accuracy: 0.2590 - val_precision: 0.6313 - val_recall: 0.6462 - val_auc: 0.9594 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1477 - accuracy: 0.2693 - precision: 0.6344 - recall: 0.6088 - auc: 0.9585 - val_loss: 0.1457 - val_accuracy: 0.2544 - val_precision: 0.6192 - val_recall: 0.6785 - val_auc: 0.9594 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1474 - accuracy: 0.2646 - precision: 0.6345 - recall: 0.6115 - auc: 0.9586 - val_loss: 0.1455 - val_accuracy: 0.2628 - val_precision: 0.6231 - val_recall: 0.6520 - val_auc: 0.9594 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1470 - accuracy: 0.2657 - precision: 0.6328 - recall: 0.6119 - auc: 0.9588 - val_loss: 0.1449 - val_accuracy: 0.2830 - val_precision: 0.6186 - val_recall: 0.6834 - val_auc: 0.9598 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1467 - accuracy: 0.2700 - precision: 0.6353 - recall: 0.6158 - auc: 0.9591 - val_loss: 0.1452 - val_accuracy: 0.2665 - val_precision: 0.6158 - val_recall: 0.6881 - val_auc: 0.9597 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1464 - accuracy: 0.2700 - precision: 0.6359 - recall: 0.6195 - auc: 0.9593 - val_loss: 0.1448 - val_accuracy: 0.2917 - val_precision: 0.6306 - val_recall: 0.6355 - val_auc: 0.9597 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1462 - accuracy: 0.2727 - precision: 0.6368 - recall: 0.6152 - auc: 0.9593 - val_loss: 0.1442 - val_accuracy: 0.2710 - val_precision: 0.6283 - val_recall: 0.6616 - val_auc: 0.9601 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1459 - accuracy: 0.2702 - precision: 0.6352 - recall: 0.6209 - auc: 0.9594 - val_loss: 0.1439 - val_accuracy: 0.2686 - val_precision: 0.6273 - val_recall: 0.6644 - val_auc: 0.9603 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1457 - accuracy: 0.2731 - precision: 0.6373 - recall: 0.6209 - auc: 0.9596 - val_loss: 0.1441 - val_accuracy: 0.2457 - val_precision: 0.6195 - val_recall: 0.6852 - val_auc: 0.9603 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1455 - accuracy: 0.2713 - precision: 0.6355 - recall: 0.6250 - auc: 0.9597 - val_loss: 0.1435 - val_accuracy: 0.2798 - val_precision: 0.6249 - val_recall: 0.6733 - val_auc: 0.9605 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1452 - accuracy: 0.2718 - precision: 0.6352 - recall: 0.6252 - auc: 0.9598 - val_loss: 0.1433 - val_accuracy: 0.2535 - val_precision: 0.6281 - val_recall: 0.6610 - val_auc: 0.9606 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1449 - accuracy: 0.2702 - precision: 0.6374 - recall: 0.6210 - auc: 0.9599 - val_loss: 0.1433 - val_accuracy: 0.2750 - val_precision: 0.6216 - val_recall: 0.6747 - val_auc: 0.9606 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1449 - accuracy: 0.2747 - precision: 0.6354 - recall: 0.6277 - auc: 0.9599 - val_loss: 0.1427 - val_accuracy: 0.2881 - val_precision: 0.6369 - val_recall: 0.6342 - val_auc: 0.9609 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1448 - accuracy: 0.2723 - precision: 0.6361 - recall: 0.6264 - auc: 0.9599 - val_loss: 0.1427 - val_accuracy: 0.3079 - val_precision: 0.6290 - val_recall: 0.6595 - val_auc: 0.9609 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1444 - accuracy: 0.2784 - precision: 0.6368 - recall: 0.6256 - auc: 0.9602 - val_loss: 0.1425 - val_accuracy: 0.2829 - val_precision: 0.6238 - val_recall: 0.6779 - val_auc: 0.9610 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1442 - accuracy: 0.2749 - precision: 0.6372 - recall: 0.6276 - auc: 0.9604 - val_loss: 0.1433 - val_accuracy: 0.2893 - val_precision: 0.6185 - val_recall: 0.6900 - val_auc: 0.9605 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1440 - accuracy: 0.2765 - precision: 0.6372 - recall: 0.6270 - auc: 0.9604 - val_loss: 0.1421 - val_accuracy: 0.2506 - val_precision: 0.6209 - val_recall: 0.6920 - val_auc: 0.9613 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1439 - accuracy: 0.2732 - precision: 0.6366 - recall: 0.6303 - auc: 0.9605 - val_loss: 0.1417 - val_accuracy: 0.3141 - val_precision: 0.6235 - val_recall: 0.6803 - val_auc: 0.9614 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1437 - accuracy: 0.2789 - precision: 0.6375 - recall: 0.6327 - auc: 0.9606 - val_loss: 0.1413 - val_accuracy: 0.2760 - val_precision: 0.6263 - val_recall: 0.6748 - val_auc: 0.9618 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1434 - accuracy: 0.2758 - precision: 0.6378 - recall: 0.6311 - auc: 0.9607 - val_loss: 0.1416 - val_accuracy: 0.2769 - val_precision: 0.6207 - val_recall: 0.6958 - val_auc: 0.9615 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1432 - accuracy: 0.2752 - precision: 0.6363 - recall: 0.6336 - auc: 0.9608 - val_loss: 0.1415 - val_accuracy: 0.3058 - val_precision: 0.6405 - val_recall: 0.6291 - val_auc: 0.9615 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.1430 - accuracy: 0.2770 - precision: 0.6386 - recall: 0.6273 - auc: 0.9609\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1430 - accuracy: 0.2772 - precision: 0.6384 - recall: 0.6276 - auc: 0.9609 - val_loss: 0.1414 - val_accuracy: 0.2845 - val_precision: 0.6144 - val_recall: 0.7118 - val_auc: 0.9618 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1421 - accuracy: 0.2784 - precision: 0.6395 - recall: 0.6381 - auc: 0.9615 - val_loss: 0.1399 - val_accuracy: 0.2690 - val_precision: 0.6269 - val_recall: 0.6845 - val_auc: 0.9626 - lr: 2.0000e-04\n",
      "Epoch 47/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1415 - accuracy: 0.2788 - precision: 0.6411 - recall: 0.6347 - auc: 0.9618 - val_loss: 0.1397 - val_accuracy: 0.2705 - val_precision: 0.6321 - val_recall: 0.6672 - val_auc: 0.9625 - lr: 2.0000e-04\n",
      "Epoch 48/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1413 - accuracy: 0.2769 - precision: 0.6425 - recall: 0.6353 - auc: 0.9619 - val_loss: 0.1394 - val_accuracy: 0.2677 - val_precision: 0.6274 - val_recall: 0.6859 - val_auc: 0.9629 - lr: 2.0000e-04\n",
      "Epoch 49/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1408 - accuracy: 0.2779 - precision: 0.6432 - recall: 0.6365 - auc: 0.9621 - val_loss: 0.1390 - val_accuracy: 0.2757 - val_precision: 0.6310 - val_recall: 0.6755 - val_auc: 0.9630 - lr: 2.0000e-04\n",
      "Epoch 50/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1408 - accuracy: 0.2788 - precision: 0.6426 - recall: 0.6357 - auc: 0.9621 - val_loss: 0.1390 - val_accuracy: 0.2707 - val_precision: 0.6231 - val_recall: 0.7003 - val_auc: 0.9632 - lr: 2.0000e-04\n",
      "Epoch 51/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1403 - accuracy: 0.2777 - precision: 0.6422 - recall: 0.6418 - auc: 0.9625 - val_loss: 0.1391 - val_accuracy: 0.2729 - val_precision: 0.6394 - val_recall: 0.6509 - val_auc: 0.9626 - lr: 2.0000e-04\n",
      "Epoch 52/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1404 - accuracy: 0.2787 - precision: 0.6421 - recall: 0.6395 - auc: 0.9624 - val_loss: 0.1381 - val_accuracy: 0.2715 - val_precision: 0.6373 - val_recall: 0.6593 - val_auc: 0.9632 - lr: 2.0000e-04\n",
      "Epoch 53/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1400 - accuracy: 0.2810 - precision: 0.6426 - recall: 0.6393 - auc: 0.9625 - val_loss: 0.1377 - val_accuracy: 0.2705 - val_precision: 0.6329 - val_recall: 0.6732 - val_auc: 0.9635 - lr: 2.0000e-04\n",
      "Epoch 54/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1396 - accuracy: 0.2791 - precision: 0.6444 - recall: 0.6424 - auc: 0.9627 - val_loss: 0.1374 - val_accuracy: 0.2748 - val_precision: 0.6433 - val_recall: 0.6470 - val_auc: 0.9635 - lr: 2.0000e-04\n",
      "Epoch 55/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1394 - accuracy: 0.2811 - precision: 0.6439 - recall: 0.6440 - auc: 0.9629 - val_loss: 0.1380 - val_accuracy: 0.2867 - val_precision: 0.6317 - val_recall: 0.7206 - val_auc: 0.9641 - lr: 2.0000e-04\n",
      "Epoch 56/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1393 - accuracy: 0.2833 - precision: 0.6447 - recall: 0.6485 - auc: 0.9630 - val_loss: 0.1362 - val_accuracy: 0.2756 - val_precision: 0.6379 - val_recall: 0.6696 - val_auc: 0.9643 - lr: 2.0000e-04\n",
      "Epoch 57/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1391 - accuracy: 0.2846 - precision: 0.6450 - recall: 0.6469 - auc: 0.9631 - val_loss: 0.1362 - val_accuracy: 0.2735 - val_precision: 0.6425 - val_recall: 0.6544 - val_auc: 0.9642 - lr: 2.0000e-04\n",
      "Epoch 58/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1388 - accuracy: 0.2839 - precision: 0.6468 - recall: 0.6479 - auc: 0.9632 - val_loss: 0.1380 - val_accuracy: 0.2976 - val_precision: 0.6304 - val_recall: 0.7286 - val_auc: 0.9644 - lr: 2.0000e-04\n",
      "Epoch 59/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1390 - accuracy: 0.2865 - precision: 0.6440 - recall: 0.6500 - auc: 0.9631 - val_loss: 0.1352 - val_accuracy: 0.2793 - val_precision: 0.6388 - val_recall: 0.7024 - val_auc: 0.9652 - lr: 2.0000e-04\n",
      "Epoch 60/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1389 - accuracy: 0.2832 - precision: 0.6446 - recall: 0.6481 - auc: 0.9631 - val_loss: 0.1354 - val_accuracy: 0.2792 - val_precision: 0.6392 - val_recall: 0.6881 - val_auc: 0.9648 - lr: 2.0000e-04\n",
      "Epoch 61/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1386 - accuracy: 0.2847 - precision: 0.6443 - recall: 0.6532 - auc: 0.9633 - val_loss: 0.1349 - val_accuracy: 0.2800 - val_precision: 0.6425 - val_recall: 0.6757 - val_auc: 0.9649 - lr: 2.0000e-04\n",
      "Epoch 62/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1380 - accuracy: 0.2873 - precision: 0.6464 - recall: 0.6521 - auc: 0.9637 - val_loss: 0.1356 - val_accuracy: 0.3008 - val_precision: 0.6364 - val_recall: 0.7198 - val_auc: 0.9655 - lr: 2.0000e-04\n",
      "Epoch 63/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1380 - accuracy: 0.2872 - precision: 0.6459 - recall: 0.6552 - auc: 0.9636 - val_loss: 0.1343 - val_accuracy: 0.2781 - val_precision: 0.6451 - val_recall: 0.6856 - val_auc: 0.9653 - lr: 2.0000e-04\n",
      "Epoch 64/100\n",
      "41/41 [==============================] - 1s 21ms/step - loss: 0.1376 - accuracy: 0.2887 - precision: 0.6487 - recall: 0.6541 - auc: 0.9639 - val_loss: 0.1342 - val_accuracy: 0.2971 - val_precision: 0.6414 - val_recall: 0.7049 - val_auc: 0.9658 - lr: 2.0000e-04\n",
      "Epoch 65/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1374 - accuracy: 0.2910 - precision: 0.6472 - recall: 0.6563 - auc: 0.9639 - val_loss: 0.1341 - val_accuracy: 0.2948 - val_precision: 0.6407 - val_recall: 0.7085 - val_auc: 0.9659 - lr: 2.0000e-04\n",
      "Epoch 66/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1380 - accuracy: 0.2906 - precision: 0.6456 - recall: 0.6554 - auc: 0.9636 - val_loss: 0.1336 - val_accuracy: 0.2951 - val_precision: 0.6425 - val_recall: 0.6996 - val_auc: 0.9659 - lr: 2.0000e-04\n",
      "Epoch 67/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1376 - accuracy: 0.2893 - precision: 0.6467 - recall: 0.6572 - auc: 0.9638 - val_loss: 0.1363 - val_accuracy: 0.2736 - val_precision: 0.6476 - val_recall: 0.6456 - val_auc: 0.9638 - lr: 2.0000e-04\n",
      "Epoch 68/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1377 - accuracy: 0.2920 - precision: 0.6471 - recall: 0.6552 - auc: 0.9637 - val_loss: 0.1346 - val_accuracy: 0.2916 - val_precision: 0.6417 - val_recall: 0.6872 - val_auc: 0.9651 - lr: 2.0000e-04\n",
      "Epoch 69/100\n",
      "38/41 [==========================>...] - ETA: 0s - loss: 0.1374 - accuracy: 0.2928 - precision: 0.6492 - recall: 0.6561 - auc: 0.9640\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1373 - accuracy: 0.2923 - precision: 0.6491 - recall: 0.6576 - auc: 0.9641 - val_loss: 0.1347 - val_accuracy: 0.2731 - val_precision: 0.6482 - val_recall: 0.6525 - val_auc: 0.9646 - lr: 2.0000e-04\n",
      "Epoch 70/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1367 - accuracy: 0.2884 - precision: 0.6510 - recall: 0.6549 - auc: 0.9644 - val_loss: 0.1329 - val_accuracy: 0.3087 - val_precision: 0.6467 - val_recall: 0.6967 - val_auc: 0.9662 - lr: 4.0000e-05\n",
      "Epoch 71/100\n",
      "41/41 [==============================] - 1s 20ms/step - loss: 0.1361 - accuracy: 0.2942 - precision: 0.6504 - recall: 0.6610 - auc: 0.9647 - val_loss: 0.1328 - val_accuracy: 0.3076 - val_precision: 0.6475 - val_recall: 0.6932 - val_auc: 0.9662 - lr: 4.0000e-05\n",
      "Epoch 72/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1363 - accuracy: 0.2935 - precision: 0.6509 - recall: 0.6600 - auc: 0.9646 - val_loss: 0.1328 - val_accuracy: 0.3059 - val_precision: 0.6510 - val_recall: 0.6833 - val_auc: 0.9661 - lr: 4.0000e-05\n",
      "Epoch 73/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1363 - accuracy: 0.2921 - precision: 0.6500 - recall: 0.6588 - auc: 0.9645 - val_loss: 0.1327 - val_accuracy: 0.3087 - val_precision: 0.6494 - val_recall: 0.6905 - val_auc: 0.9663 - lr: 4.0000e-05\n",
      "Epoch 74/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1363 - accuracy: 0.2960 - precision: 0.6497 - recall: 0.6604 - auc: 0.9646 - val_loss: 0.1327 - val_accuracy: 0.3059 - val_precision: 0.6528 - val_recall: 0.6819 - val_auc: 0.9662 - lr: 4.0000e-05\n",
      "Epoch 75/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1360 - accuracy: 0.2952 - precision: 0.6501 - recall: 0.6619 - auc: 0.9647 - val_loss: 0.1329 - val_accuracy: 0.3009 - val_precision: 0.6543 - val_recall: 0.6731 - val_auc: 0.9659 - lr: 4.0000e-05\n",
      "Epoch 76/100\n",
      "40/41 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.2940 - precision: 0.6516 - recall: 0.6606 - auc: 0.9647\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1360 - accuracy: 0.2941 - precision: 0.6517 - recall: 0.6603 - auc: 0.9647 - val_loss: 0.1326 - val_accuracy: 0.3066 - val_precision: 0.6545 - val_recall: 0.6768 - val_auc: 0.9662 - lr: 4.0000e-05\n",
      "Epoch 77/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1356 - accuracy: 0.2971 - precision: 0.6533 - recall: 0.6582 - auc: 0.9650 - val_loss: 0.1325 - val_accuracy: 0.3084 - val_precision: 0.6533 - val_recall: 0.6824 - val_auc: 0.9663 - lr: 8.0000e-06\n",
      "Epoch 78/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1363 - accuracy: 0.2951 - precision: 0.6509 - recall: 0.6582 - auc: 0.9646 - val_loss: 0.1325 - val_accuracy: 0.3090 - val_precision: 0.6527 - val_recall: 0.6844 - val_auc: 0.9663 - lr: 8.0000e-06\n",
      "Epoch 79/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1355 - accuracy: 0.2969 - precision: 0.6533 - recall: 0.6623 - auc: 0.9651 - val_loss: 0.1324 - val_accuracy: 0.3094 - val_precision: 0.6533 - val_recall: 0.6841 - val_auc: 0.9663 - lr: 8.0000e-06\n",
      "Epoch 80/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1357 - accuracy: 0.2959 - precision: 0.6523 - recall: 0.6617 - auc: 0.9649 - val_loss: 0.1324 - val_accuracy: 0.3095 - val_precision: 0.6531 - val_recall: 0.6834 - val_auc: 0.9664 - lr: 8.0000e-06\n",
      "Epoch 81/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1361 - accuracy: 0.2944 - precision: 0.6506 - recall: 0.6580 - auc: 0.9646 - val_loss: 0.1324 - val_accuracy: 0.3093 - val_precision: 0.6531 - val_recall: 0.6836 - val_auc: 0.9663 - lr: 8.0000e-06\n",
      "Epoch 82/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1357 - accuracy: 0.2960 - precision: 0.6531 - recall: 0.6619 - auc: 0.9649 - val_loss: 0.1324 - val_accuracy: 0.3086 - val_precision: 0.6533 - val_recall: 0.6819 - val_auc: 0.9663 - lr: 8.0000e-06\n",
      "Epoch 83/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.2947 - precision: 0.6530 - recall: 0.6579 - auc: 0.9649\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1357 - accuracy: 0.2947 - precision: 0.6530 - recall: 0.6579 - auc: 0.9649 - val_loss: 0.1324 - val_accuracy: 0.3095 - val_precision: 0.6530 - val_recall: 0.6832 - val_auc: 0.9663 - lr: 8.0000e-06\n",
      "Epoch 84/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1359 - accuracy: 0.2952 - precision: 0.6528 - recall: 0.6581 - auc: 0.9648 - val_loss: 0.1324 - val_accuracy: 0.3100 - val_precision: 0.6532 - val_recall: 0.6836 - val_auc: 0.9664 - lr: 1.6000e-06\n",
      "Epoch 85/100\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1361 - accuracy: 0.2965 - precision: 0.6519 - recall: 0.6593 - auc: 0.9647 - val_loss: 0.1324 - val_accuracy: 0.3094 - val_precision: 0.6531 - val_recall: 0.6826 - val_auc: 0.9663 - lr: 1.6000e-06\n",
      "Epoch 86/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.2959 - precision: 0.6525 - recall: 0.6596 - auc: 0.9650\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1356 - accuracy: 0.2959 - precision: 0.6525 - recall: 0.6596 - auc: 0.9650 - val_loss: 0.1324 - val_accuracy: 0.3091 - val_precision: 0.6534 - val_recall: 0.6825 - val_auc: 0.9663 - lr: 1.6000e-06\n",
      "Epoch 87/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1355 - accuracy: 0.2959 - precision: 0.6530 - recall: 0.6582 - auc: 0.9649 - val_loss: 0.1324 - val_accuracy: 0.3095 - val_precision: 0.6531 - val_recall: 0.6837 - val_auc: 0.9664 - lr: 3.2000e-07\n",
      "Epoch 88/100\n",
      "41/41 [==============================] - 1s 18ms/step - loss: 0.1359 - accuracy: 0.2954 - precision: 0.6531 - recall: 0.6566 - auc: 0.9648 - val_loss: 0.1324 - val_accuracy: 0.3093 - val_precision: 0.6530 - val_recall: 0.6835 - val_auc: 0.9663 - lr: 3.2000e-07\n",
      "Epoch 89/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.2952 - precision: 0.6528 - recall: 0.6583 - auc: 0.9650Restoring model weights from the end of the best epoch: 84.\n",
      "\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "41/41 [==============================] - 1s 19ms/step - loss: 0.1356 - accuracy: 0.2952 - precision: 0.6528 - recall: 0.6583 - auc: 0.9650 - val_loss: 0.1324 - val_accuracy: 0.3093 - val_precision: 0.6529 - val_recall: 0.6837 - val_auc: 0.9664 - lr: 3.2000e-07\n",
      "Epoch 89: early stopping\n",
      "\n",
      "=== Model Performance Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006/1006 [==============================] - 2s 2ms/step\n",
      "\n",
      "Detailed Performance Metrics:\n",
      "--------------------------------------------------------------------------------\n",
      "Disease              Accuracy   Precision  Recall     F1-Score  \n",
      "--------------------------------------------------------------------------------\n",
      "Anemia                   99.56%     16.67%      0.72%      1.39%\n",
      "Cholesterol              83.61%     61.03%     79.74%     69.14%\n",
      "CKD                      99.88%     84.62%     37.93%     52.38%\n",
      "Diabetes                 85.93%     66.91%     72.21%     69.46%\n",
      "Heart Disease            99.29%     94.87%     84.86%     89.59%\n",
      "Hypertension             74.60%     66.10%     70.95%     68.44%\n",
      "Metabolic Syndrome       99.48%     57.38%     19.89%     29.54%\n",
      "NAFLD                    99.17%     52.00%      9.70%     16.35%\n",
      "Obesity                 100.00%      0.00%      0.00%      0.00%\n",
      "Stroke                   89.59%     66.55%     37.37%     47.86%\n",
      "\n",
      "Average Metrics:\n",
      "----------------------------------------\n",
      "Accuracy       : 93.11%\n",
      "Precision      : 56.61%\n",
      "Recall         : 41.34%\n",
      "F1             : 44.42%\n",
      "\n",
      "Saving model...\n",
      "✓ Saved combined model in HDF5 format\n",
      "✓ Saved scaler\n",
      "\n",
      "All models and scaler saved successfully!\n",
      "Using average value for blood_pressure: 130.83\n",
      "Using average value for cholesterol: 98.87\n",
      "Using average value for blood_glucose: 18.57\n",
      "\n",
      "Derived Features:\n",
      "bmi: 23.44\n",
      "sodium: 1200.00\n",
      "fat: 9.00\n",
      "cholesterol_level: 206.79\n",
      "protein: 54.00\n",
      "carbs: 180.00\n",
      "\n",
      "Disease Risk Predictions:\n",
      "anemia: 0.00%\n",
      "cholesterol: 0.00%\n",
      "ckd: 0.00%\n",
      "diabetes: 0.00%\n",
      "heart: 0.00%\n",
      "hypertension: 0.00%\n",
      "ms: 0.00%\n",
      "nafld: 100.00%\n",
      "obesity: 0.00%\n",
      "stroke: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_and_process_data(file_path='dataset/health_data1/combined_dataset.csv'):\n",
    "    print(\"Loading and processing data...\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    \n",
    "    available_features = ['height', 'weight', 'gender', 'age', 'bp', 'bc', 'bg', 'bmi', 'sodium']\n",
    "    target_variables = ['anemia', 'cholesterol', 'ckd', 'diabetes', 'heart',\n",
    "                        'hypertension', 'ms', 'nafld', 'obesity', 'stroke']\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    print(\"Processing features...\")\n",
    "    numeric_features = [col for col in available_features if pd.api.types.is_numeric_dtype(df[col])]\n",
    "    categorical_features = [col for col in available_features if col not in numeric_features]\n",
    "    \n",
    "    print(\"Handling missing values...\")\n",
    "    print(f\"Found {len(numeric_features)} numeric features and {len(categorical_features)} categorical features\")\n",
    "    \n",
    "    numeric_imputer = IterativeImputer(random_state=42, max_iter=100, sample_posterior=True)\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    df_processed[numeric_features] = numeric_imputer.fit_transform(df_processed[numeric_features])\n",
    "    if categorical_features:\n",
    "        df_processed[categorical_features] = categorical_imputer.fit_transform(df_processed[categorical_features])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df_processed[numeric_features] = scaler.fit_transform(df_processed[numeric_features])\n",
    "    \n",
    "    df_processed = add_engineered_features(df_processed)\n",
    "    \n",
    "    final_features = available_features + [\n",
    "        'bmi_category', 'age_category', 'bp_category',\n",
    "        'bmi_age', 'bp_age', 'bmi_bp'\n",
    "    ]\n",
    "    \n",
    "    print(\"Processing target variables...\")\n",
    "    df_processed[target_variables] = df[target_variables].fillna(0)\n",
    "    \n",
    "    print(\"Data processing completed!\")\n",
    "    return df_processed[final_features], df_processed[target_variables]\n",
    "\n",
    "def add_engineered_features(X):\n",
    "    X_new = X.copy()\n",
    "    X_new['bmi_category'] = pd.cut(X_new['bmi'], \n",
    "                                  bins=[float('-inf'), 18.5, 25, 30, float('inf')],\n",
    "                                  labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['age_category'] = pd.cut(X_new['age'], \n",
    "                                  bins=[float('-inf'), 30, 45, 60, float('inf')],\n",
    "                                  labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['bp_category'] = pd.cut(X_new['bp'], \n",
    "                                 bins=[float('-inf'), 120, 140, 160, float('inf')],\n",
    "                                 labels=[0, 1, 2, 3])\n",
    "    \n",
    "    X_new['bmi_age'] = X_new['bmi'] * X_new['age']\n",
    "    X_new['bp_age'] = X_new['bp'] * X_new['age']\n",
    "    X_new['bmi_bp'] = X_new['bmi'] * X_new['bp']\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def train_disease_models(X, y):\n",
    "    print(\"\\n=== Starting Model Training Process ===\")\n",
    "    models = {}\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X = np.array(X)\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    \n",
    "    BATCH_SIZE = 2560\n",
    "    BUFFER_SIZE = 10000\n",
    "    \n",
    "    X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "    y_train, y_val = train_test_split(y.values, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\\\n",
    "        .cache()\\\n",
    "        .shuffle(BUFFER_SIZE)\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\\\n",
    "        .cache()\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    available_features = ['height', 'weight', 'gender', 'age', 'bp', 'bc', 'bg', 'bmi', 'sodium']\n",
    "    \n",
    "    final_features = available_features + [\n",
    "        'bmi_category', 'age_category', 'bp_category',\n",
    "        'bmi_age', 'bp_age', 'bmi_bp'\n",
    "    ]\n",
    "    \n",
    "    combined_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(len(final_features),)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(y.columns), activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    combined_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                tf.keras.metrics.Precision(name='precision'),\n",
    "                tf.keras.metrics.Recall(name='recall'),\n",
    "                tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nStarting combined model training...\")\n",
    "    history = combined_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=100,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return combined_model, scaler, history\n",
    "\n",
    "def evaluate_model_performance(model, X_test, y_test, scaler):\n",
    "    \"\"\"\n",
    "    Evaluates model performance using various metrics\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    print(\"\\n=== Model Performance Evaluation ===\")\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    disease_names = ['Anemia', 'Cholesterol', 'CKD', 'Diabetes', 'Heart Disease',\n",
    "                    'Hypertension', 'Metabolic Syndrome', 'NAFLD', 'Obesity', 'Stroke']\n",
    "    \n",
    "    print(\"\\nDetailed Performance Metrics:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Disease':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    overall_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}\n",
    "    \n",
    "    for i, disease in enumerate(disease_names):\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_test.iloc[:, i], y_pred_binary[:, i])\n",
    "            precision = precision_score(y_test.iloc[:, i], y_pred_binary[:, i], zero_division=0)\n",
    "            recall = recall_score(y_test.iloc[:, i], y_pred_binary[:, i], zero_division=0)\n",
    "            f1 = f1_score(y_test.iloc[:, i], y_pred_binary[:, i], zero_division=0)\n",
    "            \n",
    "            overall_metrics['accuracy'] += accuracy\n",
    "            overall_metrics['precision'] += precision\n",
    "            overall_metrics['recall'] += recall\n",
    "            overall_metrics['f1'] += f1\n",
    "            \n",
    "            print(f\"{disease:<20} {accuracy*100:>9.2f}% {precision*100:>9.2f}% {recall*100:>9.2f}% {f1*100:>9.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error evaluating {disease}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    n_diseases = len(disease_names)\n",
    "    print(\"\\nAverage Metrics:\")\n",
    "    print(\"-\" * 40)\n",
    "    for metric, value in overall_metrics.items():\n",
    "        print(f\"{metric.capitalize():<15}: {value/n_diseases*100:.2f}%\")\n",
    "    \n",
    "    return overall_metrics\n",
    "\n",
    "def save_models(model, scaler, save_dir='models'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    print(\"\\nSaving model...\")\n",
    "    \n",
    "    model_path = os.path.join(save_dir, 'disease-prediction-tf-model.h5')\n",
    "    model.save(model_path)\n",
    "    print(\"✓ Saved combined model in HDF5 format\")\n",
    "    \n",
    "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(\"✓ Saved scaler\")\n",
    "    \n",
    "    print(\"\\nAll models and scaler saved successfully!\")\n",
    "\n",
    "def load_models(save_dir='models'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        raise FileNotFoundError(f\"Directory {save_dir} not found\")\n",
    "    \n",
    "    print(\"\\nLoading model...\")\n",
    "    model_path = os.path.join(save_dir, 'disease-prediction-tf-model.h5')\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(\"✓ Loaded combined model from HDF5\")\n",
    "    \n",
    "    scaler_path = os.path.join(save_dir, 'scaler.joblib')\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print(\"✓ Loaded scaler\")\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "def predict_diseases(input_data, models, scaler):\n",
    "    print(\"\\n--- Predicting Diseases ---\")\n",
    "    \n",
    "    if not isinstance(input_data, pd.DataFrame):\n",
    "        input_data = pd.DataFrame([input_data])\n",
    "    \n",
    "    X_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    predictions = {}\n",
    "    for disease, model in models.items():\n",
    "        pred_prob = model.predict(X_scaled, verbose=0)[0][0]\n",
    "        predictions[disease] = {\n",
    "            'probability': float(pred_prob),\n",
    "            'prediction': 1 if pred_prob >= 0.5 else 0\n",
    "        }\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def predict_disease_risks(user_input, combined_model, prediction_scaler):\n",
    "    expected_features = [\n",
    "        'height', 'weight', 'gender', 'age', 'bp', 'bc', 'bg', 'bmi', 'sodium',\n",
    "        'bmi_category', 'age_category', 'bp_category',\n",
    "        'bmi_age', 'bp_age', 'bmi_bp'\n",
    "    ]\n",
    "    \n",
    "    if isinstance(user_input, pd.DataFrame):\n",
    "        for feature in expected_features:\n",
    "            if feature not in user_input.columns:\n",
    "                user_input[feature] = 0\n",
    "        \n",
    "        user_input = user_input[expected_features]\n",
    "    \n",
    "    X_scaled = prediction_scaler.transform(user_input)\n",
    "    \n",
    "    predictions = combined_model.predict(X_scaled, verbose=0)[0]\n",
    "    \n",
    "    disease_names = ['anemia', 'cholesterol', 'ckd', 'diabetes', 'heart',\n",
    "                    'hypertension', 'ms', 'nafld', 'obesity', 'stroke']\n",
    "    \n",
    "    predictions_percent = {disease: prob * 100 \n",
    "                         for disease, prob in zip(disease_names, predictions)}\n",
    "    \n",
    "    return predictions_percent\n",
    "\n",
    "def calculate_derived_features(height, weight, gender, age, blood_pressure, cholesterol, blood_glucose):\n",
    "    # bmi calculation\n",
    "    height_m = height / 100\n",
    "    bmi = weight / (height_m ** 2)\n",
    "    \n",
    "    # sodium calculation\n",
    "    sodium = weight * 20\n",
    "    \n",
    "    # fat based on gender calculatonn\n",
    "    fat = weight * (0.15 if gender == 1 else 0.25)\n",
    "    \n",
    "    # chols level calc\n",
    "    cholesterol_level = (bmi * 2) + (age * 0.15) + (blood_pressure * 0.05) + (blood_glucose * 0.02) + 150\n",
    "    \n",
    "    # protein calc\n",
    "    protein = weight * 0.9\n",
    "    \n",
    "    # carbo calc\n",
    "    carbs = weight * 3\n",
    "    \n",
    "    return {\n",
    "        'bmi': bmi,\n",
    "        'sodium': sodium,\n",
    "        'fat': fat,\n",
    "        'cholesterol_level': cholesterol_level,\n",
    "        'protein': protein,\n",
    "        'carbs': carbs\n",
    "    }\n",
    "\n",
    "def get_average_values(df):\n",
    "    return {\n",
    "        'height': df['height'].mean(),\n",
    "        'weight': df['weight'].mean(),\n",
    "        'gender': round(df['gender'].mean()),\n",
    "        'age': df['age'].mean(),\n",
    "        'blood_pressure': df['bp'].mean(),\n",
    "        'cholesterol': df['bc'].mean(),\n",
    "        'blood_glucose': df['bg'].mean()\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \n",
    "    df = pd.read_csv('dataset/health_data1/combined_dataset.csv')\n",
    "    avg_values = get_average_values(df)\n",
    "    \n",
    "    X, y = load_and_process_data()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    combined_model, scaler, history = train_disease_models(X_train, y_train)\n",
    "\n",
    "    evaluate_model_performance(combined_model, X_test, y_test, scaler)\n",
    "    \n",
    "    save_models(combined_model, scaler)\n",
    "    \n",
    "    # Default user input is \"None\", dont miss type it\n",
    "    user_input = {\n",
    "        'height': 160,\n",
    "        'weight': 60,\n",
    "        'gender': 1,  # 1=male, 0=female\n",
    "        'age': 20,\n",
    "        'blood_pressure': None,\n",
    "        'cholesterol': None,\n",
    "        'blood_glucose': None\n",
    "    }\n",
    "    \n",
    "    for key in user_input:\n",
    "        if user_input[key] is None:\n",
    "            user_input[key] = avg_values[key]\n",
    "            print(f\"Using average value for {key}: {user_input[key]:.2f}\")\n",
    "    \n",
    "    derived_features = calculate_derived_features(\n",
    "        user_input['height'],\n",
    "        user_input['weight'],\n",
    "        user_input['gender'],\n",
    "        user_input['age'],\n",
    "        user_input['blood_pressure'],\n",
    "        user_input['cholesterol'],\n",
    "        user_input['blood_glucose']\n",
    "    )\n",
    "    \n",
    "    user_input.update(derived_features)\n",
    "    \n",
    "    input_df = pd.DataFrame([user_input])\n",
    "    \n",
    "    expected_features = X.columns\n",
    "    for feature in expected_features:\n",
    "        if feature not in input_df:\n",
    "            input_df[feature] = avg_values.get(feature, 0)\n",
    "    \n",
    "    predictions = predict_disease_risks(input_df, combined_model, scaler)\n",
    "    \n",
    "    print(\"\\nDerived Features:\")\n",
    "    for feature, value in derived_features.items():\n",
    "        print(f\"{feature}: {value:.2f}\")\n",
    "    \n",
    "    print(\"\\nDisease Risk Predictions:\")\n",
    "    for disease, risk in predictions.items():\n",
    "        print(f\"{disease}: {risk:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Health Risk Assessment ===\n",
      "Using average value for blood_pressure: 130.83\n",
      "Using average value for cholesterol: 98.87\n",
      "Using average value for blood_glucose: 18.57\n",
      "\n",
      "Loading model...\n",
      "✓ Loaded combined model from HDF5\n",
      "✓ Loaded scaler\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "\n",
      "BMI: 20.2\n",
      "\n",
      "Disease Risk Predictions:\n",
      "ANEMIA: 100.0%\n",
      "CHOLESTEROL: 0.0%\n",
      "CKD: 0.0%\n",
      "DIABETES: 0.0%\n",
      "HEART: 0.0%\n",
      "HYPERTENSION: 0.0%\n",
      "MS: 0.0%\n",
      "NAFLD: 0.0%\n",
      "OBESITY: 0.0%\n",
      "STROKE: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def calculate_derived_features(height, weight, gender, age, blood_pressure, cholesterol, blood_glucose):\n",
    "    bmi = weight / ((height/100) ** 2)\n",
    "    \n",
    "    bmi_category = 0 if bmi < 18.5 else 1 if bmi < 25 else 2 if bmi < 30 else 3\n",
    "    age_category = 0 if age < 30 else 1 if age < 45 else 2 if age < 60 else 3\n",
    "    bp_category = 0 if blood_pressure < 120 else 1 if blood_pressure < 140 else 2 if blood_pressure < 160 else 3\n",
    "    \n",
    "    bmi_age = bmi * age\n",
    "    bp_age = blood_pressure * age\n",
    "    bmi_bp = bmi * blood_pressure\n",
    "    \n",
    "    return {\n",
    "        'bmi': bmi,\n",
    "        'bmi_category': bmi_category,\n",
    "        'age_category': age_category,\n",
    "        'bp_category': bp_category,\n",
    "        'bmi_age': bmi_age,\n",
    "        'bp_age': bp_age,\n",
    "        'bmi_bp': bmi_bp\n",
    "    }\n",
    "\n",
    "def predict_health_status(user_input):\n",
    "    # Load average values from dataset\n",
    "    df = pd.read_csv('dataset/health_data1/combined_dataset.csv')\n",
    "    \n",
    "    # Map column names untuk average values\n",
    "    column_mapping = {\n",
    "        'height': 'height',\n",
    "        'weight': 'weight',\n",
    "        'gender': 'gender',\n",
    "        'age': 'age',\n",
    "        'blood_pressure': 'bp',\n",
    "        'cholesterol': 'bc',\n",
    "        'blood_glucose': 'bg'\n",
    "    }\n",
    "    \n",
    "    # Calculate average values with proper column mapping\n",
    "    avg_values = {}\n",
    "    for key, col in column_mapping.items():\n",
    "        if col in df.columns:\n",
    "            avg_values[key] = df[col].mean()\n",
    "        else:\n",
    "            print(f\"Warning: Column {col} not found in dataset\")\n",
    "            avg_values[key] = 0\n",
    "    \n",
    "    # Round gender average\n",
    "    if 'gender' in avg_values:\n",
    "        avg_values['gender'] = round(avg_values['gender'])\n",
    "    \n",
    "    # Replace None values with averages\n",
    "    for key in ['blood_pressure', 'cholesterol', 'blood_glucose']:\n",
    "        if user_input[key] is None:\n",
    "            user_input[key] = avg_values[key]\n",
    "            print(f\"Using average value for {key}: {user_input[key]:.2f}\")\n",
    "    \n",
    "    model, scaler = load_models()\n",
    "    if model is None or scaler is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        features = {\n",
    "            'height': user_input['height'],\n",
    "            'weight': user_input['weight'],\n",
    "            'gender': user_input['gender'],\n",
    "            'age': user_input['age'],\n",
    "            'bp': user_input['blood_pressure'],\n",
    "            'bc': user_input['cholesterol'],\n",
    "            'bg': user_input['blood_glucose'],\n",
    "            'bmi': user_input['weight'] / ((user_input['height']/100) ** 2),\n",
    "            'sodium': 1200  # default value\n",
    "        }\n",
    "        \n",
    "        derived = calculate_derived_features(\n",
    "            user_input['height'],\n",
    "            user_input['weight'],\n",
    "            user_input['gender'],\n",
    "            user_input['age'],\n",
    "            user_input['blood_pressure'],\n",
    "            user_input['cholesterol'],\n",
    "            user_input['blood_glucose']\n",
    "        )\n",
    "        features.update(derived)\n",
    "        \n",
    "        input_df = pd.DataFrame([features])\n",
    "        input_scaled = scaler.transform(input_df)\n",
    "        \n",
    "        predictions = model.predict(input_scaled)\n",
    "        \n",
    "        diseases = ['anemia', 'cholesterol', 'ckd', 'diabetes', 'heart', \n",
    "                   'hypertension', 'ms', 'nafld', 'obesity', 'stroke']\n",
    "        results = {disease: float(pred*100) for disease, pred in zip(diseases, predictions[0])}\n",
    "        \n",
    "        return results, derived['bmi']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # input data user to check\n",
    "    user_input = {\n",
    "        'height': 165,\n",
    "        'weight': 55,\n",
    "        'gender': 1,  # 1 for male, 0 for female\n",
    "        'age': 20,\n",
    "        'blood_pressure': None,\n",
    "        'cholesterol': None,\n",
    "        'blood_glucose': None\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Health Risk Assessment ===\")\n",
    "    results = predict_health_status(user_input)\n",
    "    \n",
    "    if results:\n",
    "        predictions, bmi = results\n",
    "        print(f\"\\nBMI: {bmi:.1f}\")\n",
    "        print(\"\\nDisease Risk Predictions:\")\n",
    "        for disease, risk in predictions.items():\n",
    "            print(f\"{disease.upper()}: {risk:.1f}%\")\n",
    "    else:\n",
    "        print(\"Failed to generate predictions. Please check if the model files exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 03:17:41.475245: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING:tensorflow:From C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:root:Failure to load the inference.so custom c++ tensorflow ops. This error is likely caused the version of TensorFlow and TensorFlow Decision Forests are not compatible. Full error:C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\Scripts\\tensorflowjs_converter.exe\\__main__.py\", line 4, in <module>\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflowjs\\__init__.py\", line 21, in <module>\n",
      "    from tensorflowjs import converters\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflowjs\\converters\\__init__.py\", line 21, in <module>\n",
      "    from tensorflowjs.converters.converter import convert\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflowjs\\converters\\converter.py\", line 38, in <module>\n",
      "    from tensorflowjs.converters import tf_saved_model_conversion_v2\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflowjs\\converters\\tf_saved_model_conversion_v2.py\", line 28, in <module>\n",
      "    import tensorflow_decision_forests\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_decision_forests\\__init__.py\", line 64, in <module>\n",
      "    from tensorflow_decision_forests import keras\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_decision_forests\\keras\\__init__.py\", line 53, in <module>\n",
      "    from tensorflow_decision_forests.keras import core\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_decision_forests\\keras\\core.py\", line 62, in <module>\n",
      "    from tensorflow_decision_forests.keras import core_inference\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_decision_forests\\keras\\core_inference.py\", line 36, in <module>\n",
      "    from tensorflow_decision_forests.tensorflow.ops.inference import api as tf_op\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\api.py\", line 179, in <module>\n",
      "    from tensorflow_decision_forests.tensorflow.ops.inference import op\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op.py\", line 15, in <module>\n",
      "    from tensorflow_decision_forests.tensorflow.ops.inference.op_dynamic import *\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py\", line 24, in <module>\n",
      "    raise e\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py\", line 21, in <module>\n",
      "    ops = tf.load_op_library(resource_loader.get_path_to_datafile(\"inference.so\"))\n",
      "  File \"C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py\", line 54, in load_op_library\n",
      "    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: C:\\Users\\Dana\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter --input_format keras {models\\disease-prediction-tf-model.h5} ./models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
